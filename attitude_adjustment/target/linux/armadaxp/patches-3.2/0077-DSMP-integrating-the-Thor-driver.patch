From 92f68fc42e2a340a67238ec62483ef922ec25e78 Mon Sep 17 00:00:00 2001
From: Seif Mazareeb <seif@marvell.com>
Date: Mon, 5 Mar 2012 00:37:23 +0200
Subject: [PATCH 077/609] DSMP integrating the Thor driver

Signed-off-by: Seif Mazareeb <seif@marvell.com>
---
 drivers/scsi/Kconfig                               |    6 +
 drivers/scsi/Makefile                              |    1 +
 drivers/scsi/thor/Makefile                         |  174 +
 drivers/scsi/thor/Makefile.kbuild                  |  187 +
 drivers/scsi/thor/core/thor/consolid.c             |  524 +++
 drivers/scsi/thor/core/thor/consolid.h             |   60 +
 drivers/scsi/thor/core/thor/core_api.c             |  710 +++
 drivers/scsi/thor/core/thor/core_api.h             |   44 +
 drivers/scsi/thor/core/thor/core_ata.h             |  265 ++
 drivers/scsi/thor/core/thor/core_cons.h            |   28 +
 drivers/scsi/thor/core/thor/core_exp.c             | 4651 ++++++++++++++++++++
 drivers/scsi/thor/core/thor/core_exp.h             |   67 +
 drivers/scsi/thor/core/thor/core_init.c            | 2818 ++++++++++++
 drivers/scsi/thor/core/thor/core_init.h            |  135 +
 drivers/scsi/thor/core/thor/core_inter.h           |   77 +
 drivers/scsi/thor/core/thor/core_sat.c             |  836 ++++
 drivers/scsi/thor/core/thor/core_sat.h             |   77 +
 drivers/scsi/thor/core/thor/core_sata.h            |   50 +
 drivers/scsi/thor/core/thor/core_swxor.c           |  528 +++
 drivers/scsi/thor/core/thor/core_thor.h            |  500 +++
 drivers/scsi/thor/core/thor/core_xor.c             |  846 ++++
 drivers/scsi/thor/core/thor/core_xor.h             |   10 +
 drivers/scsi/thor/core/thor/scsi2sata.c            |  787 ++++
 drivers/scsi/thor/include/com_dbg.h                |  164 +
 drivers/scsi/thor/include/com_list.c               |  183 +
 drivers/scsi/thor/include/com_list.h               |  193 +
 drivers/scsi/thor/include/com_mod_mgmt.h           |  137 +
 drivers/scsi/thor/include/com_nvram.h              |  106 +
 drivers/scsi/thor/include/com_scsi.h               |  299 ++
 drivers/scsi/thor/include/com_sgd.h                |  383 ++
 drivers/scsi/thor/include/com_tag.h                |   33 +
 drivers/scsi/thor/include/com_type.h               |  427 ++
 drivers/scsi/thor/include/com_u64.h                |   22 +
 drivers/scsi/thor/include/com_util.h               |  241 +
 drivers/scsi/thor/include/csmisas.h                | 1223 +++++
 drivers/scsi/thor/include/generic/com_define.h     |  345 ++
 drivers/scsi/thor/include/generic/com_error.h      |   61 +
 .../scsi/thor/include/generic/com_event_define.h   |  353 ++
 .../thor/include/generic/com_event_define_ext.h    |  357 ++
 drivers/scsi/thor/include/generic/com_struct.h     |  684 +++
 drivers/scsi/thor/include/icommon/com_api.h        |  167 +
 .../scsi/thor/include/icommon/com_event_struct.h   |   40 +
 drivers/scsi/thor/include/icommon/com_flash.h      |   26 +
 drivers/scsi/thor/include/icommon/com_ioctl.h      |   59 +
 drivers/scsi/thor/lib/common/com_dbg.c             |   33 +
 drivers/scsi/thor/lib/common/com_nvram.c           |  164 +
 drivers/scsi/thor/lib/common/com_scsi.c            |   84 +
 drivers/scsi/thor/lib/common/com_sgd.c             |  935 ++++
 drivers/scsi/thor/lib/common/com_tag.c             |   71 +
 drivers/scsi/thor/lib/common/com_u64.c             |  148 +
 drivers/scsi/thor/lib/common/com_util.c            |  736 ++++
 drivers/scsi/thor/linux/hba_exp.c                  |  305 ++
 drivers/scsi/thor/linux/hba_exp.h                  |  196 +
 drivers/scsi/thor/linux/hba_header.h               |   27 +
 drivers/scsi/thor/linux/hba_inter.h                |   64 +
 drivers/scsi/thor/linux/hba_mod.c                  | 2822 ++++++++++++
 drivers/scsi/thor/linux/hba_mod.h                  |   21 +
 drivers/scsi/thor/linux/hba_timer.c                |  315 ++
 drivers/scsi/thor/linux/hba_timer.h                |   74 +
 drivers/scsi/thor/linux/linux_iface.c              |  684 +++
 drivers/scsi/thor/linux/linux_iface.h              |   56 +
 drivers/scsi/thor/linux/linux_main.c               |  307 ++
 drivers/scsi/thor/linux/linux_main.h               |   48 +
 drivers/scsi/thor/linux/mv_os.h                    |  208 +
 drivers/scsi/thor/linux/oss_wrapper.c              |  111 +
 drivers/scsi/thor/linux/oss_wrapper.h              |  118 +
 drivers/scsi/thor/linux/res_mgmt.c                 |  201 +
 drivers/scsi/thor/linux/res_mgmt.h                 |   47 +
 drivers/scsi/thor/mv_conf.mk                       |    1 +
 drivers/scsi/thor/mv_config.h                      |   17 +
 drivers/scsi/thor/mv_include.h                     |   49 +
 drivers/scsi/thor/mv_thor.h                        |  174 +
 drivers/scsi/thor/patch.kbuild                     |   10 +
 drivers/scsi/thor/patch.sh                         |   87 +
 74 files changed, 26997 insertions(+)
 create mode 100644 drivers/scsi/thor/Makefile
 create mode 100644 drivers/scsi/thor/Makefile.kbuild
 create mode 100644 drivers/scsi/thor/core/thor/consolid.c
 create mode 100644 drivers/scsi/thor/core/thor/consolid.h
 create mode 100644 drivers/scsi/thor/core/thor/core_api.c
 create mode 100644 drivers/scsi/thor/core/thor/core_api.h
 create mode 100644 drivers/scsi/thor/core/thor/core_ata.h
 create mode 100644 drivers/scsi/thor/core/thor/core_cons.h
 create mode 100644 drivers/scsi/thor/core/thor/core_exp.c
 create mode 100644 drivers/scsi/thor/core/thor/core_exp.h
 create mode 100644 drivers/scsi/thor/core/thor/core_init.c
 create mode 100644 drivers/scsi/thor/core/thor/core_init.h
 create mode 100644 drivers/scsi/thor/core/thor/core_inter.h
 create mode 100644 drivers/scsi/thor/core/thor/core_sat.c
 create mode 100644 drivers/scsi/thor/core/thor/core_sat.h
 create mode 100644 drivers/scsi/thor/core/thor/core_sata.h
 create mode 100644 drivers/scsi/thor/core/thor/core_swxor.c
 create mode 100644 drivers/scsi/thor/core/thor/core_thor.h
 create mode 100644 drivers/scsi/thor/core/thor/core_xor.c
 create mode 100644 drivers/scsi/thor/core/thor/core_xor.h
 create mode 100644 drivers/scsi/thor/core/thor/scsi2sata.c
 create mode 100644 drivers/scsi/thor/include/com_dbg.h
 create mode 100644 drivers/scsi/thor/include/com_list.c
 create mode 100644 drivers/scsi/thor/include/com_list.h
 create mode 100644 drivers/scsi/thor/include/com_mod_mgmt.h
 create mode 100644 drivers/scsi/thor/include/com_nvram.h
 create mode 100644 drivers/scsi/thor/include/com_scsi.h
 create mode 100644 drivers/scsi/thor/include/com_sgd.h
 create mode 100644 drivers/scsi/thor/include/com_tag.h
 create mode 100644 drivers/scsi/thor/include/com_type.h
 create mode 100644 drivers/scsi/thor/include/com_u64.h
 create mode 100644 drivers/scsi/thor/include/com_util.h
 create mode 100644 drivers/scsi/thor/include/csmisas.h
 create mode 100644 drivers/scsi/thor/include/generic/com_define.h
 create mode 100644 drivers/scsi/thor/include/generic/com_error.h
 create mode 100644 drivers/scsi/thor/include/generic/com_event_define.h
 create mode 100644 drivers/scsi/thor/include/generic/com_event_define_ext.h
 create mode 100644 drivers/scsi/thor/include/generic/com_struct.h
 create mode 100644 drivers/scsi/thor/include/icommon/com_api.h
 create mode 100644 drivers/scsi/thor/include/icommon/com_event_struct.h
 create mode 100644 drivers/scsi/thor/include/icommon/com_flash.h
 create mode 100644 drivers/scsi/thor/include/icommon/com_ioctl.h
 create mode 100644 drivers/scsi/thor/lib/common/com_dbg.c
 create mode 100644 drivers/scsi/thor/lib/common/com_nvram.c
 create mode 100644 drivers/scsi/thor/lib/common/com_scsi.c
 create mode 100644 drivers/scsi/thor/lib/common/com_sgd.c
 create mode 100644 drivers/scsi/thor/lib/common/com_tag.c
 create mode 100644 drivers/scsi/thor/lib/common/com_u64.c
 create mode 100644 drivers/scsi/thor/lib/common/com_util.c
 create mode 100644 drivers/scsi/thor/linux/hba_exp.c
 create mode 100644 drivers/scsi/thor/linux/hba_exp.h
 create mode 100644 drivers/scsi/thor/linux/hba_header.h
 create mode 100644 drivers/scsi/thor/linux/hba_inter.h
 create mode 100644 drivers/scsi/thor/linux/hba_mod.c
 create mode 100644 drivers/scsi/thor/linux/hba_mod.h
 create mode 100644 drivers/scsi/thor/linux/hba_timer.c
 create mode 100644 drivers/scsi/thor/linux/hba_timer.h
 create mode 100644 drivers/scsi/thor/linux/linux_iface.c
 create mode 100644 drivers/scsi/thor/linux/linux_iface.h
 create mode 100644 drivers/scsi/thor/linux/linux_main.c
 create mode 100644 drivers/scsi/thor/linux/linux_main.h
 create mode 100644 drivers/scsi/thor/linux/mv_os.h
 create mode 100644 drivers/scsi/thor/linux/oss_wrapper.c
 create mode 100644 drivers/scsi/thor/linux/oss_wrapper.h
 create mode 100644 drivers/scsi/thor/linux/res_mgmt.c
 create mode 100644 drivers/scsi/thor/linux/res_mgmt.h
 create mode 100644 drivers/scsi/thor/mv_conf.mk
 create mode 100644 drivers/scsi/thor/mv_config.h
 create mode 100644 drivers/scsi/thor/mv_include.h
 create mode 100644 drivers/scsi/thor/mv_thor.h
 create mode 100644 drivers/scsi/thor/patch.kbuild
 create mode 100644 drivers/scsi/thor/patch.sh

--- a/drivers/scsi/Kconfig
+++ b/drivers/scsi/Kconfig
@@ -348,6 +348,12 @@ menuconfig SCSI_LOWLEVEL
 
 if SCSI_LOWLEVEL && SCSI
 
+config SCSI_MV_THOR
+        tristate "Marvell Storage Controller 6121/6122/6141/6145"
+        depends on SCSI && BLK_DEV_SD
+        help
+                Provides support for Marvell thor Storage Controller series.
+
 config ISCSI_TCP
 	tristate "iSCSI Initiator over TCP/IP"
 	depends on SCSI && INET
--- a/drivers/scsi/Makefile
+++ b/drivers/scsi/Makefile
@@ -51,6 +51,7 @@ obj-$(CONFIG_A2091_SCSI)	+= a2091.o	wd33
 obj-$(CONFIG_GVP11_SCSI)	+= gvp11.o	wd33c93.o
 obj-$(CONFIG_MVME147_SCSI)	+= mvme147.o	wd33c93.o
 obj-$(CONFIG_SGIWD93_SCSI)	+= sgiwd93.o	wd33c93.o
+obj-$(CONFIG_SCSI_MV_THOR)      += thor/
 obj-$(CONFIG_ATARI_SCSI)	+= atari_scsi.o
 obj-$(CONFIG_MAC_SCSI)		+= mac_scsi.o
 obj-$(CONFIG_SCSI_MAC_ESP)	+= esp_scsi.o	mac_esp.o
--- /dev/null
+++ b/drivers/scsi/thor/Makefile
@@ -0,0 +1,174 @@
+#
+#
+# Note: This Makefile is for 2.6 kernel only, at present.
+#
+# V0.0.0.6 Ver.Make
+
+# default to build for the running kernel
+ifeq ("x", "x$(KERNEL_SRC)")
+	KERNEL_SRC=/lib/modules/$(shell uname -r)/build
+endif
+
+# use KERNEL_SRC if not called by our driver disk maker
+ifeq ("x", "x$(KERNEL_SOURCE_DIR)")
+        KERNEL_SRC_DIR=$(KERNEL_SRC)
+else
+	KERNEL_SRC_DIR=$(KERNEL_SOURCE_DIR)
+endif
+
+ifeq ("x", "x$(KERNEL_BUILD_DIR)")
+	KERNEL_BLD_DIR=$(KERNEL_SRC_DIR)
+else
+        KERNEL_BLD_DIR=$(KERNEL_BUILD_DIR)
+endif
+
+#ifneq ($(KERNELRELEASE),)
+#	include $(SUBDIRS)/mv_conf.mk
+#else
+#	include mv_conf.mk
+#endif
+
+SUPPORT_THOR=y
+
+CONFIG_64BIT=$(shell [ -f $(KERNEL_BLD_DIR)/.config ] && cat $(KERNEL_BLD_DIR)/.config | grep -m 1 CONFIG_64BIT | awk -F= '{print $$2}')
+CONFIG_REGPARM=$(shell [ -f $(KERNEL_BLD_DIR)/.config ] && cat $(KERNEL_BLD_DIR)/.config | grep -m 1 CONFIG_REGPARM | awk -F= '{print $$2}')
+CONFIG_SUSE_KERNEL=$(shell [ -f $(KERNEL_BLD_DIR)/.config ] && cat $(KERNEL_BLD_DIR)/.config | grep -m 1 CONFIG_SUSE_KERNEL | awk -F= '{print $$2}')
+
+ifeq ($(ARCH), )
+ifeq ($(strip $(CONFIG_64BIT)),y)
+	ARCH_TYPE=x86_64
+else
+	ARCH_TYPE=i386
+endif
+else
+	ARCH_TYPE=$(ARCH)
+endif
+
+MV_CC     =  $(CROSS_COMPILE)$(CC)
+MV_LD     =  $(CROSS_COMPILE)$(LD)
+
+export LD_LIBRARY_PATH
+
+odin_objs_$(SUPPORT_ODIN)   := core/odin/core_exp.o   core/odin/core_init.o   \
+		               core/odin/core_ibcd.o  core/odin/core_adpt.o   \
+		               core/odin/core_spi.o    \
+		               core/odin/core_ses.o   core/odin/core_xor.o    \
+		               core/odin/core_api.o   core/odin/scsi2sata.o   \
+		               core/odin/core_i2c.o                           \
+		               core/odin/ata2scsi.o   core/odin/core_helper.o \
+			       core/odin/core_extern.o core/odin/core_swxor.o  \
+#    core/odin/consolid.o \
+#   core/odin/core_eeprom.o \
+
+thor_objs_$(SUPPORT_THOR)   := core/thor/core_exp.o core/thor/core_init.o\
+		   core/thor/core_api.o core/thor/core_xor.o\
+		   core/thor/scsi2sata.o core/thor/core_swxor.o core/thor/core_sat.o
+
+#cons_objs_$(SUPPORT_CONSOLIDATE) := core/thor/consolid.o
+
+CORE_OBJS       :=  $(odin_objs_y) $(thor_objs_y)
+
+
+COMM_OBJS       := lib/common/com_util.o lib/common/com_u64.o             \
+		   lib/common/com_scsi.o lib/common/com_tag.o             \
+		   lib/common/com_sgd.o  lib/common/com_nvram.o
+
+
+OSDEP_OBJS      := linux/linux_main.o    linux/hba_exp.o          \
+		   linux/hba_mod.o       linux/hba_timer.o        \
+		   linux/oss_wrapper.o   linux/linux_iface.o      \
+	           linux/res_mgmt.o
+
+# for partial source tree building
+core_dir_$(SUPPORT_ODIN) :=core/odin
+core_dir_$(SUPPORT_THOR) :=core/thor
+include_dir := include  include/generic  include/icommon
+
+header_dirs     :=   $(include_dir)
+
+LIBMV_OBJS	:=  $(COMM_OBJS) $(RAID_OBJS)
+
+HBA_OBJS        := $(OSDEP_OBJS) $(CORE_OBJS) $(LIBMV_OBJS)
+
+INCLUDE_DIR     = -I$(KERNEL_BLD_DIR)/include                          \
+		  -I$(KERNEL_BLD_DIR)/include/scsi                     \
+		  -I$(KERNEL_BLD_DIR)/drivers/scsi                     \
+		  -I$(KERNEL_SRC_DIR)/include                          \
+		  -I$(KERNEL_SRC_DIR)/include/scsi                     \
+		  -I$(KERNEL_SRC_DIR)/drivers/scsi
+
+
+ifneq ($(KERNELRELEASE),)
+ifeq ($(SUPPORT_ODIN), y)
+obj-m         :=   mv64xx.o
+mv64xx-objs   :=   $(HBA_OBJS)
+endif
+
+ifeq ($(SUPPORT_THOR), y)
+obj-m         :=   mv61xx.o
+mv61xx-objs   :=   $(HBA_OBJS)
+endif
+
+clean-files   +=   Modules.symvers
+clean-files   +=   Module.symvers
+EXTRA_CFLAGS  :=   -I$(src)/include         \
+		   -I$(src)/core            \
+		   -I$(src)/.               \
+		   -I$(src)/linux           \
+                   -I$(src)/include/generic \
+		   -I$(src)/include/icommon
+
+EXTRA_CFLAGS  +=   -D__MV_LINUX__ $(INCLUDE_DIR)
+
+ifeq ($(CONFIG_64BIT), y)
+EXTRA_CFLAGS  +=   -D_64_SYS_
+EXTRA_CFLAGS  +=   -D_64_BIT_COMPILER
+else
+EXTRA_CFLAGS  +=   -D_32_LEGACY_
+endif
+ifeq ($(CONFIG_4KSTACKS), y)
+EXTRA_CFLAGS  +=   -D_4K_STACKS_
+endif
+
+ifeq ($(CONFIG_SUSE_KERNEL), y)
+EXTRA_CFLAGS  += -DIS_OPENSUSE_SLED_SLES=1
+else
+EXTRA_CFLAGS  += -DIS_OPENSUSE_SLED_SLES=0
+endif
+EXTRA_CFLAGS  +=   -include $(src)/mv_config.h
+
+ifeq ($(SUPPORT_ODIN), y)
+EXTRA_CFLAGS  +=   -I$(src)/core/odin -DPRODUCTNAME_ODIN
+endif
+
+ifeq ($(SUPPORT_THOR), y)
+EXTRA_CFLAGS  +=   -I$(src)/core/thor -DPRODUCTNAME_THOR
+endif
+
+EXTRA_CFLAGS  += #-D__MV_DEBUG__
+EXTRA_CFLAGS  += -Wno-unused-variable  -Wno-pointer-arith
+EXTRA_CFLAGS  += -D__LEGACY_OSSW__=1
+
+else
+
+# Why use SUBDIRS? for backward compatibility
+all:
+	$(MAKE) ARCH=$(ARCH_TYPE) CC=$(MV_CC) LD=$(MV_LD) CROSS_COMPILE=$(CROSS_COMPILE) V=$(V) -C $(KERNEL_BLD_DIR) SUBDIRS=`pwd` modules
+
+kbuild:
+	-@cat patch.kbuild | patch -p1 -N -d $(KERNEL_SRC_DIR)/drivers/scsi
+	@./patch.sh $(KERNEL_SRC_DIR)/drivers/scsi a
+	@mkdir -p $(KERNEL_SRC_DIR)/drivers/scsi/mv
+	@cp -r * $(KERNEL_SRC_DIR)/drivers/scsi/mv
+	@cp -f Makefile.kbuild $(KERNEL_SRC_DIR)/drivers/scsi/mv/Makefile
+	@echo done.
+
+ukbuild:
+	-@cat patch.kbuild | patch -p1 -N -R -d $(KERNEL_SRC_DIR)/drivers/scsi
+	@./patch.sh $(KERNEL_SRC_DIR)/drivers/scsi
+	@rm -rf $(KERNEL_SRC_DIR)/drivers/scsi/mv
+	@echo Patch has been successfully rewinded.
+
+clean:
+	$(MAKE) ARCH=$(ARCH_TYPE) CC=$(MV_CC) LD=$(MV_LD) CROSS_COMPILE=$(CROSS_COMPILE) V=$(V) -C $(KERNEL_BLD_DIR) SUBDIRS=`pwd` clean
+endif
--- /dev/null
+++ b/drivers/scsi/thor/Makefile.kbuild
@@ -0,0 +1,187 @@
+#
+#
+# Note: This Makefile is for 2.6 kernel, non-raid only, at present.
+#
+# V0.0.0.2 Ver.Make for kbuild
+
+# default to build for the running kernel
+ifeq ("x", "x$(KERNEL_SRC)")
+	KERNEL_SRC=/lib/modules/$(shell uname -r)/build
+endif
+
+# use KERNEL_SRC if not called by our driver disk maker
+ifeq ("x", "x$(KERNEL_SOURCE_DIR)")
+        KERNEL_SRC_DIR=$(KERNEL_SRC)
+else
+	KERNEL_SRC_DIR=$(KERNEL_SOURCE_DIR)
+endif
+
+ifeq ("x", "x$(KERNEL_BUILD_DIR)")
+	KERNEL_BLD_DIR=$(KERNEL_SRC_DIR)
+else
+        KERNEL_BLD_DIR=$(KERNEL_BUILD_DIR)
+endif
+src=$(KERNEL_SRC)/drivers/scsi/mv
+include $(src)/mv_conf.mk
+
+ifeq ("xy", "x$(RAID_MODULE)")
+	LIB_TYPE=_raid_
+endif
+
+# KERNEL_VER := $(shell cat $(KERNEL_BLD_SRC)/include/linux/version.h | grep UTS_RELEASE | cut -c22-24 | head -n 1)
+
+# we may build for 32bit kernel on a 64bit system
+CONFIG_64BIT=$(shell [ -f $(KERNEL_BLD_DIR)/.config ] && cat $(KERNEL_BLD_DIR)/.config | grep -m 1 CONFIG_64BIT | awk -F= '{print $$2}')
+CONFIG_REGPARM=$(shell [ -f $(KERNEL_BLD_DIR)/.config ] && cat $(KERNEL_BLD_DIR)/.config | grep -m 1 CONFIG_REGPARM | awk -F= '{print $$2}')
+CONFIG_4KSTACKS=$(shell [ -f $(KERNEL_BLD_DIR)/.config ] && cat $(KERNEL_BLD_DIR)/.config | grep -m 1 CONFIG_4KSTACKS | awk -F= '{print $$2}')
+CONFIG_SUSE_KERNEL=$(shell [ -f $(KERNEL_BLD_DIR)/.config ] && cat $(KERNEL_BLD_DIR)/.config | grep -m 1 CONFIG_SUSE_KERNEL | awk -F= '{print $$2}')
+
+ifeq ($(ARCH), )
+ifeq ($(strip $(CONFIG_64BIT)),y)
+	ARCH_TYPE=x86_64
+		RAID_LIB_NAME=libmv$(LIB_TYPE)64
+else
+	ARCH_TYPE=i386
+	ifeq ($(strip $(CONFIG_REGPARM)),y)
+		RAID_LIB_NAME=libmv$(LIB_TYPE)32
+	else
+		RAID_LIB_NAME=libmv$(LIB_TYPE)32_noregparm
+	endif
+endif
+else
+	ARCH_TYPE=$(ARCH)
+endif
+
+MV_CC     =  $(CROSS_COMPILE)$(CC)
+MV_LD     =  $(CROSS_COMPILE)$(LD)
+
+export LD_LIBRARY_PATH
+
+odin_objs_$(SUPPORT_ODIN)   := core/odin/core_exp.o   core/odin/core_init.o   \
+		               core/odin/core_ibcd.o  core/odin/core_adpt.o   \
+		               core/odin/core_spi.o    \
+		               core/odin/core_ses.o   core/odin/core_xor.o    \
+		               core/odin/core_api.o   core/odin/scsi2sata.o   \
+		               core/odin/core_i2c.o                           \
+		               core/odin/ata2scsi.o   core/odin/core_helper.o \
+			       core/odin/core_extern.o core/odin/core_swxor.o  \
+#    core/odin/consolid.o \
+#   core/odin/core_eeprom.o \
+
+thor_objs_$(SUPPORT_THOR)   := core/thor/core_exp.o core/thor/core_init.o\
+		   core/thor/core_api.o core/thor/core_xor.o\
+		   core/thor/scsi2sata.o core/thor/core_swxor.o core/thor/core_sat.o
+
+#cons_objs_$(SUPPORT_CONSOLIDATE) := core/thor/consolid.o
+
+CORE_OBJS       :=  $(odin_objs_y) $(thor_objs_y)
+
+
+COMM_OBJS       := lib/common/com_util.o lib/common/com_u64.o             \
+		   lib/common/com_scsi.o lib/common/com_tag.o             \
+		   lib/common/com_sgd.o  lib/common/com_nvram.o
+
+raid_objs_$(RAID_MODULE) := raid/ddf_handler.o       raid/raid0.o         \
+			    raid/raid1.o             raid/raid_util_ext.o \
+		            raid/raid_api.o          raid/raid_bga.o      \
+	                    raid/raid_cmd_handler.o  raid/raid_ddf.o      \
+		            raid/raid_eh.o           raid/raid_exp.o      \
+			    raid/raid_manage_ext.o   raid/raid_mp.o       \
+		            raid/raid_manage.o       raid/raid_res_mgmt.o \
+		            raid/raid_tset.o         raid/raid_util.o     \
+			    raid/ddf_handler_ext.o   raid/raid_api_ext.o  \
+                            raid/raid_cmd_ext.o      raid/raid_ddf_ext.o  \
+			    raid/raid_exp_ext.o
+
+cache_objs_$(CACHE_MODULE) := cache/cache_exp.o      cache/cache_mod.o
+
+raid6_objs_$(RAID6_MODULE) := raid/raid6.o math/math_matrix.o
+
+RAID_OBJS       := $(raid_objs_y)  $(raid6_objs_y) $(cache_objs_y)
+
+OSDEP_OBJS      := linux/linux_main.o    linux/hba_exp.o          \
+		   linux/hba_mod.o       linux/hba_timer.o        \
+		   linux/oss_wrapper.o   linux/linux_iface.o      \
+	           linux/res_mgmt.o
+
+# for partial source tree building
+core_dir_$(SUPPORT_ODIN) :=core/odin
+core_dir_$(SUPPORT_THOR) :=core/thor
+include_dir := include  include/generic  include/icommon
+
+header_dirs     :=   $(include_dir)
+
+LIBMV_OBJS	:=  $(COMM_OBJS) $(RAID_OBJS)
+
+HBA_OBJS        := $(OSDEP_OBJS) $(CORE_OBJS) $(LIBMV_OBJS)
+
+INCLUDE_DIR     = -I$(KERNEL_BLD_DIR)/include                          \
+		  -I$(KERNEL_BLD_DIR)/include/scsi                     \
+		  -I$(KERNEL_BLD_DIR)/drivers/scsi                     \
+		  -I$(KERNEL_SRC_DIR)/include                          \
+		  -I$(KERNEL_SRC_DIR)/include/scsi                     \
+		  -I$(KERNEL_SRC_DIR)/drivers/scsi
+
+
+ifeq ($(SUPPORT_ODIN), y)
+obj-$(CONFIG_SCSI_MV_64xx)      :=   mv64xx.o
+mv64xx-objs   :=   $(HBA_OBJS)
+endif
+
+ifeq ($(SUPPORT_THOR), y)
+obj-$(CONFIG_SCSI_MV_61xx)      :=   mv61xx.o
+mv61xx-objs   :=   $(HBA_OBJS)
+endif
+
+clean-files   +=   Modules.symvers
+clean-files   +=   Module.symvers
+EXTRA_CFLAGS  :=   -I$(src)/include    -I$(src)/core            \
+		   -I$(src)/raid       -I$(src)/.               \
+		   -I$(src)/linux  -I$(src)/math            \
+                   -I$(src)/cache      -I$(src)/include/generic \
+		   -I$(src)/include/icommon
+
+EXTRA_CFLAGS  +=   -D__MV_LINUX__ $(INCLUDE_DIR)
+
+ifeq ($(CONFIG_64BIT), y)
+EXTRA_CFLAGS  +=   -D_64_SYS_
+EXTRA_CFLAGS  +=   -D_64_BIT_COMPILER
+else
+EXTRA_CFLAGS  +=   -D_32_LEGACY_
+endif
+ifeq ($(CONFIG_4KSTACKS), y)
+EXTRA_CFLAGS  +=   -D_4K_STACKS_
+endif
+
+ifeq ($(CONFIG_SUSE_KERNEL), y)
+EXTRA_CFLAGS  += -DIS_OPENSUSE_SLED_SLES=1
+else
+EXTRA_CFLAGS  += -DIS_OPENSUSE_SLED_SLES=0
+endif
+
+EXTRA_CFLAGS  +=   -include $(src)/mv_config.h
+
+ifeq ($(RAID_MODULE), y)
+EXTRA_CFLAGS  +=   -DRAID_DRIVER=1
+endif
+
+ifeq ($(RAID6_MODULE), y)
+EXTRA_CFLAGS  +=   -DSUPPORT_RAID6=1
+endif
+
+ifeq ($(CACHE_MODULE), y)
+EXTRA_CFLAGS  +=   -DCACHE_MODULE_SUPPORT=1
+endif
+
+ifeq ($(SUPPORT_ODIN), y)
+EXTRA_CFLAGS  +=   -I$(src)/core/odin -DPRODUCTNAME_ODIN
+endif
+
+ifeq ($(SUPPORT_THOR), y)
+EXTRA_CFLAGS  +=   -I$(src)/core/thor -DPRODUCTNAME_THOR
+endif
+
+
+EXTRA_CFLAGS  += #-D__MV_DEBUG__
+EXTRA_CFLAGS  += -Wno-unused-variable  -Wno-pointer-arith
+EXTRA_CFLAGS  +=   -D__LEGACY_OSSW__=1
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/consolid.c
@@ -0,0 +1,524 @@
+#include "mv_include.h"
+
+#ifdef SUPPORT_CONSOLIDATE
+
+#include "consolid.h"
+
+#ifdef MV_DEBUG
+#define TUNE_CONSOLIDATE
+#endif
+
+#ifdef TUNE_CONSOLIDATE
+#define CONSOLIDATE_STATISTICS_COUNT	8
+static MV_U32 gConsolidateStatistics[CONSOLIDATE_STATISTICS_COUNT];
+typedef enum{
+	CONSOLIDATE_NOT_READ_WRITE,
+	CONSOLIDATE_REQUEST_TOO_BIG,
+	CONSOLIDATE_READ_WRITE_DIFFERENT,
+	CONSOLIDATE_NO_RUNNING_REQUEST,
+	CONSOLIDATE_LESS_THAN_SEQUENTIAL_THRESHOLD,
+	CONSOLIDATE_NO_RESOURCE,
+	CONSOLIDATE_GOT_PUSHED,
+	CONSOLIDATA_RESERVED0
+}Consolidate_Statistics_Enum;
+
+void UpdateConsolidateStatistics(Consolidate_Statistics_Enum catogory)
+{
+	MV_U8 i;
+	if ( gConsolidateStatistics[catogory]==0xFFFFFFFF )
+	{
+		for ( i=0; i<CONSOLIDATE_STATISTICS_COUNT; i++ )
+			MV_DPRINT(("Consolidate statistics[%d]=0x%x.\n", i,
+			gConsolidateStatistics[i]));
+		MV_ZeroMemory(gConsolidateStatistics, sizeof(MV_U32)*CONSOLIDATE_STATISTICS_COUNT);
+	}
+
+	gConsolidateStatistics[catogory]++;
+}
+#else
+#define UpdateConsolidateStatistics(x)
+#endif
+
+/*
+ * Instruction: How to plug-in this command consolidate sub module to your own module.
+ * 1. Include one .h file which supplies some helper funtions like CONS_GET_EXTENSION
+ * 2. Allocate memory resouce for Consolidate_Extension and Consolidate_Device
+ * 3. Initialize command consolidate module.
+ * 	Call Consolid_InitializeExtension to initialize Consolidate_Extension
+ *	Call Consolid_InitializeDevice for each Consolidate_Device
+ * 4. When you request comes call Consolid_ModuleSendRequest
+ * 5. At proper time, please push command consolidate module.
+ *	Sometimes command consolidate is accumulating requests and hasn't sent this internal request,
+ *	if there is nothing running now, just push this internal request out.
+ */
+#include "core_cons.h"
+
+PMV_Request Consolid_GetInternalRequest(MV_PVOID This);
+void Consolid_InitialInternalRequest(MV_PVOID This, PMV_Request, MV_BOOLEAN);
+
+void Consolid_ConsolidateRequest(PConsolidate_Extension, PMV_Request, PMV_Request);
+void Consolid_CloseRequest(PConsolidate_Extension, PConsolidate_Device, PMV_Request);
+
+void Consolid_RequestCallBack(MV_PVOID This, PMV_Request pReq);
+
+#ifdef _OS_LINUX
+#include "hba_timer.h"
+void CONS_SEND_REQUEST(MV_PVOID this, PMV_Request req)
+{
+	__hba_dump_req_info(10, req);
+	Core_InternalSendRequest(this, req);
+}
+
+static MV_VOID __consolid_timer_handler(MV_PVOID data)
+{
+	PMV_Request req = (PMV_Request)data;
+	MV_U8 dev_id = req->Device_Id;
+	PConsolidate_Device cons_dev =
+		CONS_GET_DEVICE(req->Cmd_Initiator, dev_id);
+	MV_U32 sector = req->Data_Transfer_Length>>9;
+
+	SCSI_CDB10_SET_SECTOR(req->Cdb, sector);
+	cons_dev->Holding_Request = NULL;
+	CONS_SEND_REQUEST(req->Cmd_Initiator, req);
+}
+
+static MV_VOID __consolid_mod_timer(MV_PVOID data, PMV_Request req)
+{
+	req->Cmd_Initiator = data;
+	if (req->cons_timeout == NULL) {
+		__consolid_timer_handler(req);
+	}
+	__hba_mod_timer(req->cons_timeout, 10, req, __consolid_timer_handler);
+}
+
+static MV_VOID __consolid_del_timer(PMV_Request req)
+{
+	if (req->cons_timeout) {
+		__hba_del_timer(req->cons_timeout);
+		req->cons_timeout = NULL;
+	}
+}
+#endif
+
+/*
+ * Consolidate sub-module has got a request.
+ * Two parameters:
+ * This: is the pointer of the command initiator extention pointer.
+ * pReq: request
+ * Will send:
+ *		a. one internal request
+ *		b. this external request and maybe one holding internal request if exists.
+ *		c. NULL if consolidate module holds this request.
+ */
+void Consolid_ModuleSendRequest(MV_PVOID This, PMV_Request pReq)
+{
+	PConsolidate_Extension pCons = CONS_GET_EXTENSION(This);
+	MV_U16 deviceId = pReq->Device_Id;
+	PConsolidate_Device pConsDevice = NULL;
+	PMV_Request pInternal = NULL;
+	MV_LBA startLBA;
+	MV_U32 sectorCount;
+
+	if ( deviceId>=MAX_DEVICE_NUMBER )
+	{
+		goto return_original_req;
+	}
+	pConsDevice = CONS_GET_DEVICE(This, deviceId);
+
+	if( pReq->Req_Flag & REQ_FLAG_NO_CONSOLIDATE )
+	{
+		goto return_original_req;
+	}
+
+	/*
+	 * We only handle CDB 10 read/write.
+	 * Otherwise, change the following code which gets the LBA and Sector Count from the CDB.
+	 */
+	if ( (pReq->Cdb[0]!=SCSI_CMD_READ_10)&&(pReq->Cdb[0]!=SCSI_CMD_WRITE_10) )
+	{
+		UpdateConsolidateStatistics(CONSOLIDATE_NOT_READ_WRITE);
+		goto return_original_req;
+	}
+
+	/* It's read/write request. But is it too big for command consolidate */
+	if ( pReq->Data_Transfer_Length>CONS_MAX_EXTERNAL_REQUEST_SIZE )
+	{
+		UpdateConsolidateStatistics(CONSOLIDATE_REQUEST_TOO_BIG);
+		goto return_original_req;
+	}
+
+	/* Check whether they are all read requests or write requests. */
+	if (
+		( (pReq->Cdb[0]==SCSI_CMD_READ_10)&&(!pConsDevice->Is_Read) )
+		||
+		( (pReq->Cdb[0]==SCSI_CMD_WRITE_10)&&(pConsDevice->Is_Read) )
+		)
+	{
+		UpdateConsolidateStatistics(CONSOLIDATE_READ_WRITE_DIFFERENT);
+		pConsDevice->Is_Read = (pReq->Cdb[0]==SCSI_CMD_READ_10)?1:0;
+		goto return_original_req;
+	}
+
+	/* Update the consolidate device statistic including last LBA and sequential counter. */
+	U64_SET_VALUE(startLBA, SCSI_CDB10_GET_LBA(pReq->Cdb));
+	sectorCount = SCSI_CDB10_GET_SECTOR(pReq->Cdb);
+	/* Check whether it's a sequential request. */
+	if ( U64_COMPARE_U64(startLBA, pConsDevice->Last_LBA) )
+		pConsDevice->Sequential = 0;
+	else
+		pConsDevice->Sequential++;	/* When equals, return 0. */
+
+	/* Last_LBA is actually the next expect sequential LBA. */
+	pConsDevice->Last_LBA = U64_ADD_U32(startLBA, sectorCount);
+	if ( pConsDevice->Sequential>CONS_SEQUENTIAL_MAX )	/* To avoid overflow */
+		pConsDevice->Sequential=CONS_SEQUENTIAL_THRESHOLD;
+
+	/* Is there any requests running on this device? If no, by pass. */
+	if ( !CONS_DEVICE_IS_BUSY(This, deviceId) )
+	{
+		UpdateConsolidateStatistics(CONSOLIDATE_NO_RUNNING_REQUEST);
+		goto return_original_req;
+	}
+#ifdef _OS_WINDOWS
+	/* Do we reach the sequential counter threshold? */
+	if ( pConsDevice->Sequential<CONS_SEQUENTIAL_THRESHOLD )
+	{
+		UpdateConsolidateStatistics(CONSOLIDATE_LESS_THAN_SEQUENTIAL_THRESHOLD);
+		goto return_original_req;
+	}
+#endif
+	pInternal = pConsDevice->Holding_Request;
+
+	/* Don't accumulate this request too big. */
+	if ( pInternal &&
+		( (pInternal->Data_Transfer_Length+pReq->Data_Transfer_Length>CONS_MAX_INTERNAL_REQUEST_SIZE)
+		  ||
+		  (pInternal->SG_Table.Valid_Entry_Count+pReq->SG_Table.Valid_Entry_Count>pInternal->SG_Table.Max_Entry_Count)
+		)
+	   )
+	{
+		Consolid_CloseRequest(pCons, pConsDevice, pInternal);
+		CONS_SEND_REQUEST(This, pInternal);
+		pInternal = NULL;	/* After Consolid_CloseRequest, pConsDevice->Holding_Request==NULL */
+	}
+
+	/* Get one internal request if we don't have. */
+	if ( pConsDevice->Holding_Request==NULL )
+	{
+		pConsDevice->Holding_Request = Consolid_GetInternalRequest(This);
+	}
+	pInternal = pConsDevice->Holding_Request;
+
+	/* We are out of resource. */
+	if ( pInternal==NULL )
+	{
+		UpdateConsolidateStatistics(CONSOLIDATE_NO_RESOURCE);
+		goto return_original_req;
+	}
+
+	/* Now we should be able to do consolidate requests now. */
+	Consolid_ConsolidateRequest(pCons, pInternal, pReq);
+
+	/* Is this internal request bigger enough to send? */
+	if ( pInternal->Data_Transfer_Length>=CONS_MIN_INTERNAL_REQUEST_SIZE )
+	{
+		Consolid_CloseRequest(pCons, pConsDevice, pInternal);
+		CONS_SEND_REQUEST(This, pInternal);
+		return;	/* Send this internal request. */
+	}
+	else
+	{
+#ifdef _OS_LINUX
+		__consolid_mod_timer(This, pInternal);
+#endif
+		return;	/* Hold this request. */
+	}
+
+return_original_req:
+	/*
+	 * To keep the command order,
+	 * if we cannot do the consolidate for pReq but we hold some internal request,
+	 * run the internal request and then run the new pReq.
+	 */
+	if ( pConsDevice && (pConsDevice->Holding_Request) )
+	{
+		pInternal = pConsDevice->Holding_Request;
+		Consolid_CloseRequest(pCons, pConsDevice, pInternal);
+		/* After Consolid_CloseRequest, pConsDevice->Holding_Request is NULL. */
+		CONS_SEND_REQUEST(This, pInternal);
+	}
+	CONS_SEND_REQUEST(This, pReq);
+	return;
+}
+
+PMV_Request Consolid_GetInternalRequest(MV_PVOID This)
+{
+	PConsolidate_Extension pCons = CONS_GET_EXTENSION(This);
+	PMV_Request pReq = NULL;
+	if ( !List_Empty(&pCons->Free_Queue) )
+		pReq = List_GetFirstEntry(&pCons->Free_Queue, MV_Request, Queue_Pointer);
+
+	/* Let's intialize this request */
+	if ( pReq )
+		Consolid_InitialInternalRequest(This, pReq, MV_FALSE);
+
+	return pReq;
+}
+
+void Consolid_ReleaseInternalRequest(PConsolidate_Extension pCons, PMV_Request pReq)
+{
+#ifdef _OS_LINUX
+	if (pReq->cons_timeout) {
+		__hba_del_timer(pReq->cons_timeout);
+		kfree(pReq->cons_timeout);
+		pReq->cons_timeout = NULL;
+	}
+#endif
+	List_AddTail(&pReq->Queue_Pointer, &pCons->Free_Queue);
+}
+
+void Consolid_ConsolidateRequest(
+	IN PConsolidate_Extension pCons,
+	IN OUT PMV_Request pInternal,
+	IN PMV_Request pExternal
+	)
+{
+	MV_U8 i;
+	PMV_Request pAttachedReq=NULL;
+
+	/* So far we only handle SCSI Read 10 and SCSI Write 10 */
+	MV_DASSERT( (pExternal->Cdb[0]==SCSI_CMD_READ_10) || (pExternal->Cdb[0]==SCSI_CMD_WRITE_10) );
+	pAttachedReq = (PMV_Request) pInternal->Org_Req;
+
+	if ( pInternal->Data_Transfer_Length==0 )
+	{
+		/* One external request is attached to that yet. */
+		pInternal->Device_Id = pExternal->Device_Id;
+		MV_DASSERT( pAttachedReq==NULL );
+
+		pInternal->Org_Req = pExternal;
+		MV_LIST_HEAD_INIT( &pExternal->Queue_Pointer );
+
+		pInternal->Cdb[0] = pExternal->Cdb[0];	/* Command type */
+		pInternal->Cdb[2] = pExternal->Cdb[2];	/* Start LBA */
+		pInternal->Cdb[3] = pExternal->Cdb[3];
+		pInternal->Cdb[4] = pExternal->Cdb[4];
+		pInternal->Cdb[5] = pExternal->Cdb[5];
+
+		if ( pExternal->Cdb[0]==SCSI_CMD_READ_10 )
+		{
+			pInternal->Cmd_Flag = CMD_FLAG_DMA | CMD_FLAG_DATA_IN;
+		}
+		else
+		{
+			pInternal->Cmd_Flag = CMD_FLAG_DMA;
+		}
+	}
+	else
+	{
+		MV_DASSERT( pInternal->Device_Id==pExternal->Device_Id );
+		MV_DASSERT( pAttachedReq!=NULL );
+		List_AddTail(&pExternal->Queue_Pointer, &pAttachedReq->Queue_Pointer);
+	}
+
+	/* Don't set the sector count every time. Just before send, set the count. */
+	pInternal->Data_Transfer_Length += pExternal->Data_Transfer_Length;
+
+#ifdef USE_NEW_SGTABLE
+	sgdt_append_reftbl(
+		&pInternal->SG_Table,
+		&pExternal->SG_Table,
+		0,
+		pExternal->SG_Table.Byte_Count );
+
+#else
+	for ( i=0; i<pExternal->SG_Table.Valid_Entry_Count; i++ )
+	{
+		SGTable_Append(&pInternal->SG_Table,
+			pExternal->SG_Table.Entry_Ptr[i].Base_Address,
+			pExternal->SG_Table.Entry_Ptr[i].Base_Address_High,
+			pExternal->SG_Table.Entry_Ptr[i].Size);
+	}
+#endif
+}
+
+void Consolid_CloseRequest(
+	IN PConsolidate_Extension pCons,
+	IN PConsolidate_Device pConsDevice,
+	IN OUT PMV_Request pInternal
+	)
+{
+	/*
+	 * This internal request is ready for handling now.
+	 * Do whatever we need do before we send this request.
+	 */
+	MV_U32 sectorCount = pInternal->Data_Transfer_Length>>9;
+	MV_DASSERT(  pInternal->Data_Transfer_Length%512==0 );
+#ifdef _OS_LINUX
+	__consolid_del_timer(pInternal);
+#endif
+	SCSI_CDB10_SET_SECTOR(pInternal->Cdb, sectorCount);
+	pConsDevice->Holding_Request = NULL;
+}
+
+void Consolid_RequestCallBack(MV_PVOID This, PMV_Request pReq)
+{
+	PConsolidate_Extension pCons = CONS_GET_EXTENSION(This);
+	PConsolidate_Device pConsDevice = CONS_GET_DEVICE(This, pReq->Device_Id);
+	PMV_Request pExternal;
+	PMV_Request pAttachedReq = (PMV_Request) pReq->Org_Req;
+
+	if ( pReq->Scsi_Status==REQ_STATUS_SUCCESS )
+	{
+		/* Extract all the external requests. Update status and return. */
+		while ( !List_Empty(&pAttachedReq->Queue_Pointer) )
+		{
+			pExternal = List_GetFirstEntry(&pAttachedReq->Queue_Pointer, MV_Request, Queue_Pointer);
+			pExternal->Scsi_Status = REQ_STATUS_SUCCESS;
+			pExternal->Completion(pExternal->Cmd_Initiator, pExternal);
+		}
+		pAttachedReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		pAttachedReq->Completion(pAttachedReq->Cmd_Initiator, pAttachedReq);
+	}
+	else
+	{
+		/* Make sure we won't do consolidate again for these requests. */
+		pConsDevice->Sequential = 0;
+		MV_DPRINT(("Request error in consolidate.\n"));
+
+		/* If consolidate request has error, Re-send these original requests.
+		 * They go to the hardware directly. Bypass the consolidate module. */
+		while ( !List_Empty(&pAttachedReq->Queue_Pointer) )
+		{
+			pExternal = List_GetFirstEntry(&pAttachedReq->Queue_Pointer, MV_Request, Queue_Pointer);
+			pExternal->Req_Flag |= REQ_FLAG_NO_CONSOLIDATE;
+			pExternal->Scsi_Status = REQ_STATUS_PENDING;
+			CONS_SEND_REQUEST(This, pExternal);
+		}
+		CONS_SEND_REQUEST(This, pAttachedReq);
+	}
+
+	/* Release this request back to the pool. */
+	Consolid_ReleaseInternalRequest(pCons, pReq);
+}
+
+/* Initialize the command consolidate internal request. */
+void Consolid_InitialInternalRequest(
+	IN MV_PVOID This,
+	IN OUT PMV_Request pInternal,
+	IN MV_BOOLEAN firstTime
+	)
+{
+	/*
+	 * Link pointer:
+	 * When request is free, Queue_Pointer is linked together in the request pool queue.
+	 * When request is in use, Org_Req is pointer to the first external request.
+	 * This first external request uses Queue_Pointer to link other external requests.
+	 * We cannot use internal request's Queue_Pointer to link external requests.
+	 * Because after send to core driver, this pointer will be destroyed.
+	 */
+	pInternal->Org_Req = NULL;				/* Use Queue_Pointer as the linker */
+	pInternal->Req_Flag |= REQ_FLAG_CONSOLIDATE;
+	pInternal->Scsi_Status = REQ_STATUS_PENDING;
+	pInternal->Data_Transfer_Length = 0;
+	pInternal->Cmd_Flag = 0;
+	SGTable_Init(&pInternal->SG_Table, 0);
+#ifdef _OS_LINUX
+	if (firstTime == MV_FALSE) {
+		MV_ASSERT(pInternal->cons_timeout == NULL);
+		pInternal->cons_timeout = kmalloc(sizeof(struct timer_list),
+						GFP_ATOMIC);
+		if (pInternal->cons_timeout == NULL) {
+			MV_ASSERT(0);
+			return;
+		}
+		__hba_init_timer(pInternal->cons_timeout);
+		pInternal->Cmd_Initiator = This;
+	}
+#else
+	MV_ZeroMemory(pInternal->Context, sizeof(MV_PVOID)*MAX_POSSIBLE_MODULE_NUMBER);
+#endif
+	/*
+	 * Some variables only need initialization once.
+	 * It won't change no matter during the life time.
+	 */
+	if ( firstTime )
+	{
+		pInternal->Device_Id = 0;
+		pInternal->Tag = 0;						/* Haven't used. */
+		pInternal->Cmd_Initiator = This;
+		pInternal->Sense_Info_Buffer_Length = 0;
+		pInternal->Sense_Info_Buffer = NULL;
+		pInternal->Data_Buffer = NULL;			/* After consolidate, virtual address is not valid. */
+#ifdef _OS_LINUX
+		memset(pInternal->Context, 0, sizeof(MV_PVOID)*MAX_POSSIBLE_MODULE_NUMBER);
+#endif
+#ifdef _OS_WINDOWS
+		pInternal->Context[MODULE_CORE] = NULL;
+#endif
+		pInternal->Completion = Consolid_RequestCallBack;
+		MV_LIST_HEAD_INIT(&pInternal->Queue_Pointer);
+		MV_ZeroMemory(pInternal->Cdb, MAX_CDB_SIZE);
+		U64_SET_VALUE(pInternal->LBA, 0);
+		pInternal->Sector_Count = 0;
+#ifdef _OS_LINUX
+		pInternal->cons_timeout = NULL;
+#endif
+	}
+}
+
+/* Initialize the Consolidate_Extension */
+void Consolid_InitializeExtension(MV_PVOID This)
+{
+	PConsolidate_Extension pCons = CONS_GET_EXTENSION(This);
+	PMV_Request pReq;
+	MV_U32 i;
+
+	MV_LIST_HEAD_INIT(&pCons->Free_Queue);
+	for (i = 0; i < CONS_MAX_INTERNAL_REQUEST_COUNT; i++) {
+		pReq = &pCons->Requests[i];
+
+		Consolid_InitialInternalRequest(This, pReq, MV_TRUE);
+		List_AddTail(&pReq->Queue_Pointer, &pCons->Free_Queue);
+	}
+}
+
+/*
+ * Initialize the Consolidate_Device.
+ * I don't initialize all the devices at once.
+ * Caller should call device one by one.
+ * So in this way, consolidate module doesn't all the Consolidate_Device are together
+ * or they are embedded in some caller data structure.
+ * One more advantage is that caller itself can map the Device_Id to related Consolidate_Device buffer.
+ * We don't need contiguous Device_Id.
+ */
+void Consolid_InitializeDevice(MV_PVOID This, MV_U16 Device_Id)
+{
+	PConsolidate_Device pConsDevice = CONS_GET_DEVICE(This, Device_Id);
+
+	MV_ZeroMemory(pConsDevice, sizeof(Consolidate_Device));
+}
+
+
+/*
+ * Caller pushes us to send the holding request if any.
+ */
+void
+Consolid_PushSendRequest(
+	MV_PVOID This,
+	MV_U16 Device_Id
+	)
+{
+	PConsolidate_Extension pCons = CONS_GET_EXTENSION(This);
+	PConsolidate_Device pConsDevice = CONS_GET_DEVICE(This, Device_Id);
+	PMV_Request pInternal = pConsDevice->Holding_Request;
+
+	if ( pInternal==NULL ) return;
+
+	UpdateConsolidateStatistics(CONSOLIDATE_GOT_PUSHED);
+
+	Consolid_CloseRequest(pCons, pConsDevice, pInternal);
+	/* After Consolid_CloseRequest pConsDevice->Holding_Request is NULL. */
+	CONS_SEND_REQUEST(This, pInternal);
+}
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/consolid.h
@@ -0,0 +1,60 @@
+#ifndef _CONSOLIDATE_H
+#define _CONSOLIDATE_H
+
+/*
+ * Here is the definition for the command consolidate sub module
+ * This is only changed when we modify the consolidate algorithm.
+ */
+#define CONS_MAX_INTERNAL_REQUEST_COUNT	32
+
+#define CONS_MAX_EXTERNAL_REQUEST_SIZE	(1024*64)
+#define CONS_SEQUENTIAL_MAX				0x7FFF		/* Avoid overflow. It's determined by Sequential variable size */
+#define CONS_SEQUENTIAL_THRESHOLD		64			/* Must bigger than OS outstanding request. Refer to Consolid_RequestCallBack */
+
+#define CONS_MAX_INTERNAL_REQUEST_SIZE	(1024*128)	/* The maximum request size hardware can handle. */
+#if defined(_OS_LINUX) && defined(SUPPORT_RAID6)
+#define CONS_MIN_INTERNAL_REQUEST_SIZE	(1024*64)
+#else
+#define CONS_MIN_INTERNAL_REQUEST_SIZE	(1024*128)	/* We'll accumulate the request to this size and then fire. */
+#endif
+
+typedef struct _Consolidate_Extension
+{
+	MV_Request	Requests[CONS_MAX_INTERNAL_REQUEST_COUNT];
+	List_Head	Free_Queue;
+}Consolidate_Extension, *PConsolidate_Extension;
+
+typedef struct _Consolidate_Device
+{
+	MV_LBA		Last_LBA;				/* last LBA*/
+	PMV_Request Holding_Request;		/* Internal request which already consolidate some external requests. */
+	MV_U16		Sequential;				/* sequential counter */
+	MV_BOOLEAN	Is_Read;				/* The last request is read or write. */
+	MV_U8		Reserved0;
+	MV_U16		Reserved1[2];
+}Consolidate_Device, *PConsolidate_Device;
+
+void
+Consolid_ModuleSendRequest(
+	MV_PVOID This,
+	PMV_Request pReq
+	);
+
+void
+Consolid_InitializeExtension(
+	MV_PVOID This
+	);
+
+void
+Consolid_InitializeDevice(
+	MV_PVOID This,
+	MV_U16 Device_Id
+	);
+
+void
+Consolid_PushSendRequest(
+	MV_PVOID This,
+	MV_U16 Device_Id
+	);
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_api.c
@@ -0,0 +1,710 @@
+#include "mv_include.h"
+
+#ifdef CORE_SUPPORT_API
+
+#include "core_api.h"
+#include "core_exp.h"
+#include "core_inter.h"
+#include "core_init.h"
+#include "com_error.h"
+
+typedef MV_BOOLEAN (*CORE_Management_Command_Handler)(PCore_Driver_Extension, PMV_Request);
+CORE_Management_Command_Handler core_pd_cmd_handler[];
+
+MV_BOOLEAN
+Core_MapHDId(
+	IN PCore_Driver_Extension pCore,
+	IN MV_U16 HDId,
+	OUT MV_PU8 portId,
+	OUT MV_PU8 deviceId
+	);
+
+MV_VOID
+Core_GetHDInformation(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice,
+	OUT PHD_Info pHD
+	);
+
+MV_VOID
+Core_GetExpInformation(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice,
+	OUT PExp_Info pExp
+	);
+
+MV_VOID
+Core_GetPMInformation(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	OUT PPM_Info pPM
+	);
+
+MV_VOID
+Core_GetHDConfiguration(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice,
+	OUT PHD_Config pHD
+	);
+
+MV_VOID
+Core_SetHDConfiguration(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice,
+	IN PHD_Config pHD
+	);
+
+/*
+ * Exposed Functions
+ */
+MV_VOID
+Core_GetHDInfo(
+	IN MV_PVOID extension,
+	IN MV_U16 HDId,
+	OUT PHD_Info pHD
+	)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)extension;
+	MV_U16 startId, endId;
+	MV_U8 portId, deviceId;
+	PDomain_Port pPort = NULL;
+	PDomain_Device pDevice = NULL;
+	MV_U16 i;
+
+	if ( HDId==0xFF )	/* Get all the HD information */
+	{
+		/* First set invalid flag in buffer */
+		for (i=0; i<MAX_DEVICE_SUPPORTED; i++) {
+			pHD[i].Link.Self.DevID = i;
+			pHD[i].Link.Self.DevType = DEVICE_TYPE_NONE;
+		}
+		startId = 0;
+		endId = MV_MAX_HD_DEVICE_ID-1;
+	}
+	else
+	{
+		startId = HDId;
+		endId = HDId;
+	}
+
+	for ( i=startId; i<=endId; i++ )
+	{
+		if ( Core_MapHDId(pCore, i, &portId, &deviceId) )
+		{
+			pPort = &pCore->Ports[portId];
+			pDevice = &pPort->Device[deviceId];
+			Core_GetHDInformation( pCore, pPort, pDevice, pHD );
+		}
+/*
+		else
+		{
+			pHD->ID = i;
+			pHD->Type = DEVICE_TYPE_NONE;
+		}
+*/
+		//MV_DASSERT( pHD->Id==i );
+		pHD++;
+	}
+}
+
+#ifdef SUPPORT_PM
+MV_VOID
+Core_GetExpInfo(
+	IN MV_PVOID extension,
+	IN MV_U16 ExpId,
+	OUT PExp_Info pExp
+	)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)extension;
+	MV_U16 startId, endId;
+	MV_U8 portId, deviceId;
+	PDomain_Port pPort = NULL;
+	PDomain_Device pDevice = NULL;
+	MV_U16 i;
+
+	if ( ExpId==0xFF )
+	{
+		/* Get all the HD information */
+		startId = 0;
+		if (MAX_EXPANDER_SUPPORTED > 0)
+			endId = MAX_EXPANDER_SUPPORTED-1;
+		else
+			endId = 0;
+	}
+	else
+	{
+		startId = ExpId;
+		endId = ExpId;
+	}
+
+	for ( i=startId; i<=endId; i++ )
+	{
+		if ( Core_MapHDId(pCore, i, &portId, &deviceId) )
+		{
+			pPort = &pCore->Ports[portId];
+			pDevice = &pPort->Device[deviceId];
+			Core_GetExpInformation( pCore, pPort, pDevice, pExp );
+		}
+		else
+		{
+			pExp->Link.Self.DevID = i;
+			pExp->Link.Self.DevType = DEVICE_TYPE_NONE;
+		}
+		pExp++;
+	}
+}
+
+MV_VOID
+Core_GetPMInfo(
+	IN MV_PVOID extension,
+	IN MV_U16 PMId,
+	OUT PPM_Info pPM
+	)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)extension;
+	MV_U16 startId, endId;
+	PDomain_Port pPort = NULL;
+	MV_U16 i;
+
+	if ( PMId==0xFF )
+	{
+		/* Get all the PM information */
+		startId = 0;
+		endId = pCore->Port_Num - 1;
+	}
+	else
+	{
+		startId = PMId;
+		endId = PMId;
+	}
+
+	for ( i=startId; i<=endId; i++ )
+	{
+		pPort = &pCore->Ports[i];
+		if ( pPort->Type != PORT_TYPE_PM )
+		{
+			if ( PMId != 0xFF )
+			{
+				// not a PM, return error
+			}
+		}
+		else
+		{
+			Core_GetPMInformation( pCore, pPort, pPM );
+			pPM++;
+		}
+	}
+}
+#endif	/* #ifdef SUPPORT_PM */
+
+MV_VOID
+Core_GetHDConfig(
+	IN MV_PVOID extension,
+	IN MV_U16 HDId,
+	OUT PHD_Config pHD
+	)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)extension;
+	MV_U16 startId, endId;
+	MV_U8 portId, deviceId;
+	PDomain_Port pPort = NULL;
+	PDomain_Device pDevice = NULL;
+	MV_U16 i;
+
+	/* Get all the HD configuration */
+	if ( HDId==0xFF )
+	{
+		/* First set invalid flag in buffer */
+		for (i=0; i<MAX_DEVICE_SUPPORTED; i++)
+			pHD[i].HDID = 0xFF;
+		startId = 0;
+		endId = MV_MAX_HD_DEVICE_ID-1;
+	}
+	else
+	{
+		startId = HDId;
+		endId = HDId;
+	}
+
+
+	for ( i=startId; i<=endId; i++ )
+	{
+		if ( Core_MapHDId(pCore, i, &portId, &deviceId) )
+		{
+			pPort = &pCore->Ports[portId];
+			pDevice = &pPort->Device[deviceId];
+			Core_GetHDConfiguration( pCore, pPort, pDevice, pHD );
+		}
+		pHD++;
+	}
+}
+/*
+ * Internal Functions
+ */
+MV_BOOLEAN
+Core_MapHDId(
+	IN PCore_Driver_Extension pCore,
+	IN MV_U16 HDId,
+	OUT MV_PU8 portId,
+	OUT MV_PU8 deviceId
+	)
+{
+	if ( portId ) *portId = PATA_MapPortId(HDId);
+	if ( deviceId ) *deviceId = PATA_MapDeviceId(HDId);
+
+	if ( ((portId)&&(*portId>=MAX_PORT_NUMBER))
+		|| ((deviceId)&&(*deviceId>=MAX_DEVICE_PER_PORT))
+		)
+		return MV_FALSE;
+	else
+		return MV_TRUE;
+}
+
+MV_VOID
+Core_GetHDInformation(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice,
+	OUT PHD_Info pHD
+	)
+{
+	pHD->Link.Self.DevID = pDevice->Id ;
+	if ( !(pDevice->Status&DEVICE_STATUS_FUNCTIONAL) )
+	{
+		pHD->Link.Self.DevType = DEVICE_TYPE_NONE;
+		return;
+	}
+
+	// check if device type is correct; if not, generate sense
+	pHD->Link.Self.DevType = DEVICE_TYPE_HD;
+	pHD->Link.Self.PhyCnt = 1;
+	pHD->Link.Self.PhyID[0] = 0;
+
+	pHD->Link.Parent.DevID = pPort->Id;
+	pHD->Link.Parent.PhyCnt = 1;
+	if ( pPort->Type==PORT_TYPE_PM )
+	{
+		pHD->Link.Parent.DevType = DEVICE_TYPE_PM;
+		pHD->Link.Parent.PhyID[0] = pDevice->PM_Number;
+	}
+	else
+	{
+		pHD->Link.Parent.DevType = DEVICE_TYPE_PORT;
+		pHD->Link.Parent.PhyID[0] = pPort->Id;
+	}
+
+	pHD->Status = 0;
+	if ( pPort->Type==PORT_TYPE_PATA )
+		pHD->HDType = HD_TYPE_PATA;
+	else
+		pHD->HDType = HD_TYPE_SATA;
+	if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+		pHD->HDType |= HD_TYPE_ATAPI;
+
+	pHD->PIOMode = pDevice->PIO_Mode;
+	pHD->MDMAMode = pDevice->MDMA_Mode;
+	pHD->UDMAMode = pDevice->UDMA_Mode;
+	pHD->CurrentPIOMode = pDevice->Current_PIO;
+	pHD->CurrentMDMAMode = pDevice->Current_MDMA;
+	pHD->CurrentUDMAMode = pDevice->Current_UDMA;
+
+	if ( pDevice->Capacity & DEVICE_CAPACITY_NCQ_SUPPORTED )
+		pHD->FeatureSupport |= HD_FEATURE_NCQ;
+	if ( pDevice->Capacity & DEVICE_CAPACITY_WRITECACHE_SUPPORTED )
+		pHD->FeatureSupport |= HD_FEATURE_WRITE_CACHE;
+	if ( pDevice->Capacity & DEVICE_CAPACITY_48BIT_SUPPORTED )
+		pHD->FeatureSupport |= HD_FEATURE_48BITS;
+	if ( pDevice->Capacity & DEVICE_CAPACITY_SMART_SUPPORTED )
+		pHD->FeatureSupport |= HD_FEATURE_SMART;
+
+	if ( pDevice->Capacity & DEVICE_CAPACITY_RATE_1_5G )
+		pHD->FeatureSupport |= HD_FEATURE_1_5G;
+	else if ( pDevice->Capacity & DEVICE_CAPACITY_RATE_3G )
+		pHD->FeatureSupport |= HD_FEATURE_3G;
+
+	MV_CopyMemory(pHD->Model, pDevice->Model_Number, 40);
+	MV_CopyMemory(pHD->SerialNo, pDevice->Serial_Number, 20);
+	MV_CopyMemory(pHD->FWVersion, pDevice->Firmware_Revision, 8);
+
+	*(MV_PU32)pHD->WWN = pDevice->WWN;
+	pHD->Size = U64_ADD_U32(pDevice->Max_LBA, 1);
+}
+
+#ifdef SUPPORT_PM
+MV_VOID
+Core_GetExpInformation(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice,
+	OUT PExp_Info pExp
+	)
+{
+}
+
+MV_VOID
+Core_GetPMInformation(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	OUT PPM_Info pPM
+	)
+{
+#ifndef RAID_SIMULATE_CONFIGURATION
+	pPM->Link.Self.DevType = DEVICE_TYPE_PM;
+	pPM->Link.Self.DevID = pPort->Id;
+	pPM->Link.Self.PhyCnt = 1;
+	pPM->Link.Self.PhyID[0] = 0;
+
+	pPM->Link.Parent.DevType = DEVICE_TYPE_PORT;
+	pPM->Link.Parent.DevID = 0;
+	pPM->Link.Parent.PhyCnt = 1;
+	pPM->Link.Parent.PhyID[0] = pPort->Id;
+
+	pPM->VendorId = pPort->PM_Vendor_Id;
+	pPM->DeviceId = pPort->PM_Device_Id;
+	pPM->ProductRevision = pPort->PM_Product_Revision;
+	pPM->PMSpecRevision = pPort->PM_Spec_Revision;
+	pPM->NumberOfPorts = pPort->PM_Num_Ports;
+#endif
+}
+#endif	/* #ifdef SUPPORT_PM */
+
+MV_VOID
+Core_GetHDConfiguration(
+	PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice,
+	OUT PHD_Config pHD
+	)
+{
+	if ( !(pDevice->Status & DEVICE_STATUS_FUNCTIONAL) )
+	{
+		pHD->HDID = 0xFF;
+		return;
+	}
+
+	pHD->HDID = pDevice->Id ;
+
+	if (pDevice->Setting & DEVICE_SETTING_WRITECACHE_ENABLED)
+		pHD->WriteCacheOn = MV_TRUE;
+	else
+		pHD->WriteCacheOn = MV_FALSE;
+
+	if ( pDevice->Setting & DEVICE_SETTING_SMART_ENABLED )
+		pHD->SMARTOn = MV_TRUE;
+	else
+		pHD->SMARTOn = MV_FALSE;
+
+	if (pDevice->Capacity & DEVICE_CAPACITY_RATE_3G)
+		pHD->DriveSpeed = HD_SPEED_3G;
+	else
+		pHD->DriveSpeed = HD_SPEED_1_5G;
+
+}
+
+MV_BOOLEAN core_pd_request_get_HD_info(PCore_Driver_Extension pCore, PMV_Request pMvReq)
+{
+	PHD_Info pHDInfo = (PHD_Info)pMvReq->Data_Buffer;
+	MV_U16 HDID;
+
+	MV_CopyMemory(&HDID, &pMvReq->Cdb[2], 2);
+	Core_GetHDInfo( pCore, HDID, pHDInfo );
+	if (HDID != 0xFF && pHDInfo->Link.Self.DevType == DEVICE_TYPE_NONE)
+	{
+		if (pMvReq->Sense_Info_Buffer != NULL)
+			((MV_PU8)pMvReq->Sense_Info_Buffer)[0] = ERR_INVALID_HD_ID;
+		pMvReq->Scsi_Status = REQ_STATUS_ERROR_WITH_SENSE;
+	}
+	else
+		pMvReq->Scsi_Status = REQ_STATUS_SUCCESS;
+	return MV_TRUE;
+}
+
+#ifdef SUPPORT_PM
+MV_BOOLEAN core_pd_request_get_expander_info( PCore_Driver_Extension pCore, PMV_Request pMvReq )
+{
+	PExp_Info pExpInfo = (PExp_Info)pMvReq->Data_Buffer;
+	MV_U16 ExpID;
+	MV_U8	status = REQ_STATUS_SUCCESS;
+
+	MV_CopyMemory(&ExpID, &pMvReq->Cdb[2], 2);
+	if (ExpID != 0xFF && ExpID > MAX_EXPANDER_SUPPORTED)
+	{
+		status = ERR_INVALID_EXP_ID;
+	}
+	else
+	{
+		Core_GetExpInfo( pCore, ExpID, pExpInfo );
+
+		if (ExpID != 0xFF && pExpInfo->Link.Self.DevType == DEVICE_TYPE_NONE)
+		{
+			status = ERR_INVALID_EXP_ID;
+		}
+	}
+
+	if (status != REQ_STATUS_SUCCESS)
+	{
+		if (pMvReq->Sense_Info_Buffer != NULL)
+			((MV_PU8)pMvReq->Sense_Info_Buffer)[0] = status;
+		pMvReq->Scsi_Status = REQ_STATUS_ERROR_WITH_SENSE;
+	}
+	else
+		pMvReq->Scsi_Status = REQ_STATUS_SUCCESS;
+
+	return MV_TRUE;
+}
+
+MV_BOOLEAN core_pd_request_get_PM_info( PCore_Driver_Extension pCore, PMV_Request pMvReq )
+{
+	PPM_Info pPMInfo = (PPM_Info)pMvReq->Data_Buffer;
+	MV_U16 PMID;
+
+	MV_CopyMemory(&PMID, &pMvReq->Cdb[2], 2);
+	if (PMID != 0xFF && PMID > MAX_PM_SUPPORTED)
+	{
+		if (pMvReq->Sense_Info_Buffer != NULL)
+			((MV_PU8)pMvReq->Sense_Info_Buffer)[0] = ERR_INVALID_PM_ID;
+		pMvReq->Scsi_Status = REQ_STATUS_ERROR_WITH_SENSE;
+		return MV_TRUE;
+	}
+
+	Core_GetPMInfo( pCore, PMID, pPMInfo );
+
+	pMvReq->Scsi_Status = REQ_STATUS_SUCCESS;
+	return MV_TRUE;
+}
+#endif	/*#ifdef SUPPORT_PM */
+
+MV_BOOLEAN core_pd_request_get_HD_config( PCore_Driver_Extension pCore, PMV_Request pMvReq )
+{
+	PHD_Config pHDConfig = (PHD_Config)pMvReq->Data_Buffer;
+	MV_U16 ConfigID;
+
+	MV_CopyMemory(&ConfigID, &pMvReq->Cdb[2], 2);
+	Core_GetHDConfig( pCore, ConfigID, pHDConfig );
+
+	if (ConfigID != 0xFF && pHDConfig->HDID == 0xFF)
+	{
+		if (pMvReq->Sense_Info_Buffer != NULL)
+			((MV_PU8)pMvReq->Sense_Info_Buffer)[0] = ERR_INVALID_HD_ID;
+		pMvReq->Scsi_Status = REQ_STATUS_ERROR_WITH_SENSE;
+	}
+	else
+		pMvReq->Scsi_Status = REQ_STATUS_SUCCESS;
+
+	return MV_TRUE;
+}
+
+MV_BOOLEAN core_pd_request_get_HD_status( PCore_Driver_Extension pCore, PMV_Request pMvReq )
+{
+	PHD_Status pHDStatus = (PHD_Status)pMvReq->Data_Buffer;
+	PDomain_Port pPort = NULL;
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId;
+	MV_U16 HDId;
+	MV_U8	cacheMode = 0;
+	MV_U8 status = REQ_STATUS_SUCCESS;
+
+	MV_CopyMemory(&HDId, &pMvReq->Cdb[2], 2);
+
+	if ( Core_MapHDId(pCore, HDId, &portId, &deviceId) )
+	{
+		pPort = &pCore->Ports[portId];
+		pDevice = &pPort->Device[deviceId];
+
+		if (pDevice->Setting & DEVICE_SETTING_SMART_ENABLED)
+		{
+			if ( !(pDevice->Status & DEVICE_STATUS_FUNCTIONAL) )
+			{
+				status = ERR_INVALID_HD_ID;
+			}
+			else
+			{
+				if (pMvReq->Cdb[4] == APICDB4_PD_SMART_RETURN_STATUS)
+				{
+					cacheMode = CDB_CORE_SMART_RETURN_STATUS;
+				}
+				else
+					status = ERR_INVALID_REQUEST;
+			}
+
+			if (status == REQ_STATUS_SUCCESS)
+			{
+				// Convert it into SCSI_CMD_MARVELL_SPECIFIC request.
+				pMvReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+				pMvReq->Cdb[1] = CDB_CORE_MODULE;
+				pMvReq->Cdb[2] = cacheMode;
+				pMvReq->Device_Id = pDevice->Id;
+				if (pHDStatus)
+					pHDStatus->HDID = pDevice->Id;
+			}
+		}
+		else
+			status = ERR_INVALID_REQUEST;
+	}
+	else
+	{
+		status = ERR_INVALID_HD_ID;
+	}
+
+	if (status != REQ_STATUS_SUCCESS)
+	{
+		if (pMvReq->Sense_Info_Buffer != NULL)
+			((MV_PU8)pMvReq->Sense_Info_Buffer)[0] = status;
+		pMvReq->Scsi_Status = REQ_STATUS_ERROR_WITH_SENSE;
+		return MV_TRUE;
+	}
+	else
+	{
+		pMvReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		return MV_FALSE;	// Need to access hardware.
+	}
+}
+
+MV_BOOLEAN core_pd_request_set_HD_config( PCore_Driver_Extension pCore, PMV_Request pMvReq )
+{
+	//PHD_Config pHDConfig = (PHD_Config)pMvReq->Data_Buffer;
+	PDomain_Port pPort = NULL;
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId;
+	MV_U16 HDId;
+	MV_U8	cacheMode = 0;
+	MV_U8 status = REQ_STATUS_PENDING;
+
+	MV_CopyMemory(&HDId, &pMvReq->Cdb[2], 2);
+
+	if ( Core_MapHDId(pCore, HDId, &portId, &deviceId) )
+	{
+		pPort = &pCore->Ports[portId];
+		pDevice = &pPort->Device[deviceId];
+
+		if ( !(pDevice->Status & DEVICE_STATUS_FUNCTIONAL) )
+		{
+			status = ERR_INVALID_HD_ID;
+		}
+		else if ( pDevice->Device_Type & DEVICE_TYPE_ATAPI )
+		{
+			status = ERR_INVALID_REQUEST;
+		}
+		else
+		{
+			if (pMvReq->Cdb[4] == APICDB4_PD_SET_WRITE_CACHE_OFF)
+			{
+				cacheMode = CDB_CORE_DISABLE_WRITE_CACHE;
+			}
+			else if (pMvReq->Cdb[4] == APICDB4_PD_SET_WRITE_CACHE_ON)
+			{
+				cacheMode = CDB_CORE_ENABLE_WRITE_CACHE;
+			}
+			else if (pMvReq->Cdb[4] == APICDB4_PD_SET_SMART_OFF)
+			{
+				if ( !(pDevice->Setting&DEVICE_SETTING_SMART_ENABLED) )
+					status = REQ_STATUS_SUCCESS;
+				cacheMode = CDB_CORE_DISABLE_SMART;
+			}
+			else if (pMvReq->Cdb[4] == APICDB4_PD_SET_SMART_ON)
+			{
+				if ( pDevice->Setting&DEVICE_SETTING_SMART_ENABLED )
+					status = REQ_STATUS_SUCCESS;
+				cacheMode = CDB_CORE_ENABLE_SMART;
+			}
+			else
+			{
+				status = ERR_INVALID_REQUEST;
+			}
+		}
+
+		if (status == REQ_STATUS_PENDING)
+		{
+			// Convert it into SCSI_CMD_MARVELL_SPECIFIC request.
+			pMvReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+			pMvReq->Cdb[1] = CDB_CORE_MODULE;
+			pMvReq->Cdb[2] = cacheMode;
+			pMvReq->Device_Id = pDevice->Id;
+		}
+	}
+	else
+	{
+		status = ERR_INVALID_HD_ID;
+	}
+
+	if (status == REQ_STATUS_SUCCESS)
+	{
+		pMvReq->Scsi_Status = status;
+		return MV_TRUE;
+	}
+	else if (status == REQ_STATUS_PENDING)
+	{
+		//pMvReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		return MV_FALSE;	// Need to access hardware.
+	}
+	else
+	{
+		if (pMvReq->Sense_Info_Buffer != NULL)
+			((MV_PU8)pMvReq->Sense_Info_Buffer)[0] = status;
+		pMvReq->Scsi_Status = REQ_STATUS_ERROR_WITH_SENSE;
+		return MV_TRUE;
+	}
+}
+
+MV_BOOLEAN core_pd_request_BSL_dump( PCore_Driver_Extension pCore, PMV_Request pMvReq )
+{
+	pMvReq->Scsi_Status = REQ_STATUS_ERROR;
+	return MV_TRUE;
+}
+
+MV_BOOLEAN core_pd_request_HD_MP_check( PCore_Driver_Extension pCore, PMV_Request pMvReq )
+{
+	pMvReq->Scsi_Status = REQ_STATUS_ERROR;
+	return MV_TRUE;
+}
+
+MV_BOOLEAN core_pd_request_HD_get_MP_status( PCore_Driver_Extension pCore, PMV_Request pMvReq )
+{
+	pMvReq->Scsi_Status = REQ_STATUS_ERROR;
+	return MV_TRUE;
+}
+
+CORE_Management_Command_Handler BASEATTR core_pd_cmd_handler[APICDB1_PD_MAX] =
+{
+	core_pd_request_get_HD_info,
+#ifdef SUPPORT_PM
+	core_pd_request_get_expander_info,
+	core_pd_request_get_PM_info,
+#else
+	NULL,
+	NULL,
+#endif
+	core_pd_request_get_HD_config,
+	core_pd_request_set_HD_config,
+	core_pd_request_BSL_dump,
+	core_pd_request_HD_MP_check,
+	core_pd_request_HD_get_MP_status,
+	core_pd_request_get_HD_status,
+	NULL
+};
+
+MV_BOOLEAN
+Core_pd_command(
+	IN MV_PVOID extension,
+	IN PMV_Request pReq
+	)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)extension;
+
+	if ( pReq->Cdb[1] >= APICDB1_PD_MAX )
+	{
+		pReq->Scsi_Status = REQ_STATUS_INVALID_PARAMETER;
+		return MV_TRUE;
+	}
+	if (core_pd_cmd_handler[pReq->Cdb[1]] == NULL)
+	{
+		pReq->Scsi_Status = REQ_STATUS_INVALID_PARAMETER;
+		return MV_TRUE;
+	}
+	return core_pd_cmd_handler[pReq->Cdb[1]](pCore, pReq);
+}
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_api.h
@@ -0,0 +1,44 @@
+#if !defined(CORE_API_H)
+#define CORE_API_H
+
+#ifdef CORE_SUPPORT_API
+
+#define HD_WRITECACHE_OFF		0
+#define HD_WRITECACHE_ON		1
+
+MV_VOID
+Core_GetHDInfo(
+	IN MV_PVOID extension,
+	IN MV_U16 HDId,
+	OUT PHD_Info pHD );
+
+MV_VOID
+Core_GetExpInfo(
+	IN MV_PVOID extension,
+	IN MV_U16 ExpId,
+	OUT PExp_Info pExp
+	);
+
+MV_VOID
+Core_GetPMInfo(
+	IN MV_PVOID extension,
+	IN MV_U16 PMId,
+	OUT PPM_Info pPM
+	);
+
+MV_VOID
+Core_GetHDConfig(
+	IN MV_PVOID extension,
+	IN MV_U16 HDId,
+	OUT PHD_Config pHD
+	);
+
+MV_BOOLEAN
+Core_pd_command(
+	IN MV_PVOID extension,
+	IN PMV_Request pReq
+	);
+
+#endif
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_ata.h
@@ -0,0 +1,265 @@
+#if !defined(CORE_ATA_H)
+#define CORE_ATA_H
+
+
+/*
+ * ATA IDE Command definition
+ */
+/* PIO command */
+#define ATA_CMD_READ_PIO				0x20
+#define ATA_CMD_READ_PIO_EXT			0x24
+#define ATA_CMD_READ_PIO_MULTIPLE_EXT 	0x29
+#define ATA_CMD_WRITE_PIO				0x30
+#define ATA_CMD_WRITE_PIO_EXT			0x34
+#define ATA_CMD_WRITE_PIO_MULTIPLE_EXT	0x39
+
+/* DMA read write command */
+#define ATA_CMD_READ_DMA				0xC8	/* 28 bit DMA read */
+#define ATA_CMD_READ_DMA_QUEUED			0xC7	/* 28 bit TCQ DMA read */
+#define ATA_CMD_READ_DMA_EXT			0x25	/* 48 bit DMA read */
+#define ATA_CMD_READ_DMA_QUEUED_EXT		0x26	/* 48 bit TCQ DMA read */
+#define ATA_CMD_READ_FPDMA_QUEUED		0x60	/* NCQ DMA read: SATA only. Always 48 bit */
+
+#define ATA_CMD_WRITE_DMA				0xCA
+#define ATA_CMD_WRITE_DMA_QUEUED		0xCC
+#define ATA_CMD_WRITE_DMA_EXT  			0x35
+#define ATA_CMD_WRITE_DMA_QUEUED_EXT	0x36
+#define ATA_CMD_WRITE_FPDMA_QUEUED		0x61
+
+/* Identify command */
+#define ATA_CMD_IDENTIFY_ATA			0xEC
+#define ATA_CMD_IDENTIY_ATAPI			0xA1
+
+#define ATA_CMD_VERIFY					0x40	/* 28 bit read verifty */
+#define ATA_CMD_VERIFY_EXT				0x42	/* 48 bit read verify */
+
+#define ATA_CMD_FLUSH					0xE7	/* 28 bit flush */
+#define ATA_CMD_FLUSH_EXT				0xEA	/* 48 bit flush */
+
+#define ATA_CMD_PACKET					0xA0
+#define ATA_CMD_SMART					0xB0
+	#define ATA_CMD_SMART_READ_DATA 					0xD0
+	#define ATA_CMD_SMART_ENABLE_ATTRIBUTE_AUTOSAVE	0xD2
+	#define ATA_CMD_SMART_EXECUTE_OFFLINE				0xD4
+	#define ATA_CMD_SMART_READ_LOG						0xD5
+	#define ATA_CMD_SMART_WRITE_LOG 					0xD6
+	#define ATA_CMD_ENABLE_SMART				0xD8
+	#define ATA_CMD_DISABLE_SMART				0xD9
+	#define ATA_CMD_SMART_RETURN_STATUS			0xDA
+
+#define ATA_CMD_SET_FEATURES			0xEF
+	#define ATA_CMD_ENABLE_WRITE_CACHE			0x02
+	#define ATA_CMD_SET_TRANSFER_MODE			0x03
+	#define ATA_CMD_DISABLE_READ_LOOK_AHEAD		0x55
+	#define ATA_CMD_DISABLE_WRITE_CACHE			0x82
+	#define ATA_CMD_ENABLE_READ_LOOK_AHEAD		0xAA
+
+#define ATA_CMD_STANDBY_IMMEDIATE		0xE0
+#define ATA_CMD_SEEK					0x70
+#define ATA_CMD_READ_LOG_EXT			0x2F
+#define ATA_CMD_DOWNLOAD_MICROCODE			0x92
+
+
+
+
+#ifdef SUPPORT_ATA_SECURITY_CMD
+
+/*	security commmand */
+#define ATA_CMD_SEC_PASSWORD			0xF1
+#define ATA_CMD_SEC_UNLOCK			0xF2
+#define ATA_CMD_SEC_ERASE_PRE			0xF3
+#define ATA_CMD_SEC_ERASE_UNIT			0xF4
+#define ATA_CMD_SEC_FREEZE_LOCK			0xF5
+#define ATA_CMD_SEC_DISABLE_PASSWORD		0xF6
+
+enum _ata_passthru_protocol {
+        ATA_PROTOCOL_HARD_RESET                 = 0x00,
+        ATA_PROTOCOL_SRST                       = 0x01,
+        ATA_PROTOCOL_NON_DATA                   = 0x03,
+        ATA_PROTOCOL_PIO_IN                     = 0x04,
+        ATA_PROTOCOL_PIO_OUT                    = 0x05,
+        ATA_PROTOCOL_DMA                        = 0x06,
+        ATA_PROTOCOL_DMA_QUEUED                 = 0x07,
+        ATA_PROTOCOL_DEVICE_DIAG                = 0x08,
+        ATA_PROTOCOL_DEVICE_RESET               = 0x09,
+        ATA_PROTOCOL_UDMA_IN                    = 0x0A,
+        ATA_PROTOCOL_UDMA_OUT                   = 0x0B,
+        ATA_PROTOCOL_FPDMA                      = 0x0C,
+        ATA_PROTOCOL_RTN_INFO                   = 0x0F,
+};
+#define ATA_CMD_READ_PIO				0x20
+#define ATA_CMD_READ_PIO_MULTIPLE                       0xC4
+#define ATA_CMD_READ_PIO_EXT			0x24
+#define ATA_CMD_READ_PIO_MULTIPLE_EXT 	0x29
+#define ATA_CMD_WRITE_PIO				0x30
+#define ATA_CMD_WRITE_PIO_MULTIPLE                      0xC5
+#define ATA_CMD_WRITE_PIO_EXT			0x34
+#define ATA_CMD_WRITE_PIO_MULTIPLE_EXT	0x39
+#define ATA_CMD_WRITE_PIO_MULTIPLE_FUA_EXT              0xCE
+
+#define IS_ATA_MULTIPLE_READ_WRITE(x) \
+        ((x == ATA_CMD_READ_PIO_MULTIPLE)\
+        ||(x == ATA_CMD_READ_PIO_MULTIPLE_EXT)\
+        ||(x == ATA_CMD_WRITE_PIO_MULTIPLE)\
+        ||(x == ATA_CMD_WRITE_PIO_MULTIPLE_EXT)\
+        ||(x == ATA_CMD_WRITE_PIO_MULTIPLE_FUA_EXT))
+
+enum {
+	SG_CDB2_TLEN_NODATA	= 0 << 0,
+	SG_CDB2_TLEN_FEAT	= 1 << 0,
+	SG_CDB2_TLEN_NSECT	= 2 << 0,
+
+	SG_CDB2_TLEN_BYTES	= 0 << 2,
+	SG_CDB2_TLEN_SECTORS	= 1 << 2,
+
+	SG_CDB2_TDIR_TO_DEV	= 0 << 3,
+	SG_CDB2_TDIR_FROM_DEV	= 1 << 3,
+
+	SG_CDB2_CHECK_COND	= 1 << 5,
+};
+#define SG_READ			0
+#define SG_WRITE		1
+#define SG_PIO			0
+#define SG_DMA			1
+
+enum {
+	/*
+	 * These (redundantly) specify the category of the request
+	 */
+	TASKFILE_CMD_REQ_NODATA	= 0,	/* ide: IDE_DRIVE_TASK_NO_DATA */
+	TASKFILE_CMD_REQ_IN	= 2,	/* ide: IDE_DRIVE_TASK_IN */
+	TASKFILE_CMD_REQ_OUT	= 3,	/* ide: IDE_DRIVE_TASK_OUT */
+	TASKFILE_CMD_REQ_RAW_OUT= 4,	/* ide: IDE_DRIVE_TASK_RAW_WRITE */
+	/*
+	 * These specify the method of transfer (pio, dma, multi, ..)
+	 */
+	TASKFILE_DPHASE_NONE	= 0,	/* ide: TASKFILE_IN */
+	TASKFILE_DPHASE_PIO_IN	= 1,	/* ide: TASKFILE_IN */
+	TASKFILE_DPHASE_PIO_OUT	= 4,	/* ide: TASKFILE_OUT */
+};
+#define SG_ATA_LBA48		1
+#define SG_ATA_PROTO_NON_DATA	( 3 << 1)
+#define SG_ATA_PROTO_PIO_IN	( 4 << 1)
+#define SG_ATA_PROTO_PIO_OUT	( 5 << 1)
+#define SG_ATA_PROTO_DMA	( 6 << 1)
+#define SG_ATA_PROTO_UDMA_IN	(11 << 1) /* not yet supported in libata */
+#define SG_ATA_PROTO_UDMA_OUT	(12 << 1) /* not yet supported in libata */
+
+#endif
+
+
+
+struct _ATA_TaskFile;
+typedef struct _ATA_TaskFile ATA_TaskFile, *PATA_TaskFile;
+
+struct _ATA_TaskFile {
+	MV_U8	Features;
+	MV_U8	Sector_Count;
+	MV_U8	LBA_Low;
+	MV_U8	LBA_Mid;
+	MV_U8	LBA_High;
+	MV_U8	Device;
+	MV_U8	Command;
+
+	MV_U8	Control;
+
+	/* extension */
+	MV_U8	Feature_Exp;
+	MV_U8	Sector_Count_Exp;
+	MV_U8	LBA_Low_Exp;
+	MV_U8	LBA_Mid_Exp;
+	MV_U8	LBA_High_Exp;
+};
+
+/* ATA device identify frame */
+typedef struct _ATA_Identify_Data {
+	MV_U16 General_Config;							/*	0	*/
+	MV_U16 Obsolete0;								/*	1	*/
+	MV_U16 Specific_Config;							/*	2	*/
+	MV_U16 Obsolete1;								/*	3	*/
+	MV_U16 Retired0[2];								/*	4-5	*/
+	MV_U16 Obsolete2;								/*	6	*/
+	MV_U16 Reserved0[2];							/*	7-8	*/
+	MV_U16 Retired1;								/*	9	*/
+	MV_U8 Serial_Number[20];				        /*	10-19	*/
+	MV_U16 Retired2[2];								/*	20-21	*/
+	MV_U16 Obsolete3;								/*	22	*/
+	MV_U8 Firmware_Revision[8];						/*	23-26	*/
+	MV_U8 Model_Number[40];							/*	27-46	*/
+	MV_U16 Maximum_Block_Transfer;					/*	47	*/
+	MV_U16 Reserved1;								/*	48	*/
+	MV_U16 Capabilities[2];							/*	49-50	*/
+	MV_U16 Obsolete4[2];							/*	51-52	*/
+	MV_U16 Fields_Valid;							/*	53	*/
+	MV_U16 Obsolete5[5];							/*	54-58	*/
+	MV_U16 Current_Multiple_Sector_Setting;			/*	59	*/
+	MV_U16 User_Addressable_Sectors[2];				/*	60-61	*/
+	MV_U16 ATAPI_DMADIR;							/*	62	*/
+	MV_U16 Multiword_DMA_Modes;						/*	63	*/
+	MV_U16 PIO_Modes;								/*	64	*/
+	MV_U16 Minimum_Multiword_DMA_Cycle_Time;		/*	65	*/
+	MV_U16 Recommended_Multiword_DMA_Cycle_Time;	/*	66	*/
+	MV_U16 Minimum_PIO_Cycle_Time;					/*	67	*/
+	MV_U16 Minimum_PIO_Cycle_Time_IORDY;			/*	68	*/
+	MV_U16 Reserved2[2];							/*	69-70	*/
+	MV_U16 ATAPI_Reserved[4];						/*	71-74	*/
+	MV_U16 Queue_Depth;								/*	75	*/
+	MV_U16 SATA_Capabilities;						/*	76	*/
+	MV_U16 SATA_Reserved;							/*	77	*/
+	MV_U16 SATA_Feature_Supported;					/*	78	*/
+	MV_U16 SATA_Feature_Enabled;					/*	79	*/
+	MV_U16 Major_Version;							/*	80	*/
+	MV_U16 Minor_Version;							/*	81	*/
+	MV_U16 Command_Set_Supported[2];				/*	82-83	*/
+	MV_U16 Command_Set_Supported_Extension;			/*	84	*/
+	MV_U16 Command_Set_Enabled[2];					/*	85-86	*/
+	MV_U16 Command_Set_Default;						/*	87	*/
+	MV_U16 UDMA_Modes;								/*	88	*/
+	MV_U16 Time_For_Security_Erase;					/*	89	*/
+	MV_U16 Time_For_Enhanced_Security_Erase;		/*	90	*/
+	MV_U16 Current_Advanced_Power_Manage_Value;		/*	91	*/
+	MV_U16 Master_Password_Revision;				/*	92	*/
+	MV_U16 Hardware_Reset_Result;					/*	93	*/
+	MV_U16 Acoustic_Manage_Value;					/*	94	*/
+	MV_U16 Stream_Minimum_Request_Size;				/*	95	*/
+	MV_U16 Stream_Transfer_Time_DMA;				/*	96	*/
+	MV_U16 Stream_Access_Latency;					/*	97	*/
+	MV_U16 Stream_Performance_Granularity[2];		/*	98-99	*/
+	MV_U16 Max_LBA[4];								/*	100-103	*/
+	MV_U16 Stream_Transfer_Time_PIO;				/*	104	*/
+	MV_U16 Reserved3;								/*	105	*/
+	MV_U16 Physical_Logical_Sector_Size;			/*	106	*/
+	MV_U16 Delay_Acoustic_Testing;					/*	107	*/
+	MV_U16 NAA;										/*	108	*/
+	MV_U16 Unique_ID1;								/*	109	*/
+	MV_U16 Unique_ID2;								/*	110	*/
+	MV_U16 Unique_ID3;								/*	111	*/
+	MV_U16 Reserved4[4];							/*	112-115	*/
+	MV_U16 Reserved5;								/*	116	*/
+	MV_U16 Words_Per_Logical_Sector[2];				/*	117-118	*/
+	MV_U16 Reserved6[8];							/*	119-126	*/
+	MV_U16 Removable_Media_Status_Notification;		/*	127	*/
+	MV_U16 Security_Status;							/*	128	*/
+	MV_U16 Vendor_Specific[31];						/*	129-159	*/
+	MV_U16 CFA_Power_Mode;							/*	160	*/
+	MV_U16 Reserved7[15];							/*	161-175	*/
+	MV_U16 Current_Media_Serial_Number[30];			/*	176-205	*/
+	MV_U16 Reserved8[49];							/*	206-254	*/
+	MV_U16 Integrity_Word;							/*	255	*/
+} ATA_Identify_Data, *PATA_Identify_Data;
+
+#define ATA_REGISTER_DATA			0x08
+#define ATA_REGISTER_ERROR			0x09
+#define ATA_REGISTER_FEATURES		0x09
+#define ATA_REGISTER_SECTOR_COUNT	0x0A
+#define ATA_REGISTER_LBA_LOW		0x0B
+#define ATA_REGISTER_LBA_MID		0x0C
+#define ATA_REGISTER_LBA_HIGH		0x0D
+#define ATA_REGISTER_DEVICE			0x0E
+#define ATA_REGISTER_STATUS			0x0F
+#define ATA_REGISTER_COMMAND		0x0F
+
+#define ATA_REGISTER_ALT_STATUS		0x16
+#define ATA_REGISTER_DEVICE_CONTROL	0x16
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_cons.h
@@ -0,0 +1,28 @@
+#ifndef _CORE_DIRVER_CONSOLIDATE_H
+#define _CORE_DRIVER_CONSOLIDATE_H
+
+#include "core_inter.h"
+#include "core_exp.h"
+
+/*
+ * When you plug-in this command consolidate sub-module to some module
+ * Please define the following the definition.
+ * This is maintained by caller.
+ */
+/* Get the consolidate sub module extension */
+#define CONS_GET_EXTENSION(This)					\
+	(((PCore_Driver_Extension)(This))->pConsolid_Extent)
+
+/* Get the device related information consolidate module needs */
+#define CONS_GET_DEVICE(This, Device_Id)	\
+	&(((PCore_Driver_Extension)(This))->pConsolid_Device[(Device_Id)])
+
+/* For this device or port, is there any request running? If yes, busy. */
+#define CONS_DEVICE_IS_BUSY(This, deviceId)	\
+	(((PCore_Driver_Extension)(This))->Ports[PATA_MapPortId(deviceId)].Running_Slot!=0)
+
+extern void Core_InternalSendRequest(MV_PVOID This, PMV_Request pReq);
+/* In case there is something wrong. We need resend these requests and by pass them. */
+#define CONS_SEND_REQUEST	Core_InternalSendRequest
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_exp.c
@@ -0,0 +1,4651 @@
+#include "mv_include.h"
+
+#include "core_exp.h"
+#include "core_inter.h"
+#include "com_tag.h"
+
+#include "core_sata.h"
+#include "core_ata.h"
+
+#include "core_init.h"
+
+#if defined(_OS_LINUX)
+#include "hba_timer.h"
+#endif /* _OS_LINUX */
+
+#ifdef CORE_SUPPORT_API
+#include "core_api.h"
+#endif
+
+#ifdef SOFTWARE_XOR
+#include "core_xor.h"
+#endif
+#include "core_sat.h"
+
+#define FIS_REG_H2D_SIZE_IN_DWORD	5
+
+void sendDummyFIS( PDomain_Port pPort );
+void mv_core_put_back_request(PDomain_Port pPort);
+MV_U8 mv_core_reset_port(PDomain_Port pPort);
+#ifdef COMMAND_ISSUE_WORKROUND
+void mv_core_dump_reg(PDomain_Port pPort);
+void  mv_core_reset_command(PDomain_Port pPort);
+void mv_core_init_reset_para(PDomain_Port pPort);
+#endif
+extern MV_VOID SCSI_To_FIS(MV_PVOID pCore, PMV_Request pReq, MV_U8 tag, PATA_TaskFile pTaskFile);
+
+extern MV_BOOLEAN Category_CDB_Type(
+	IN PDomain_Device pDevice,
+	IN PMV_Request pReq
+	);
+
+extern MV_BOOLEAN ATAPI_CDB2TaskFile(
+	IN PDomain_Device pDevice,
+	IN PMV_Request pReq,
+	OUT PATA_TaskFile pTaskFile
+	);
+
+extern MV_BOOLEAN ATA_CDB2TaskFile(
+	IN PDomain_Device pDevice,
+	IN PMV_Request pReq,
+	IN MV_U8 tag,
+	OUT PATA_TaskFile pTaskFile
+	);
+
+extern void Device_IssueReadLogExt(
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice
+	);
+
+extern MV_BOOLEAN mvDeviceStateMachine(
+	PCore_Driver_Extension pCore,
+	PDomain_Device pDevice
+	);
+
+void CompleteRequest(
+	IN PCore_Driver_Extension pCore,
+	IN PMV_Request pReq,
+	IN PATA_TaskFile taskFiles
+	);
+
+void CompleteRequestAndSlot(
+	IN PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PMV_Request pReq,
+	IN PATA_TaskFile taskFiles,
+	IN MV_U8 slotId
+	);
+
+#if defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX)
+void Core_ResetChannel(MV_PVOID Device, MV_PVOID temp);
+
+static MV_VOID __core_req_timeout_handler(MV_PVOID data)
+{
+	PMV_Request req = (PMV_Request) data;
+	PCore_Driver_Extension pcore;
+	PDomain_Device dev;
+	PHBA_Extension phba;
+
+	if ( NULL == req )
+		return;
+#ifdef SUPPORT_ATA_SMART
+	if(req->Cdb[0]==SCSI_CMD_SND_DIAG) {
+	MV_DPRINT(("Command :'Execute SMART self-test routine' fail.\n"));
+	hba_remove_timer(req);
+	return;
+	}
+#endif
+#ifdef SUPPORT_ATA_SECURITY_CMD
+	if(req->Cdb[0]==ATA_16 && req->Cdb[14]==0xf4) {
+		MV_PRINT("The command of Security Erase is running\n");
+		hba_remove_timer(req);
+		hba_add_timer(req,60, __core_req_timeout_handler);
+		return;
+	}
+#endif
+	pcore = HBA_GetModuleExtension(req->Cmd_Initiator, MODULE_CORE);
+	dev   = &pcore->Ports[PATA_MapPortId(req->Device_Id)].Device[PATA_MapDeviceId(req->Device_Id)];
+	phba = HBA_GetModuleExtension(req->Cmd_Initiator, MODULE_HBA);
+#ifdef MV_DEBUG
+	MV_DPRINT(("Request time out. Resetting channel %d. \n", PATA_MapPortId(req->Device_Id)));
+	MV_DumpRequest(req, 0);
+#endif
+	hba_spin_lock_irq(&phba->desc->hba_desc->global_lock);
+	Core_ResetChannel((MV_PVOID) dev, NULL);
+	hba_spin_unlock_irq(&phba->desc->hba_desc->global_lock);
+}
+#endif /* SUPPORT_ERROR_HANDLING && _OS_LINUX */
+
+#ifdef SUPPORT_SCSI_PASSTHROUGH
+// Read TaskFile
+void readTaskFiles(IN PDomain_Port pPort, PDomain_Device pDevice, PATA_TaskFile pTaskFiles)
+{
+	MV_U32 taskFile[3];
+
+	if (pPort->Type==PORT_TYPE_PATA)
+	{
+		if ( pDevice->Is_Slave )
+		{
+			taskFile[1] = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_SLAVE_TF1);
+			taskFile[2] = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_SLAVE_TF2);
+		}
+		else
+		{
+			taskFile[1] = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_MASTER_TF1);
+			taskFile[2] = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_MASTER_TF2);
+		}
+
+		pTaskFiles->Sector_Count = (MV_U8)((taskFile[1] >> 24) & 0xFF);
+		pTaskFiles->Sector_Count_Exp = (MV_U8)((taskFile[1] >> 16) & 0xFF);
+		pTaskFiles->LBA_Low = (MV_U8)((taskFile[1] >> 8) & 0xFF);
+		pTaskFiles->LBA_Low_Exp = (MV_U8)(taskFile[1] & 0xFF);
+
+		pTaskFiles->LBA_Mid = (MV_U8)((taskFile[2] >> 24) & 0xFF);
+		pTaskFiles->LBA_Mid_Exp = (MV_U8)((taskFile[2] >> 16) & 0xFF);
+		pTaskFiles->LBA_High = (MV_U8)((taskFile[2] >> 8) & 0xFF);
+		pTaskFiles->LBA_High_Exp = (MV_U8)(taskFile[2] & 0xFF);
+	}
+	else
+	{
+//		taskFile[0] = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_TFDATA);
+		taskFile[1] = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_SIG);
+//		taskFile[2] = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_SCR);
+
+		pTaskFiles->Sector_Count = (MV_U8)((taskFile[1]) & 0xFF);
+		pTaskFiles->LBA_Low = (MV_U8)((taskFile[1] >> 8) & 0xFF);
+		pTaskFiles->LBA_Mid = (MV_U8)((taskFile[1] >> 16) & 0xFF);
+		pTaskFiles->LBA_High = (MV_U8)((taskFile[1] >> 24) & 0xFF);
+
+	}
+}
+#endif
+
+
+void mv_core_set_running_slot(PDomain_Port pPort, MV_U32 slot_num, PMV_Request pReq)
+{
+
+#ifdef MV_DEBUG
+	if ( pPort->Running_Slot >> slot_num & MV_BIT(0)  )
+	{
+		MV_DPRINT(("Why the slot[%d] is used.\n",slot_num));
+	}
+
+	if(pPort->Running_Req[slot_num]){
+		MV_DPRINT(("Why the running req[%d] is used.\n",slot_num));
+		MV_DumpRequest(pPort->Running_Req[slot_num], MV_FALSE);
+	}
+#endif
+	pPort->Running_Slot |= 1<<slot_num;
+	pPort->Running_Req[slot_num] = pReq;
+}
+
+void mv_core_reset_running_slot(PDomain_Port pPort, MV_U32 slot_num)
+{
+
+#ifdef MV_DEBUG
+	if ( !(pPort->Running_Slot >> slot_num & MV_BIT(0)) )
+	{
+		MV_DPRINT(("Why the slot[%d] is not used.\n",slot_num));
+	}
+
+	if(!pPort->Running_Req[slot_num]){
+		MV_DPRINT(("Why the running req[%d] is not used.\n",slot_num));
+		MV_DumpRequest(pPort->Running_Req[slot_num], MV_FALSE);
+	}
+#endif
+
+	pPort->Running_Req[slot_num] = NULL;
+	pPort->Running_Slot &= ~(1L<<slot_num);
+	Tag_ReleaseOne(&pPort->Tag_Pool, slot_num);
+}
+
+
+
+MV_U32 Core_ModuleGetResourceQuota(enum Resource_Type type, MV_U16 maxIo)
+{
+	MV_U32 size = 0;
+	MV_U8 sgEntryCount, tagCount;
+
+	/* Extension quota */
+	if ( type==RESOURCE_CACHED_MEMORY )
+	{
+		size = ROUNDING(sizeof(Core_Driver_Extension), 8);
+	#ifdef SUPPORT_CONSOLIDATE
+		if ( maxIo>1 )
+		{
+			size += ROUNDING(sizeof(Consolidate_Extension), 8);
+			size += ROUNDING(sizeof(Consolidate_Device), 8)*MAX_DEVICE_NUMBER;
+		}
+	#endif
+
+		/* resource for SG Entry and Tag Pool */
+		if (maxIo==1) {
+			sgEntryCount = MAX_SG_ENTRY_REDUCED;
+			tagCount = 1;
+		}
+		else {
+			sgEntryCount = MAX_SG_ENTRY;
+			tagCount = MAX_SLOT_NUMBER - 1;		/* leave the last slot for PM hot-plug */
+		}
+
+#ifdef _OS_LINUX
+#ifdef USE_NEW_SGVP
+		sgEntryCount *= 2;
+#endif
+#endif /* _OS_LINUX */
+
+		size += sizeof(MV_SG_Entry) * sgEntryCount * INTERNAL_REQ_COUNT;
+		size += sizeof(MV_Request) * INTERNAL_REQ_COUNT;
+
+		/* tag pool */
+		size += ROUNDING(sizeof(MV_U16) * tagCount * MAX_PORT_NUMBER, 8);
+
+#ifdef SUPPORT_CONSOLIDATE
+		/* resource for Consolidate_Extension->Requests[] SG Entry */
+		if ( maxIo>1 )
+			size += sizeof(MV_SG_Entry) * sgEntryCount * CONS_MAX_INTERNAL_REQUEST_COUNT;
+#endif
+		return size;
+	}
+
+	/* Uncached memory quota */
+	if ( type==RESOURCE_UNCACHED_MEMORY )
+	{
+		/*
+		 * SATA port alignment quota:
+		 * Command list and received FIS is 64 byte aligned.
+		 * Command table is 128 byte aligned.
+		 * Data buffer is 8 byte aligned.
+		 * This is different with AHCI.
+		 */
+		/*
+		 * PATA port alignment quota: Same with SATA.
+		 * The only difference is that PATA doesn't have the FIS.
+		 */
+		MV_DPRINT(("Command List Size = 0x%x.\n", (MV_U32)SATA_CMD_LIST_SIZE));
+		MV_DPRINT(("Received FIS Size = 0x%x.\n", (MV_U32)SATA_RX_FIS_SIZE));
+		MV_DPRINT(("Command Table Size = 0x%x.\n", (MV_U32)SATA_CMD_TABLE_SIZE));
+		MV_ASSERT(SATA_CMD_LIST_SIZE==ROUNDING(SATA_CMD_LIST_SIZE, 64));
+		MV_ASSERT(SATA_RX_FIS_SIZE==ROUNDING(SATA_RX_FIS_SIZE, 64));
+//		MV_ASSERT(SATA_CMD_TABLE_SIZE==ROUNDING(SATA_CMD_TABLE_SIZE, 128));
+		MV_ASSERT(SATA_SCRATCH_BUFFER_SIZE==ROUNDING(SATA_SCRATCH_BUFFER_SIZE, 8));
+		if ( maxIo>1 )
+		{
+			size = 64 + SATA_CMD_LIST_SIZE*MAX_PORT_NUMBER;								/* Command List*/
+			size += 64 + SATA_RX_FIS_SIZE*MAX_SATA_PORT_NUMBER;							/* Received FIS */
+			size += 128 + SATA_CMD_TABLE_SIZE*MAX_SLOT_NUMBER*MAX_PORT_NUMBER;			/* Command Table */
+			size += 8 + SATA_SCRATCH_BUFFER_SIZE*MAX_DEVICE_NUMBER;						/* Buffer for initialization like identify */
+		}
+		else
+		{
+		#ifndef HIBERNATION_ROUNTINE
+			size = 64 + SATA_CMD_LIST_SIZE*MAX_PORT_NUMBER;
+			size += 64 + SATA_RX_FIS_SIZE*MAX_SATA_PORT_NUMBER;
+			size += 128 + SATA_CMD_TABLE_SIZE*MAX_PORT_NUMBER;
+			size += 8 + SATA_SCRATCH_BUFFER_SIZE*MAX_DEVICE_NUMBER;
+		#else
+			size = 64 + SATA_CMD_LIST_SIZE;			/* Command List*/
+			size += 64 + SATA_RX_FIS_SIZE;			/* Received FIS */
+			size += 128 + SATA_CMD_TABLE_SIZE; 		/* Command Table */
+			size += 8 + SATA_SCRATCH_BUFFER_SIZE;	/* Buffer for initialization like identify */
+		#endif
+		}
+
+		return size;
+	}
+
+	return 0;
+}
+
+void Core_ModuleInitialize(MV_PVOID ModulePointer, MV_U32 extensionSize, MV_U16 maxIo)
+{
+#if defined(NEW_LINUX_DRIVER)
+	struct mv_mod_desc *mod_desc = (struct mv_mod_desc *)ModulePointer;
+	MV_PVOID		This = (MV_PVOID)mod_desc->extension;
+	PCore_Driver_Extension pCore    = (PCore_Driver_Extension) mod_desc->extension;
+	MV_U32 size=0;
+#else
+	MV_PVOID		This=ModulePointer;
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)ModulePointer;
+	Controller_Infor controller;
+#endif	/* NEW_LINUX_DRIVER*/
+
+
+	PMV_Request pReq;
+	Assigned_Uncached_Memory dmaResource;
+	PDomain_Port port;
+	MV_PVOID memVir;
+	MV_PHYSICAL_ADDR memDMA;
+	MV_PTR_INTEGER temp, tmpSG;
+	MV_U32 offset, internalReqSize, tagSize;
+	MV_U8 i,j, flagSaved, sgEntryCount, tagCount;
+	MV_U32 vsr_c[MAX_SATA_PORT_NUMBER];
+	MV_U8 vsrSkipPATAPort = 0;
+#ifndef _OS_LINUX
+	MV_PVOID pTopLayer = HBA_GetModuleExtension(pCore, MODULE_HBA);
+#endif
+	flagSaved=pCore->VS_Reg_Saved;
+
+	if(flagSaved==VS_REG_SIG)
+	{
+		for ( j=0; j<MAX_SATA_PORT_NUMBER; j++ )
+		{
+			port = &pCore->Ports[j];
+			vsr_c[j]=port->VS_RegC;
+		}
+		/* Save the PATA Port detection skip flag */
+		vsrSkipPATAPort = pCore->Flag_Fastboot_Skip & FLAG_SKIP_PATA_PORT;
+	}
+
+	/*
+	 * Zero core driver extension. After that, I'll ignore many variables initialization.
+	 */
+	MV_ZeroMemory(This, extensionSize);
+
+	if(flagSaved==VS_REG_SIG)
+	{
+		pCore->VS_Reg_Saved=flagSaved;
+
+		for ( j=0; j<MAX_SATA_PORT_NUMBER; j++ )
+		{
+			port = &pCore->Ports[j];
+			port->VS_RegC=vsr_c[j];
+		}
+		/* Restore the PATA Port detection skip flag */
+		/* Only this flag should survive the S3 */
+		/* The others should be kept as default (0) */
+		pCore->Flag_Fastboot_Skip = vsrSkipPATAPort;
+	}
+
+	pCore->State = CORE_STATE_IDLE;
+	/* Set up controller information */
+#if defined(__MM_SE__)
+	/* Set up controller information */
+	pCore->desc        = mod_desc;
+	pCore->Vendor_Id   = mod_desc->hba_desc->vendor;
+	pCore->Device_Id   = mod_desc->hba_desc->device;
+	pCore->Revision_Id = mod_desc->hba_desc->Revision_Id;
+	MV_DPRINT(("pCore->Device_Id = 0x%x.\n",pCore->Device_Id));
+	/*
+	pCore->Sub_System_Id =mod_desc->hba_desc->Sub_System_Id;
+	pCore->Sub_Vendor_Id =mod_desc->hba_desc->Sub_Vendor_Id;
+	*/
+	for ( i=0; i<MAX_BASE_ADDRESS; i++ )
+	{
+		pCore->Base_Address[i] = mod_desc->hba_desc->Base_Address[i];
+		MV_DPRINT(("pCore->Base_Address[%d]=%p.\n", i,pCore->Base_Address[i]));
+	}
+	pCore->Mmio_Base = mod_desc->hba_desc->Base_Address[MV_PCI_BAR];
+ #else
+	/* Set up controller information */
+	HBA_GetControllerInfor(pCore, &controller);
+	pCore->Vendor_Id = controller.Vendor_Id;
+	pCore->Device_Id = controller.Device_Id;
+	pCore->Revision_Id = controller.Revision_Id;
+	for ( i=0; i<MAX_BASE_ADDRESS; i++ )
+	{
+		pCore->Base_Address[i] = controller.Base_Address[i];
+	}
+	pCore->Mmio_Base = controller.Base_Address[MV_PCI_BAR];
+
+#endif /*_OS_LINUX*/
+	pCore->Adapter_State = ADAPTER_INITIALIZING;
+	MV_LIST_HEAD_INIT(&pCore->Waiting_List);
+	MV_LIST_HEAD_INIT(&pCore->Internal_Req_List);
+
+	if ( maxIo==1 )
+		pCore->Is_Dump = MV_TRUE;
+	else
+		pCore->Is_Dump = MV_FALSE;
+
+	if (flagSaved!=VS_REG_SIG) {	/* Added for tuning boot up time */
+		/* This initialization is during boot up time, but not S3 */
+		/* Read registers modified by BIOS to set detection flag */
+		if ( (pCore->Device_Id==DEVICE_ID_THOR_4S1P_NEW) ||
+			 (pCore->Device_Id==DEVICE_ID_THORLITE_2S1P_WITH_FLASH) ||
+			 (pCore->Device_Id==DEVICE_ID_THORLITE_2S1P)) {
+			MV_U32 tmpReg = 0;
+			/* Read Bit[3] of PCI CNFG offset 60h to get flag for */
+			/* PATA port enable/disable (0 - default, need to detect) */
+#ifdef _OS_WINDOWS
+#undef MV_PCI_READ_CONFIG_DWORD
+#define MV_PCI_READ_CONFIG_DWORD(mod_ext, offset, reg) \
+                reg = MV_PCI_READ_DWORD(mod_ext, offset)
+			MV_PCI_READ_CONFIG_DWORD(pTopLayer, 0x60, tmpReg);
+#else
+			MV_PCI_READ_CONFIG_DWORD(pCore, 0x60, &tmpReg);
+#endif /* _OS_WINDOWS */
+			tmpReg &= MV_BIT(3);
+
+			pCore->Flag_Fastboot_Skip |= (tmpReg >> 3);		/* bit 0 */
+			/* Read Bit[10], Bit [11] of BAR5 offset A4h to get flag for */
+			/* PATA device detection (0 - default, need to detect) and */
+			/* PM detection (0 - default, need to detect) */
+			tmpReg = MV_REG_READ_DWORD(pCore->Mmio_Base, VENDOR_DETECT) &
+						(VENDOR_DETECT_PATA | VENDOR_DETECT_PM);
+			pCore->Flag_Fastboot_Skip |= (tmpReg >> 9);		/* bit 1, 2 */
+		}
+	}
+
+	if ( (pCore->Device_Id==DEVICE_ID_THORLITE_2S1P)||(pCore->Device_Id==DEVICE_ID_THORLITE_2S1P_WITH_FLASH) )
+	{
+		pCore->SATA_Port_Num = 2;
+		pCore->PATA_Port_Num = 1;
+		pCore->Port_Num = 3;
+		MV_DPRINT(("DEVICE_ID_THORLITE_2S1P is found.\n"));
+
+	}
+	else if ( pCore->Device_Id==DEVICE_ID_THORLITE_0S1P )
+	{
+		pCore->SATA_Port_Num = 0;
+		pCore->PATA_Port_Num = 1;
+		pCore->Port_Num = 1;
+		MV_DPRINT(("DEVICE_ID_THORLITE_0S1P is found.\n"));
+
+	}
+	else
+	{
+		pCore->SATA_Port_Num = 4;
+		pCore->PATA_Port_Num = 1;
+		pCore->Port_Num = 5;
+		MV_DPRINT(("DEVICE_ID_THOR is found.\n"));
+
+	}
+
+#if /*(VER_OEM==VER_OEM_ASUS) ||*/(VER_OEM==VER_OEM_INTEL)
+	pCore->Port_Num -= pCore->PATA_Port_Num;
+	pCore->PATA_Port_Num = 0;
+#else
+	if (pCore->Flag_Fastboot_Skip & FLAG_SKIP_PATA_PORT) {
+		pCore->Port_Num -= pCore->PATA_Port_Num;
+		pCore->PATA_Port_Num = 0;
+	}
+#endif
+
+	if (pCore->Is_Dump) {
+		sgEntryCount = MAX_SG_ENTRY_REDUCED;
+		tagCount = 1;	/* do not support hot-plug when hibernation */
+	}
+	else {
+		sgEntryCount = MAX_SG_ENTRY;
+		tagCount = MAX_SLOT_NUMBER - 1;		/* reserve the last slot for PM hot-plug */
+	}
+
+#ifdef _OS_LINUX
+#ifdef USE_NEW_SGVP
+	sgEntryCount *= 2;
+#endif
+#endif /* _OS_LINUX */
+
+	tmpSG = (MV_PTR_INTEGER)This + ROUNDING(sizeof(Core_Driver_Extension),8);
+	temp = 	tmpSG + sizeof(MV_SG_Entry) * sgEntryCount * INTERNAL_REQ_COUNT;
+
+	internalReqSize = MV_REQUEST_SIZE * INTERNAL_REQ_COUNT;
+	MV_ASSERT( extensionSize >= ROUNDING(sizeof(Core_Driver_Extension),8) + internalReqSize );
+	for ( i=0; i<INTERNAL_REQ_COUNT; i++ )
+	{
+		pReq = (PMV_Request)temp;
+		pReq->SG_Table.Entry_Ptr = (PMV_SG_Entry)tmpSG;
+		pReq->SG_Table.Max_Entry_Count = sgEntryCount;
+		List_AddTail(&pReq->Queue_Pointer, &pCore->Internal_Req_List);
+		tmpSG += sizeof(MV_SG_Entry) * sgEntryCount;
+		temp += MV_REQUEST_SIZE;	/* MV_Request is 64bit aligned. */
+	}
+//	temp = ROUNDING( (MV_PTR_INTEGER)temp, 8 );		/* Don't round the extension pointer */
+
+	/* tag pool */
+	tagSize = sizeof(MV_U16) * tagCount * MAX_PORT_NUMBER;
+
+	for ( i=0; i<MAX_PORT_NUMBER; i++ )
+	{
+		port = &pCore->Ports[i];
+		port->Tag_Pool.Stack = (MV_PU16)temp;
+		port->Tag_Pool.Size = tagCount;
+		temp += sizeof(MV_U16) * tagCount;
+	}
+
+#ifdef SUPPORT_CONSOLIDATE
+	// Allocate resource for Consolidate_Extension->Requests[].
+	tmpSG = temp;
+	temp = temp + sizeof(MV_SG_Entry) * sgEntryCount * CONS_MAX_INTERNAL_REQUEST_COUNT;
+
+	if ( pCore->Is_Dump )
+	{
+		pCore->pConsolid_Device = NULL;
+		pCore->pConsolid_Extent = NULL;
+	}
+	else
+	{
+		MV_ASSERT( extensionSize>=
+			( ROUNDING(sizeof(Core_Driver_Extension),8) + internalReqSize + tagSize + ROUNDING(sizeof(Consolidate_Extension),8) + ROUNDING(sizeof(Consolidate_Device),8)*MAX_DEVICE_NUMBER )
+			);
+		pCore->pConsolid_Extent = (PConsolidate_Extension)(temp);
+
+		//Initialize some fields for pCore->pConsolid_Extent->Requests[i]
+		for (i=0; i<CONS_MAX_INTERNAL_REQUEST_COUNT; i++)
+		{
+			pReq = &pCore->pConsolid_Extent->Requests[i];
+
+			pReq->SG_Table.Max_Entry_Count = sgEntryCount;
+			pReq->SG_Table.Entry_Ptr = (PMV_SG_Entry)tmpSG;
+			tmpSG += sizeof(MV_SG_Entry) * sgEntryCount;
+		}
+
+		pCore->pConsolid_Device = (PConsolidate_Device)((MV_PTR_INTEGER)pCore->pConsolid_Extent + ROUNDING(sizeof(Consolidate_Extension),8));
+	}
+#endif
+
+	/* Port_Map and Port_Num will be read from the register */
+
+	/* Init port data structure */
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		port = &pCore->Ports[i];
+
+		port->Id = i;
+		port->Port_State = PORT_STATE_IDLE;
+		port->Core_Extension = pCore;
+		port->Mmio_Base = (MV_PU8)pCore->Mmio_Base + 0x100 + (i * 0x80);
+		port->Mmio_SCR = (MV_PU8)port->Mmio_Base + PORT_SCR;
+
+		Tag_Init(&port->Tag_Pool, tagCount);
+
+		for (j=0; j<MAX_DEVICE_PER_PORT; j++)
+		{
+			port->Device[j].Id = i*MAX_DEVICE_PER_PORT + j;
+			port->Device[j].PPort = port;
+			port->Device[j].Is_Slave = 0;	/* Which one is the slave will be determined during discovery. */
+#if defined(SUPPORT_TIMER) && defined(_OS_WINDOWS)
+			port->Device[j].Timer_ID = NO_CURRENT_TIMER;
+#endif
+			port->Device[j].Reset_Count = 0;
+		}
+
+		port->Device_Number = 0;
+
+		// Set function table for each port here.
+		if ( i>=pCore->SATA_Port_Num )
+			port->Type = PORT_TYPE_PATA;
+		else
+			port->Type = PORT_TYPE_SATA;
+
+#ifdef COMMAND_ISSUE_WORKROUND
+		OSSW_INIT_TIMER(&port->timer);
+		port->reset_hba_times= 0;
+		mv_core_init_reset_para(port);
+#endif
+
+	}
+
+	/* Get uncached memory */
+#if defined(__MM_SE__)
+	size = pCore->desc->ops->get_res_desc(RESOURCE_UNCACHED_MEMORY, maxIo);
+	if (HBA_GetResource(pCore->desc,
+			    RESOURCE_UNCACHED_MEMORY,
+			    size,
+			    &dmaResource))
+	{
+		//pCore->State = CORE_STATE_ZOMBIE;
+		MV_ASSERT(MV_FALSE);
+		return;
+	}
+#else
+	/* Get uncached memory */
+	HBA_GetResource(pCore, RESOURCE_UNCACHED_MEMORY, &dmaResource);
+
+#endif /*_OS_LINUX*/
+	memVir = dmaResource.Virtual_Address;
+	memDMA = dmaResource.Physical_Address;
+
+	/* Assign uncached memory for command list (64 byte align) */
+	offset = (MV_U32)(ROUNDING(memDMA.value,64)-memDMA.value);
+	memDMA.value += offset;
+	memVir = (MV_PU8)memVir + offset;
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		port = &pCore->Ports[i];
+		port->Cmd_List = memVir;
+		port->Cmd_List_DMA = memDMA;
+	#ifdef HIBERNATION_ROUNTINE
+		if((!pCore->Is_Dump)|| (i==(pCore->Port_Num-1)))
+	#endif
+		{
+			memVir = (MV_PU8)memVir + SATA_CMD_LIST_SIZE;
+			memDMA.value += SATA_CMD_LIST_SIZE;
+		}
+	}
+
+	/* Assign uncached memory for received FIS (64 byte align) */
+	offset = (MV_U32)(ROUNDING(memDMA.value,64)-memDMA.value);
+	memDMA.value += offset;
+	memVir = (MV_PU8)memVir + offset;
+	for ( i=0; i<pCore->SATA_Port_Num; i++ )
+	{
+		port = &pCore->Ports[i];
+		port->RX_FIS = memVir;
+		port->RX_FIS_DMA = memDMA;
+	#ifdef HIBERNATION_ROUNTINE
+		if((!pCore->Is_Dump)|| (i==(pCore->SATA_Port_Num-1)))
+	#endif
+		{
+			memVir = (MV_PU8)memVir + SATA_RX_FIS_SIZE;
+			memDMA.value += SATA_RX_FIS_SIZE;
+		}
+	}
+
+	/* Assign the 32 command tables. (128 byte align) */
+	offset = (MV_U32)(ROUNDING(memDMA.value,128)-memDMA.value);
+	memDMA.value += offset;
+	memVir = (MV_PU8)memVir + offset;
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		port = &pCore->Ports[i];
+		port->Cmd_Table = memVir;
+		port->Cmd_Table_DMA = memDMA;
+
+		if ( !pCore->Is_Dump )
+		{
+			memVir = (MV_PU8)memVir + SATA_CMD_TABLE_SIZE * MAX_SLOT_NUMBER;
+			memDMA.value += SATA_CMD_TABLE_SIZE * MAX_SLOT_NUMBER;
+		}
+		else
+		{
+		#ifdef HIBERNATION_ROUNTINE
+			if(i==(pCore->Port_Num-1))
+		#endif
+			{
+				memVir = (MV_PU8)memVir + SATA_CMD_TABLE_SIZE;
+				memDMA.value += SATA_CMD_TABLE_SIZE;
+			}
+		}
+	}
+
+	/* Assign the scratch buffer (8 byte align) */
+	offset = (MV_U32)(ROUNDING(memDMA.value,8)-memDMA.value);
+	memDMA.value += offset;
+	memVir = (MV_PU8)memVir + offset;
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		port = &pCore->Ports[i];
+		for ( j=0; j<MAX_DEVICE_PER_PORT; j++ )
+		{
+			port->Device[j].Scratch_Buffer = memVir;
+			port->Device[j].Scratch_Buffer_DMA = memDMA;
+
+		#ifdef HIBERNATION_ROUNTINE
+			if((!pCore->Is_Dump)|| (i==(pCore->Port_Num-1)))
+		#endif
+			{
+				memVir = (MV_PU8)memVir + SATA_SCRATCH_BUFFER_SIZE;
+				memDMA.value += SATA_SCRATCH_BUFFER_SIZE;
+			}
+		}
+	}
+
+	/* Let me confirm the following assumption */
+	MV_ASSERT( sizeof(SATA_FIS_REG_H2D)==sizeof(MV_U32)*FIS_REG_H2D_SIZE_IN_DWORD );
+	MV_ASSERT( sizeof(MV_Command_Table)==0x80+MAX_SG_ENTRY*sizeof(MV_SG_Entry) );
+	MV_ASSERT( sizeof(ATA_Identify_Data)==512 );
+
+#ifdef SUPPORT_CONSOLIDATE
+	if ( !pCore->Is_Dump )
+	{
+		Consolid_InitializeExtension(This);
+		for ( i=0; i<MAX_DEVICE_NUMBER; i++ )
+			Consolid_InitializeDevice(This, i);
+	}
+#endif
+}
+
+void Core_ModuleStart(MV_PVOID This)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+
+	mvAdapterStateMachine(pCore,NULL);
+}
+
+
+void Core_ModuleShutdown(MV_PVOID This)
+{
+	/*
+	 * This function is equivalent to ahci_port_stop
+	 */
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	MV_U32 tmp, i;
+	MV_LPVOID mmio;
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		mmio = pCore->Ports[i].Mmio_Base;
+
+		tmp = MV_REG_READ_DWORD(mmio, PORT_CMD);
+		if ( pCore->Ports[i].Type==PORT_TYPE_SATA )
+			tmp &= ~(PORT_CMD_START | PORT_CMD_FIS_RX);
+		else
+			tmp &= ~PORT_CMD_START;
+		MV_REG_WRITE_DWORD(mmio, PORT_CMD, tmp);
+		MV_REG_READ_DWORD(mmio, PORT_CMD); /* flush */
+
+		/*
+		 * spec says 500 msecs for each PORT_CMD_{START,FIS_RX} bit, so
+		 * this is slightly incorrect.
+		 */
+		HBA_SleepMillisecond(pCore, 500);
+	}
+
+	/* Disable the controller interrupt */
+	tmp = MV_REG_READ_DWORD(pCore->Mmio_Base, HOST_CTL);
+	tmp &= ~(HOST_IRQ_EN);
+	MV_REG_WRITE_DWORD(pCore->Mmio_Base, HOST_CTL, tmp);
+}
+
+void Core_ModuleNotification(MV_PVOID This, enum Module_Event event, struct mod_notif_param *param)
+{
+}
+
+void Core_HandleWaitingList(PCore_Driver_Extension pCore);
+void Core_InternalSendRequest(MV_PVOID This, PMV_Request pReq);
+
+void Core_ModuleSendRequest(MV_PVOID This, PMV_Request pReq)
+{
+#ifdef SUPPORT_CONSOLIDATE
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	PDomain_Device pDevice;
+	MV_U8 portId = PATA_MapPortId(pReq->Device_Id);
+	MV_U8 deviceId = PATA_MapDeviceId(pReq->Device_Id);
+
+	pDevice = &pCore->Ports[portId].Device[deviceId];
+
+	if ( (!(pDevice->Device_Type&DEVICE_TYPE_ATAPI)) && (!pCore->Is_Dump) )
+		Consolid_ModuleSendRequest(pCore, pReq);
+	else
+		Core_InternalSendRequest(pCore, pReq);
+#else
+	Core_InternalSendRequest(This, pReq);
+#endif
+}
+
+void Core_InternalSendRequest(MV_PVOID This, PMV_Request pReq)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+	PDomain_Device pdev;
+	PHBA_Extension phba;
+
+	pdev = &pCore->Ports[PATA_MapPortId(pReq->Device_Id)].Device[PATA_MapDeviceId(pReq->Device_Id)];
+	if((pdev->Status & DEVICE_STATUS_SLEEP)&&(pReq->Req_Type == REQ_TYPE_OS))
+	{
+		hba_remove_timer(pReq);
+		Core_ResetChannel((MV_PVOID) pdev, NULL);
+		pdev->Status &= ~DEVICE_STATUS_SLEEP;
+	}
+#endif
+	/* Check whether we can handle this request */
+	switch (pReq->Cdb[0])
+	{
+		case SCSI_CMD_INQUIRY:
+		case SCSI_CMD_START_STOP_UNIT:
+		case SCSI_CMD_TEST_UNIT_READY:
+		case SCSI_CMD_READ_10:
+		case SCSI_CMD_WRITE_10:
+		case SCSI_CMD_VERIFY_10:
+		case SCSI_CMD_READ_CAPACITY_10:
+		case SCSI_CMD_REQUEST_SENSE:
+		case SCSI_CMD_MODE_SELECT_10:
+		case SCSI_CMD_MODE_SENSE_10:
+		case SCSI_CMD_MARVELL_SPECIFIC:
+		default:
+			if ( pReq->Cmd_Initiator==pCore )
+			{
+				if ( !SCSI_IS_READ(pReq->Cdb[0]) && !SCSI_IS_WRITE(pReq->Cdb[0]) )
+				{
+					/* Reset request or request sense command. */
+					List_Add(&pReq->Queue_Pointer, &pCore->Waiting_List);		/* Add to the header. */
+				}
+				else
+				{
+					#ifdef SUPPORT_CONSOLIDATE
+					/* Consolidate request */
+					MV_DASSERT( !pCore->Is_Dump );
+					List_AddTail(&pReq->Queue_Pointer, &pCore->Waiting_List);	/* Append to the tail. */
+					#else
+					MV_ASSERT(MV_FALSE);
+					#endif
+				}
+			}
+			else
+			{
+				List_AddTail(&pReq->Queue_Pointer, &pCore->Waiting_List);		/* Append to the tail. */
+			}
+			Core_HandleWaitingList(pCore);
+			break;
+	}
+}
+
+void SATA_PrepareCommandHeader(PDomain_Port pPort, PMV_Request pReq, MV_U8 tag)
+{
+	MV_PHYSICAL_ADDR table_addr;
+	PMV_Command_Header header = NULL;
+	PMV_SG_Table pSGTable = &pReq->SG_Table;
+#ifdef SUPPORT_PM
+	PDomain_Device pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+#endif
+	header = SATA_GetCommandHeader(pPort, tag);
+	/*
+	 * Set up the command header.
+	 */
+	header->FIS_Length = FIS_REG_H2D_SIZE_IN_DWORD;
+	header->Packet_Command = (pReq->Cmd_Flag&CMD_FLAG_PACKET)?1:0;
+	header->Reset = 0;
+	header->NCQ = (pReq->Cmd_Flag&CMD_FLAG_NCQ)?1:0;
+
+#ifdef SUPPORT_PM
+	header->PM_Port = pDevice->PM_Number;
+#else
+	header->PM_Port = 0;
+#endif
+	*((MV_U16 *) header) = CPU_TO_LE_16( *((MV_U16 *) header) );
+	header->PRD_Entry_Count = CPU_TO_LE_16(pSGTable->Valid_Entry_Count);
+
+	table_addr.parts.high = pPort->Cmd_Table_DMA.parts.high;
+	table_addr.parts.low = pPort->Cmd_Table_DMA.parts.low + SATA_CMD_TABLE_SIZE*tag;
+	if ( table_addr.parts.low<pPort->Cmd_Table_DMA.parts.low ) {
+		MV_DPRINT(("Cross 4G boundary.\n"));
+		table_addr.parts.high++;
+	}
+
+
+	header->Table_Address = CPU_TO_LE_32(table_addr.parts.low);
+	header->Table_Address_High = CPU_TO_LE_32(table_addr.parts.high);
+}
+
+void PATA_PrepareCommandHeader(PDomain_Port pPort, PMV_Request pReq, MV_U8 tag)
+{
+	MV_PHYSICAL_ADDR table_addr;
+	PMV_PATA_Command_Header header = NULL;
+	PMV_SG_Table pSGTable = &pReq->SG_Table;
+
+	header = PATA_GetCommandHeader(pPort, tag);
+	/*
+	 * Set up the command header.
+	 * TCQ, Diagnostic_Command, Reset
+	 * Table_Address and Table_Address_High are fixed. Needn't set every time.
+	 */
+	header->PIO_Sector_Count = 0;		/* Only for PIO multiple sector commands */
+	header->Controller_Command = 0;
+	header->TCQ = 0;
+	header->Packet_Command = (pReq->Cmd_Flag&CMD_FLAG_PACKET)?1:0;
+
+#ifdef USE_DMA_FOR_ALL_PACKET_COMMAND
+	if ( pReq->Cmd_Flag&CMD_FLAG_PACKET )
+	{
+		header->DMA = (pReq->Cmd_Flag&CMD_FLAG_NON_DATA)?0:1;
+	}
+	else
+	{
+		header->DMA = (pReq->Cmd_Flag&CMD_FLAG_DMA)?1:0;
+	}
+#elif defined(USE_PIO_FOR_ALL_PACKET_COMMAND)
+	if ( pReq->Cmd_Flag&CMD_FLAG_PACKET )
+	{
+		header->DMA = 0;
+	}
+	else
+	{
+		header->DMA = (pReq->Cmd_Flag&CMD_FLAG_DMA)?1:0;
+	}
+#else
+	header->DMA = (pReq->Cmd_Flag&CMD_FLAG_DMA)?1:0;
+#endif
+
+	header->Data_In = (pReq->Cmd_Flag&CMD_FLAG_DATA_IN)?1:0;
+	header->Non_Data = (pReq->Cmd_Flag&CMD_FLAG_NON_DATA)?1:0;
+
+	header->PIO_Sector_Command = 0;
+	header->Is_48Bit = (pReq->Cmd_Flag&CMD_FLAG_48BIT)?1:0;
+	header->Diagnostic_Command = 0;
+	header->Reset = 0;
+
+	header->Is_Slave = pPort->Device[PATA_MapDeviceId(pReq->Device_Id)].Is_Slave;
+
+	*((MV_U16 *) header) = CPU_TO_LE_16( *((MV_U16 *) header) );
+	header->PRD_Entry_Count = CPU_TO_LE_16(pSGTable->Valid_Entry_Count);
+
+	table_addr.parts.high = pPort->Cmd_Table_DMA.parts.high;
+	table_addr.parts.low = pPort->Cmd_Table_DMA.parts.low + SATA_CMD_TABLE_SIZE*tag;
+	if ( table_addr.parts.low<pPort->Cmd_Table_DMA.parts.low ) {
+		MV_DPRINT(("Cross 4G boundary.\n"));
+		table_addr.parts.high++;
+	}
+
+	header->Table_Address = CPU_TO_LE_32(table_addr.parts.low);
+	header->Table_Address_High = CPU_TO_LE_32(table_addr.parts.high);
+}
+
+/*
+ * Fill SATA command table
+ */
+MV_VOID SATA_PrepareCommandTable(
+	PDomain_Port pPort,
+	PMV_Request pReq,
+	MV_U8 tag,
+	PATA_TaskFile pTaskFile
+	)
+{
+#ifdef USE_NEW_SGTABLE
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+#else
+	PMV_SG_Entry pSGEntry = NULL;
+	MV_U8 i;
+#endif
+	PMV_Command_Table pCmdTable = Port_GetCommandTable(pPort, tag);
+
+	PMV_SG_Table pSGTable = &pReq->SG_Table;
+
+	/* Step 1: fill the command FIS: MV_Command_Table */
+	SCSI_To_FIS(pPort->Core_Extension, pReq, tag, pTaskFile);
+
+	/* Step 2. fill the ATAPI CDB */
+	if ( pReq->Cmd_Flag&CMD_FLAG_PACKET )
+	{
+		MV_CopyMemory(pCmdTable->ATAPI_CDB, pReq->Cdb, MAX_CDB_SIZE);
+	}
+
+	/* Step 3: fill the PRD Table if necessary. */
+	if ( (pSGTable) && (pSGTable->Valid_Entry_Count) )
+	{
+#ifdef USE_NEW_SGTABLE
+		MV_U16 consumed;
+		PMV_Command_Header header = NULL;
+
+		header = SATA_GetCommandHeader(pPort, tag);
+		consumed = (MV_U16)sgdt_prepare_hwprd(pCore, pSGTable, pCmdTable->PRD_Entry, HW_SG_ENTRY_MAX);
+		if (consumed == 0) {
+			/* resource not enough... */
+			MV_DPRINT(("Run out of PRD entry.\n"));
+			if( pReq->Req_Flag & REQ_FLAG_CONSOLIDATE )
+			{
+				pReq->Scsi_Status = REQ_STATUS_BUSY;
+				Tag_ReleaseOne(&pPort->Tag_Pool, tag);
+				return;
+			}
+			else
+			{
+				/* check why upper layer send request with too many sg items... */
+				MV_DASSERT( MV_FALSE );
+			}
+		}
+		header->PRD_Entry_Count = CPU_TO_LE_16(consumed);
+#else /* USE_NEW_SGTABLE */
+		if ( (pSGTable) && (pSGTable->Valid_Entry_Count) )
+		{
+			/* "Transfer Byte Count" in AHCI and 614x PRD table is zero based. */
+			for ( i=0; i<pSGTable->Valid_Entry_Count; i++ )
+			{
+				pSGEntry = &pCmdTable->PRD_Entry[i];
+				pSGEntry->Base_Address = CPU_TO_LE_32(pSGTable->Entry_Ptr[i].Base_Address);
+				pSGEntry->Base_Address_High = CPU_TO_LE_32(pSGTable->Entry_Ptr[i].Base_Address_High);
+				pSGEntry->Size = CPU_TO_LE_32(pSGTable->Entry_Ptr[i].Size-1);
+			}
+		}
+		else
+		{
+			MV_DASSERT( !SCSI_IS_READ(pReq->Cdb[0]) && !SCSI_IS_WRITE(pReq->Cdb[0]) );
+		}
+#endif /* USE_NEW_SGTABLE */
+	}
+	else
+	{
+		MV_DASSERT( !SCSI_IS_READ(pReq->Cdb[0]) && !SCSI_IS_WRITE(pReq->Cdb[0]) );
+	}
+}
+
+/*
+ * Fill the PATA command table
+*/
+static MV_VOID PATA_PrepareCommandTable(
+	PDomain_Port pPort,
+	PMV_Request pReq,
+	MV_U8 tag,
+	PATA_TaskFile pTaskFile
+	)
+{
+#ifdef USE_NEW_SGTABLE
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+#endif
+	PMV_Command_Table pCmdTable = Port_GetCommandTable(pPort, tag);
+	PMV_SG_Table pSGTable = &pReq->SG_Table;
+	MV_PU8 pU8 = (MV_PU8)pCmdTable;
+	MV_U8 device_index;
+
+	device_index = PATA_MapDeviceId(pReq->Device_Id);
+
+	/* Step 1: Fill the command block */
+	(*pU8)=pTaskFile->Features; pU8++;
+	(*pU8)=pTaskFile->Feature_Exp; pU8++;
+	(*pU8)=pTaskFile->Sector_Count; pU8++;
+	(*pU8)=pTaskFile->Sector_Count_Exp; pU8++;
+	(*pU8)=pTaskFile->LBA_Low; pU8++;
+	(*pU8)=pTaskFile->LBA_Low_Exp; pU8++;
+	(*pU8)=pTaskFile->LBA_Mid; pU8++;
+	(*pU8)=pTaskFile->LBA_Mid_Exp; pU8++;
+	(*pU8)=pTaskFile->Command; pU8++;
+	(*pU8)=pTaskFile->Device; pU8++;
+	(*pU8)=pTaskFile->LBA_High; pU8++;
+	(*pU8)=pTaskFile->LBA_High_Exp; pU8++;
+	*((MV_PU32)pU8) = 0L;
+
+	/* Step 2: Fill the ATAPI CDB */
+	if ( pReq->Cmd_Flag&CMD_FLAG_PACKET )
+	{
+		MV_CopyMemory(pCmdTable->ATAPI_CDB, pReq->Cdb, MAX_CDB_SIZE);
+	}
+
+	/* Step 3: Fill the PRD Table if necessary. */
+	if ( (pSGTable) && (pSGTable->Valid_Entry_Count) )
+	{
+#ifdef USE_NEW_SGTABLE
+		MV_U16 consumed = (MV_U16) sgdt_prepare_hwprd(pCore, pSGTable, pCmdTable->PRD_Entry, HW_SG_ENTRY_MAX);
+		PMV_PATA_Command_Header header = NULL;
+		header = PATA_GetCommandHeader(pPort, tag);
+
+		if (consumed == 0) {
+			/* resource not enough... */
+			MV_DPRINT(("Run out of PRD entry.\n"));
+			if( pReq->Req_Flag & REQ_FLAG_CONSOLIDATE )
+			{
+				pReq->Scsi_Status = REQ_STATUS_BUSY;
+				Tag_ReleaseOne(&pPort->Tag_Pool, tag);
+				return;
+			}
+			else
+			{
+				/* check why upper layer send request with too many sg items... */
+				MV_DASSERT( MV_FALSE );
+			}
+		}
+		header->PRD_Entry_Count = CPU_TO_LE_16(consumed);
+#else
+		MV_U8 i;
+		PMV_SG_Entry pSGEntry = NULL;
+
+		/* "Transfer Byte Count" in AHCI and 614x PRD table is zero based. */
+		for ( i=0; i<pSGTable->Valid_Entry_Count; i++ )
+		{
+			pSGEntry = &pCmdTable->PRD_Entry[i];
+			pSGEntry->Base_Address = CPU_TO_LE_32(pSGTable->Entry_Ptr[i].Base_Address);
+			pSGEntry->Base_Address_High = CPU_TO_LE_32(pSGTable->Entry_Ptr[i].Base_Address_High);
+			pSGEntry->Size = CPU_TO_LE_32(pSGTable->Entry_Ptr[i].Size-1);
+		}
+#endif
+	}
+	else
+	{
+		MV_DASSERT( !SCSI_IS_READ(pReq->Cdb[0]) && !SCSI_IS_WRITE(pReq->Cdb[0]) );
+	}
+
+}
+
+void SATA_SendFrame(PDomain_Port pPort, PMV_Request pReq, MV_U8 tag)
+{
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	#ifdef SUPPORT_ATA_SECURITY_CMD
+	PDomain_Device pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+	#endif
+	MV_DASSERT( (pPort->Running_Slot&(1<<tag))==0 );
+	MV_DASSERT( pPort->Running_Req[tag]==0 );
+	//MV_DASSERT( (MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE)&(1<<tag))==0 );
+	//MV_DASSERT( (MV_REG_READ_DWORD(portMmio, PORT_SCR_ACT)&(1<<tag))==0 );
+
+	mv_core_set_running_slot(pPort, tag, pReq);
+	#ifdef SUPPORT_ATA_SECURITY_CMD
+	if ( pReq->Cmd_Flag&CMD_FLAG_NCQ && !((pDevice->Setting&DEVICE_SETTING_SECURITY_LOCKED)==0x10) )
+	#else
+	if ( pReq->Cmd_Flag&CMD_FLAG_NCQ)
+	#endif
+		pPort->Setting |= PORT_SETTING_NCQ_RUNNING;
+	else
+		pPort->Setting &= ~PORT_SETTING_NCQ_RUNNING;
+
+	if ( pReq->Scsi_Status==REQ_STATUS_RETRY )
+	{
+		MV_DPRINT(("Retry request[0x%p] on port[%d]\n",pReq, pPort->Id));
+		MV_DumpRequest(pReq, MV_FALSE);
+		pPort->Setting |= PORT_SETTING_DURING_RETRY;
+	}
+	else
+	{
+		pPort->Setting &= ~PORT_SETTING_DURING_RETRY;
+	}
+
+	if ( pPort->Setting&PORT_SETTING_NCQ_RUNNING )
+	{
+		MV_REG_WRITE_DWORD(portMmio, PORT_SCR_ACT, 1<<tag);
+		MV_REG_READ_DWORD(portMmio, PORT_SCR_ACT);	/* flush */
+	}
+
+	MV_REG_WRITE_DWORD(portMmio, PORT_CMD_ISSUE, 1<<tag);
+	MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE);	/* flush */
+}
+
+MV_BOOLEAN Core_WaitingForIdle(MV_PVOID pExtension)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pExtension;
+	PDomain_Port pPort = NULL;
+	MV_U8 i;
+
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		pPort = &pCore->Ports[i];
+
+		if ( pPort->Running_Slot!=0 )
+			return MV_FALSE;
+	}
+
+	return MV_TRUE;
+}
+
+MV_BOOLEAN ResetController(PCore_Driver_Extension pCore);
+
+void Core_ResetHardware(MV_PVOID pExtension)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pExtension;
+	MV_U32 i, j;
+	PDomain_Port pPort = NULL;
+	PDomain_Device pDevice = NULL;
+
+	/* Re-initialize some variables to make the reset go. */
+	pCore->Adapter_State = ADAPTER_INITIALIZING;
+	for ( i=0; i<MAX_PORT_NUMBER; i++ )
+	{
+		pPort = &pCore->Ports[i];
+		pPort->Port_State = PORT_STATE_IDLE;
+		for ( j=0; j<MAX_DEVICE_PER_PORT; j++ )
+		{
+			pDevice = &pPort->Device[j];
+			pDevice->State = DEVICE_STATE_IDLE;
+		}
+	}
+
+	/* Go through the mvAdapterStateMachine. */
+	if( pCore->Resetting==0 )
+	{
+		pCore->Resetting = 1;
+		if( !mvAdapterStateMachine(pCore,NULL) )
+		{
+			MV_ASSERT(MV_FALSE);
+		}
+	}
+	else
+	{
+		/* I suppose that we only have one chance to call Core_ResetHardware. */
+		MV_DASSERT(MV_FALSE);
+	}
+
+	return;
+}
+
+void PATA_LegacyPollSenseData(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	/*
+	 * This sense data says:
+	 * Format: Fixed format sense data
+	 * Sense key: Hardware error
+	 * Sense code and qualifier: 08h 03h LOGICAL UNIT COMMUNICATION CRC ERROR
+	 */
+	MV_U8 fakeSense[]={0xF0, 0x00, 0x04, 0x00, 0x00, 0x01,
+		0xEA, 0x0A, 0x74, 0x00, 0x00, 0x00, 0x08, 0x03, 0x00, 0x00, 0x00, 0x00};
+	MV_U32 size = MV_MIN(sizeof(fakeSense)/sizeof(MV_U8), pReq->Sense_Info_Buffer_Length);
+
+	MV_CopyMemory(pReq->Sense_Info_Buffer, fakeSense, size);
+
+}
+
+
+void Core_FillSenseData(PMV_Request pReq, MV_U8 senseKey, MV_U8 adSenseCode)
+{
+	if (pReq->Sense_Info_Buffer != NULL) {
+		((MV_PU8)pReq->Sense_Info_Buffer)[0] = 0x70;	/* Current */
+		((MV_PU8)pReq->Sense_Info_Buffer)[2] = senseKey;
+		((MV_PU8)pReq->Sense_Info_Buffer)[7] = 0;		/* additional sense length */
+		((MV_PU8)pReq->Sense_Info_Buffer)[12] = adSenseCode;	/* additional sense code */
+	}
+}
+
+
+void mvScsiInquiry(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+#ifndef _OS_BIOS
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId;
+	MV_U32 tmpLen = 0;
+
+	portId = PATA_MapPortId(pReq->Device_Id);
+	deviceId = PATA_MapDeviceId(pReq->Device_Id);
+	if ( (portId>=MAX_PORT_NUMBER)||(pReq->Device_Id >= MAX_DEVICE_NUMBER) ) {
+		pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+		return;
+	}
+	pDevice = &pCore->Ports[portId].Device[deviceId];
+	if ( (pDevice->Status & DEVICE_STATUS_FUNCTIONAL) == 0) {
+		/* Device is not functional */
+		pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+		return;
+	}
+
+	MV_ZeroMemory(pReq->Data_Buffer, pReq->Data_Transfer_Length);
+
+	if ( pReq->Cdb[1] & CDB_INQUIRY_EVPD )
+	{
+		MV_U8 MV_INQUIRY_VPD_PAGE0_DATA[6] = {0x00, 0x00, 0x00, 0x02, 0x00, 0x80};
+		MV_U8 MV_INQUIRY_VPD_PAGE83_DATA[16] = {
+			0x00, 0x83, 0x00, 0x0C, 0x01, 0x02, 0x00, 0x08,
+			0x00, 0x50, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00};
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+
+		/* Shall return the specific page of Vital Production Data */
+		switch (pReq->Cdb[2]) {
+		case 0x00:	/* Supported VPD pages */
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, 6);
+			MV_CopyMemory(pReq->Data_Buffer, MV_INQUIRY_VPD_PAGE0_DATA, tmpLen);
+			break;
+		case 0x80:	/* Unit Serial Number VPD Page */
+			if (pReq->Data_Transfer_Length > 1)
+				*(((MV_PU8)(pReq->Data_Buffer)) + 1) = 0x80;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, 4);
+			if (tmpLen >= 4) {
+				tmpLen = MV_MIN((pReq->Data_Transfer_Length-4), 20);
+				MV_CopyMemory(((MV_PU8)(pReq->Data_Buffer)+4), pDevice->Serial_Number, tmpLen);
+				*(((MV_PU8)(pReq->Data_Buffer)) + 3) = (MV_U8)tmpLen;
+				tmpLen += 4;
+			}
+			break;
+		case 0x83:	/* Device Identification VPD Page */
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, 16);
+			MV_CopyMemory(pReq->Data_Buffer, MV_INQUIRY_VPD_PAGE83_DATA, tmpLen);
+			break;
+		default:
+			pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+			Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+			break;
+		}
+		pReq->Data_Transfer_Length = tmpLen;
+	}
+	else
+	{
+		/* Standard inquiry */
+		if (pReq->Cdb[2]!=0) {
+			/* PAGE CODE field must be zero when EVPD is zero for a valid request */
+			/* sense key as ILLEGAL REQUEST and additional sense code as INVALID FIELD IN CDB */
+			pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+			Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+			return;
+		}
+		else {
+			MV_U8 Vendor[9],Product[24], temp[24];
+			MV_U8 buff[0x60]; //[42];
+			MV_U32 i, inquiryLen = 0x60; //42;
+
+			MV_ZeroMemory(buff, inquiryLen);
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, 0x60);
+			pReq->Data_Transfer_Length = tmpLen;
+
+			buff[0] = (pDevice->Device_Type&DEVICE_TYPE_ATAPI)?0x5 : 0;
+			buff[1] = (pDevice->Device_Type&DEVICE_TYPE_ATAPI)?MV_BIT(7) : 0;    /* Not Removable disk */
+			buff[2] = 0x05;    /*claim conformance to SPC-3*/
+			buff[3] = 0x12;    /* set RESPONSE DATA FORMAT to 2*/
+			buff[4] = 0x60 - 5; //42 - 5;
+			buff[6] = 0x0;     /* tagged queuing*/
+			buff[7] = 0X13;
+
+			MV_CopyMemory(temp, pDevice->Model_Number, 24);
+			for (i = 0; i < 9; i++)	{
+				if (temp[i] == ' ')
+					break;
+			}
+			if (i == 9)	{
+				if (((temp[0] == 'I') && (temp[1] == 'C')) ||
+					((temp[0] == 'H') && (temp[1] == 'T')) ||
+					((temp[0] == 'H') && (temp[1] == 'D')) ||
+					((temp[0] == 'D') && (temp[1] == 'K')))
+				{ /*Hitachi*/
+					Vendor[0] = 'H';
+					Vendor[1] = 'i';
+					Vendor[2] = 't';
+					Vendor[3] = 'a';
+					Vendor[4] = 'c';
+					Vendor[5] = 'h';
+					Vendor[6] = 'i';
+					Vendor[7] = ' ';
+					Vendor[8] = '\0';
+				}
+				else if ((temp[0] == 'S') && (temp[1] == 'T'))
+				{
+					/*Seagate*/
+					Vendor[0] = 'S';
+					Vendor[1] = 'e';
+					Vendor[2] = 'a';
+					Vendor[3] = 'g';
+					Vendor[4] = 'a';
+					Vendor[5] = 't';
+					Vendor[6] = 'e';
+					Vendor[7] = ' ';
+					Vendor[8] = '\0';
+				}
+				else
+				{
+					/*Unkown*/
+					Vendor[0] = 'A';
+					Vendor[1] = 'T';
+					Vendor[2] = 'A';
+					Vendor[3] = ' ';
+					Vendor[4] = ' ';
+					Vendor[5] = ' ';
+					Vendor[6] = ' ';
+					Vendor[7] = ' ';
+					Vendor[8] = '\0';
+				}
+				MV_CopyMemory(Product, temp, 16);
+				Product[16] = '\0';
+			}
+			else {		/* i < 9 */
+				MV_U32 j = i;
+				MV_CopyMemory(Vendor, temp, j);
+				for (; j < 9; j++)
+					Vendor[j] = ' ';
+				Vendor[8] = '\0';
+				for (; i < 24; i++)	{
+					if (temp[i] != ' ')
+						break;
+				}
+				MV_CopyMemory(Product, &temp[i], 24 - i);
+				Product[16] = '\0';
+			}
+		/*
+			MV_CopyMemory(&buff[8], Vendor, 8);
+			MV_CopyMemory(&buff[16], Product, 16);
+			MV_CopyMemory(&buff[32], pDevice->Firmware_Revision, 4);
+			MV_CopyMemory(&buff[36], "MVSATA", 6);
+		*/
+			if (tmpLen >= 16)
+				MV_CopyMemory(&buff[8], "ATA     ", 8);
+			if (tmpLen >= 32)
+				MV_CopyMemory(&buff[16], Product, 16);
+			if (tmpLen >= 36)
+				MV_CopyMemory(&buff[32], pDevice->Firmware_Revision, 4);
+			if (tmpLen >= 42)
+				MV_CopyMemory(&buff[36], "MVSATA", 6);
+
+			/*
+			* 0x00A0 SAM 5
+			* 0x0460 SPC 4
+			* 0x04C0 SBC 3
+			* 0x1EC0 SAT 2
+			*/
+			if (tmpLen >= 66) {
+				buff[58] = 0x00;
+				buff[59] = 0xA0;
+				buff[60] = 0x04;
+				buff[61] = 0x60;
+				buff[62] = 0x04;
+				buff[63] = 0xC0;
+				buff[64] = 0x1E;
+				buff[65] = 0xC0;
+			}
+
+			/*if pReq->Data_Transfer_Length <=36 ,buff[36]+ data miss*/
+			MV_CopyMemory( pReq->Data_Buffer,
+							buff,
+							tmpLen);
+							//MV_MIN(pReq->Data_Transfer_Length, inquiryLen));
+			//pReq->Data_Transfer_Length =  MV_MIN(pReq->Data_Transfer_Length, inquiryLen);
+			pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		}
+	}
+#endif	/* #ifndef _OS_BIOS */
+}
+
+void mvScsiReportLun(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	MV_U32 allocLen, lunListLen;
+	MV_PU8 pBuf = pReq->Data_Buffer;
+
+	allocLen = ((MV_U32)(pReq->Cdb[6] << 24)) |
+			   ((MV_U32)(pReq->Cdb[7] << 16)) |
+			   ((MV_U32)(pReq->Cdb[8] << 8)) |
+			   ((MV_U32)(pReq->Cdb[9]));
+
+	/* allocation length should not less than 16 bytes */
+	if (allocLen < 16) {
+		pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+		return;
+	}
+
+	MV_ZeroMemory(pBuf, pReq->Data_Transfer_Length);
+	/* Only LUN 0 has device */
+	lunListLen = 8;
+	pBuf[0] = (MV_U8)((lunListLen & 0xFF000000) >> 24);
+	pBuf[1] = (MV_U8)((lunListLen & 0x00FF0000) >> 16);
+	pBuf[2] = (MV_U8)((lunListLen & 0x0000FF00) >> 8);
+	pBuf[3] = (MV_U8)(lunListLen & 0x000000FF);
+	pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+}
+
+void mvScsiReadCapacity(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	PDomain_Device pDevice = NULL;
+	MV_LBA maxLBA;
+	MV_U32 blockLength;
+	MV_PU32 pU32Buffer;
+	MV_U8 portId, deviceId;
+
+	portId = PATA_MapPortId(pReq->Device_Id);
+	deviceId = PATA_MapDeviceId(pReq->Device_Id);
+#ifndef SECTOR_SIZE
+	#define SECTOR_SIZE	512
+#endif
+
+	MV_DASSERT( portId < MAX_PORT_NUMBER );
+
+	if ((pReq->Cdb[8] & MV_BIT(1)) == 0)
+	{
+		if ( pReq->Cdb[2] || pReq->Cdb[3] || pReq->Cdb[4] || pReq->Cdb[5] )
+		{
+			pReq->Scsi_Status = REQ_STATUS_INVALID_REQUEST;
+			return;
+		}
+	}
+
+	/*
+	 * The disk size as indicated by the ATA spec is the total addressable
+	 * sectors on the drive ; while the SCSI translation of the command
+	 * should be the last addressable sector.
+	 */
+	pDevice = &pCore->Ports[portId].Device[deviceId];
+//	maxLBA.value = pDevice->Max_LBA.value-1;
+	maxLBA = pDevice->Max_LBA;
+	blockLength = SECTOR_SIZE;
+	pU32Buffer = (MV_PU32)pReq->Data_Buffer;
+
+	if (maxLBA.parts.high != 0)
+		maxLBA.parts.low = 0xFFFFFFFF;
+
+	pU32Buffer[0] = CPU_TO_BIG_ENDIAN_32(maxLBA.parts.low);
+	pU32Buffer[1] = CPU_TO_BIG_ENDIAN_32(blockLength);
+
+	pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+}
+
+void mvScsiReadCapacity_16(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	PDomain_Device pDevice = NULL;
+	MV_U32 blockLength;
+	MV_PU32 pU32Buffer;
+	MV_U8 portId, deviceId;
+	MV_LBA maxLBA;
+
+	portId = PATA_MapPortId(pReq->Device_Id);
+	deviceId = PATA_MapDeviceId(pReq->Device_Id);
+#ifndef SECTOR_SIZE
+	#define SECTOR_SIZE	512
+#endif
+	MV_DASSERT( portId < MAX_PORT_NUMBER );
+
+	pDevice = &pCore->Ports[portId].Device[deviceId];
+	//maxLBA.value = pDevice->Max_LBA.value-1;;
+	maxLBA = pDevice->Max_LBA;
+	blockLength = SECTOR_SIZE;
+	pU32Buffer = (MV_PU32)pReq->Data_Buffer;
+	pU32Buffer[0] = CPU_TO_BIG_ENDIAN_32(maxLBA.parts.high);
+	pU32Buffer[1] = CPU_TO_BIG_ENDIAN_32(maxLBA.parts.low);
+	pU32Buffer[2] =  CPU_TO_BIG_ENDIAN_32(blockLength);
+	pReq->Scsi_Status = REQ_STATUS_SUCCESS;;
+}
+
+void Port_Monitor(PDomain_Port pPort);
+#if defined(SUPPORT_ERROR_HANDLING)
+
+MV_BOOLEAN Core_IsInternalRequest(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	PDomain_Device pDevice;
+	MV_U8 portId = PATA_MapPortId(pReq->Device_Id);
+	MV_U8 deviceId = PATA_MapDeviceId(pReq->Device_Id);
+
+	if ( portId>=MAX_PORT_NUMBER )
+		return MV_FALSE;
+	if ( deviceId>=MAX_DEVICE_PER_PORT )
+		return MV_FALSE;
+
+	pDevice = &pCore->Ports[portId].Device[deviceId];
+	if ( pReq==pDevice->Internal_Req )
+		return MV_TRUE;
+	else
+		return MV_FALSE;
+}
+
+void Core_ResetChannel_BH(MV_PVOID ext);
+
+void Core_ResetChannel(MV_PVOID Device, MV_PVOID temp)
+{
+	PDomain_Device pDevice = (PDomain_Device)Device;
+	PDomain_Port pPort = pDevice->PPort;
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PMV_Request pReq;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 tmp;
+	MV_U16 i;
+
+	//mv_core_dump_reg(pPort);
+#ifdef SUPPORT_EVENT
+	core_generate_event(pCore, EVT_ID_HD_TIMEOUT, pDevice->Id, SEVERITY_WARNING, 0, NULL);
+#endif
+
+	if (pPort->Type==PORT_TYPE_PATA){
+		tmp = MV_REG_READ_DWORD( portMmio, PORT_CMD_ISSUE );
+		MV_PRINT("CI = 0x%x\n", tmp);
+	}
+	Port_Monitor( pPort );
+
+	/* toggle the start bit in cmd register */
+	tmp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp & ~MV_BIT(0));
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp | MV_BIT(0));
+	HBA_SleepMillisecond( pCore, 100 );
+	pPort->timer_para = pDevice;
+#ifdef COMMAND_ISSUE_WORKROUND
+	if((MV_REG_READ_DWORD( portMmio, PORT_CMD ) & PORT_CMD_LIST_ON) || (pDevice->Reset_Count > CORE_MAX_RESET_COUNT))
+	{
+		pPort->Hot_Plug_Timer = 0;
+		pPort->command_callback = Core_ResetChannel_BH;
+		pPort->error_state = PORT_ERROR_AT_RUNTIME;
+		pCore->Total_Device_Count--;
+		mv_core_reset_command(pPort);
+		return;
+	}
+//	mv_core_init_reset_para(pPort);
+//	pPort->timer_para = pDevice;
+#endif
+
+	Core_ResetChannel_BH(pPort);
+
+}
+void Core_ResetChannel_BH(MV_PVOID ext)
+{
+
+	PDomain_Port pPort = (PDomain_Port)ext;
+	PDomain_Device pDevice = (PDomain_Device)pPort->timer_para;
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PMV_Request pReq;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 tmp;
+	MV_U16 i;
+	pPort->timer_para = NULL;
+	pDevice->Reset_Count++;
+
+	mv_core_put_back_request(pPort);
+
+	if ( pPort->Type == PORT_TYPE_PATA )
+		PATA_PortReset( pPort, MV_TRUE );
+	else
+		SATA_PortReset( pPort, MV_TRUE );
+	MV_DPRINT(("Finshed Core_ResetChannel_BH on port[%d].\n",pPort->Id));
+	//mv_core_init_reset_para(pPort);
+	Core_HandleWaitingList(pCore);
+}
+#endif /* SUPPORT_ERROR_HANDLING || _OS_LINUX */
+
+#define IS_SOFT_RESET_REQ(pReq) \
+	((pReq->Cdb[0]==SCSI_CMD_MARVELL_SPECIFIC)&& \
+	 (pReq->Cdb[1]==CDB_CORE_MODULE)&& \
+	 (pReq->Cdb[2]==CDB_CORE_SOFT_RESET_1 || pReq->Cdb[2]==CDB_CORE_SOFT_RESET_0))
+
+MV_BOOLEAN HandleInstantRequest(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	/*
+	 * Some of the requests can be returned immediately without hardware
+	 * access.
+	 * Handle Inquiry and Read Capacity.
+	 * If return MV_TRUE, means the request can be returned to OS now.
+	 */
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId;
+	MV_U8 ret;
+	if(IS_SOFT_RESET_REQ(pReq))
+		return MV_FALSE;
+
+#ifdef _OS_LINUX
+	if(!__is_scsi_cmd_simulated(pReq->Cdb[0])
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+	&& !IS_ATA_PASS_THROUGH_COMMAND(pReq)
+#endif
+	)
+		return	MV_FALSE;
+#endif
+	if ( pReq->Device_Id != VIRTUAL_DEVICE_ID )
+	{
+		portId = PATA_MapPortId(pReq->Device_Id);
+		deviceId = PATA_MapDeviceId(pReq->Device_Id);
+		if ( portId < MAX_PORT_NUMBER )
+			pDevice = &pCore->Ports[portId].Device[deviceId];
+	}
+
+	if (pReq->Cdb[0] == SCSI_CMD_MARVELL_SPECIFIC && pReq->Cdb[1] == CDB_CORE_MODULE)
+	{
+		if (pReq->Cdb[2] == CDB_CORE_RESET_DEVICE)
+		{
+			Core_ResetChannel(pDevice,NULL);
+			return MV_TRUE;
+		}
+	}
+
+	if (pDevice &&
+	    (pDevice->Device_Type & DEVICE_TYPE_ATAPI) &&
+	    (pDevice->Status & DEVICE_STATUS_FUNCTIONAL))
+	{
+		return MV_FALSE;
+	}
+
+	ret=MV_TRUE;
+#ifdef _OS_LINUX
+	hba_map_sg_to_buffer(pReq);
+#endif /* _OS_LINUX */
+	switch ( pReq->Cdb[0] )
+	{
+	case SCSI_CMD_INQUIRY:
+		mvScsiInquiry(pCore, pReq);
+		break;
+	case SCSI_CMD_MODE_SENSE_6:
+	case SCSI_CMD_MODE_SENSE_10:
+		mvScsiModeSense(pCore, pReq);
+		break;
+	case SCSI_CMD_REPORT_LUN:
+		mvScsiReportLun(pCore, pReq);
+		break;
+	case SCSI_CMD_READ_CAPACITY_10:
+		mvScsiReadCapacity(pCore, pReq);
+		break;
+#ifdef _OS_LINUX
+	case SCSI_CMD_READ_CAPACITY_16: /* 0x9e SERVICE_ACTION_IN */
+		if ((pReq->Cdb[1] & 0x1f)==SCSI_CMD_SAI_READ_CAPACITY_16 /* SAI_READ_CAPACITY_16 */) {
+			mvScsiReadCapacity_16(pCore, pReq);
+		}
+		else
+			pReq->Scsi_Status = REQ_STATUS_INVALID_REQUEST;
+		break;
+#endif /* _OS_LINUX */
+#ifdef SUPPORT_ATA_SMART
+	case SCSI_CMD_MODE_SELECT_6:
+	case SCSI_CMD_MODE_SELECT_10:
+		ret = mvScsiModeSelect(pCore, pReq);
+		break;
+	case SCSI_CMD_LOG_SENSE:
+		mvScsiLogSenseTranslation( pCore, pReq);
+		break;
+	case SCSI_CMD_READ_DEFECT_DATA_10:
+		mvScsiReadDefectData(pCore, pReq);
+		break;
+	case SCSI_CMD_FORMAT_UNIT:
+		pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+		break;
+#endif
+	case SCSI_CMD_REQUEST_SENSE:	/* This is only for Thor hard disk */
+	case SCSI_CMD_TEST_UNIT_READY:
+	case SCSI_CMD_RESERVE_6:	/* For Thor, just return good status */
+	case SCSI_CMD_RELEASE_6:
+#ifdef CORE_IGNORE_START_STOP_UNIT
+	case SCSI_CMD_START_STOP_UNIT:
+#endif
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		break;
+#ifdef CORE_SUPPORT_API
+	case APICDB0_PD:
+		ret=Core_pd_command(pCore, pReq);
+		break;
+#endif /* CORE_SUPPORT_API */
+	default:
+		ret = MV_FALSE;
+	}
+#ifdef _OS_LINUX
+	hba_unmap_sg_to_buffer(pReq);
+#endif /* _OS_LINUX */
+
+	return ret;
+}
+#ifdef SUPPORT_ATA_SECURITY_CMD
+void scsi_ata_check_condition(MV_Request *req, MV_U8 sense_key,
+		MV_U8 sense_code, MV_U8 sense_qualifier)
+	{
+		 req->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		 if (req->Sense_Info_Buffer) {
+			 ((MV_PU8)req->Sense_Info_Buffer)[0] = 0x70; /* As SPC-4, set Response Code
+	to 70h, or SCSI layer didn't know to set down error disk */
+			((MV_PU8)req->Sense_Info_Buffer)[2] = sense_key;
+			/* additional sense length */
+			((MV_PU8)req->Sense_Info_Buffer)[7] = 0;
+			/* additional sense code */
+			((MV_PU8)req->Sense_Info_Buffer)[12] = sense_code;
+			/* additional sense code qualifier*/
+			((MV_PU8)req->Sense_Info_Buffer)[13] = sense_qualifier;
+		 }
+	}
+u8 mv_ata_pass_through(IN PDomain_Device pDevice,IN PMV_Request req){
+
+	MV_U8 protocol, t_length, t_dir, byte, multi_rw, command;
+	MV_U32 length, tx_length = 0, cmd_flag =0;
+	MV_PU8 buf_ptr;
+
+	protocol = (req->Cdb[1] >> 1) & 0x0F;
+	if(protocol== ATA_PROTOCOL_HARD_RESET || protocol==ATA_PROTOCOL_SRST){
+		printk("ATA_16:don't support protocol=0x%x\n",protocol);
+		scsi_ata_check_condition(req, SCSI_SK_ILLEGAL_REQUEST,
+			SCSI_ASC_INVALID_FEILD_IN_CDB, 0);
+			return MV_QUEUE_COMMAND_RESULT_FINISHED;
+		}
+	multi_rw = (req->Cdb[1] >> 5) & 0x7;
+	t_length = req->Cdb[2] & 0x3;
+	byte = (req->Cdb[2] >> 2) & 0x01;
+	t_dir = (req->Cdb[2] >> 3) & 0x1;
+
+	/* Fix hdparm -A for readahead status, set sect_num */
+	if (req->Cdb[0] == ATA_12) {
+		command = req->Cdb[9];
+		if (command == ATA_CMD_IDENTIFY_ATA && req->Cdb[4] == 0)
+		req->Cdb[4] = 1;
+	} else {
+		command = req->Cdb[14];
+		if (req->Cdb[1] & 0x01)
+			cmd_flag |= CMD_FLAG_48BIT;
+		if (command == ATA_CMD_IDENTIFY_ATA && req->Cdb[6] == 0)
+			req->Cdb[6] = 1;
+	}
+
+	if (multi_rw != 0 && !IS_ATA_MULTIPLE_READ_WRITE(command)) {
+			scsi_ata_check_condition(req, SCSI_SK_ILLEGAL_REQUEST,
+			SCSI_ASC_INVALID_FEILD_IN_CDB, 0);
+			return MV_QUEUE_COMMAND_RESULT_FINISHED;
+		}
+
+		if (t_length == 0)
+			cmd_flag |= CMD_FLAG_NON_DATA;
+		else {
+			if (t_dir == 0)
+				cmd_flag |= CMD_FLAG_DATA_OUT;
+			else
+				cmd_flag |= CMD_FLAG_DATA_IN;
+		}
+
+		/* Transfer length in bytes */
+		if (byte == 0) {
+			/* Transfer length defined in Features */
+			if (t_length == 0x1) {
+				if (req->Cdb[0] == ATA_12)
+					tx_length = req->Cdb[3];
+				else
+					tx_length = req->Cdb[4];
+			}
+			/* Transfer length defined in Sector Count */
+			else if (t_length == 0x2) {
+				if (req->Cdb[0] == ATA_12)
+					tx_length = req->Cdb[4];
+				else
+					tx_length = req->Cdb[6];
+			}
+			/* t_length == 0x3 means use the length defined in the
+			nexus transaction */
+			else {
+				tx_length = req->Data_Transfer_Length;
+			}
+			/* Transfer length in sectors */
+		} else {
+			/* Transfer length defined in Features */
+			if (t_length == 0x1) {
+				if (req->Cdb[0] == ATA_12)
+					tx_length = req->Cdb[3] * 512; // fixed jyli
+				else
+					tx_length = req->Cdb[4] * 512; // fixed jyli
+			}
+			/* Transfer length defined in Sector Count */
+			else if (t_length == 0x2) {
+				if (req->Cdb[0] == ATA_12)
+					tx_length = req->Cdb[4] * 512; // fixed jyli
+				else
+					tx_length = req->Cdb[6] * 512; // fixed jyli
+			}
+			/* t_length == 0x3 means use the length defined in the
+			nexus transaction */
+			else {
+
+				tx_length = req->Data_Transfer_Length;
+			}
+		}
+	switch (protocol) {
+	case ATA_PROTOCOL_NON_DATA:
+		if (t_length != 0) {
+			scsi_ata_check_condition(req, SCSI_SK_ILLEGAL_REQUEST,
+			SCSI_ASC_INVALID_FEILD_IN_CDB, 0);
+			return MV_QUEUE_COMMAND_RESULT_FINISHED;
+		}
+
+		break;
+
+	case ATA_PROTOCOL_PIO_IN:
+		if (!(cmd_flag & CMD_FLAG_DATA_IN)) {
+			scsi_ata_check_condition(req, SCSI_SK_ILLEGAL_REQUEST,
+			SCSI_ASC_INVALID_FEILD_IN_CDB, 0);
+			return MV_QUEUE_COMMAND_RESULT_FINISHED;
+		}
+
+		cmd_flag |= CMD_FLAG_PIO;
+		break;
+
+	case ATA_PROTOCOL_PIO_OUT:
+		if (!(cmd_flag & CMD_FLAG_DATA_OUT)) {
+			scsi_ata_check_condition(req, SCSI_SK_ILLEGAL_REQUEST,
+			        SCSI_ASC_INVALID_FEILD_IN_CDB, 0);
+			return MV_QUEUE_COMMAND_RESULT_FINISHED;
+		}
+
+		cmd_flag |= CMD_FLAG_PIO;
+		break;
+
+	case ATA_PROTOCOL_DMA:
+		cmd_flag |= CMD_FLAG_DMA;
+		break;
+	case ATA_PROTOCOL_DMA_QUEUED:
+		cmd_flag |= (CMD_FLAG_DMA | CMD_FLAG_TCQ);
+		break;
+
+	case ATA_PROTOCOL_DEVICE_DIAG:
+	case ATA_PROTOCOL_DEVICE_RESET:
+		/* Do nothing(?) */
+		break;
+
+	case ATA_PROTOCOL_UDMA_IN:
+		if (!(cmd_flag & CMD_FLAG_DATA_IN)) {
+			scsi_ata_check_condition(req, SCSI_SK_ILLEGAL_REQUEST,
+			SCSI_ASC_INVALID_FEILD_IN_CDB, 0);
+			return MV_QUEUE_COMMAND_RESULT_FINISHED;
+		}
+		cmd_flag |= CMD_FLAG_DMA;
+		break;
+
+	case ATA_PROTOCOL_UDMA_OUT:
+		if (!(cmd_flag & CMD_FLAG_DATA_OUT)) {
+			scsi_ata_check_condition(req, SCSI_SK_ILLEGAL_REQUEST,
+			SCSI_ASC_INVALID_FEILD_IN_CDB, 0);
+			return MV_QUEUE_COMMAND_RESULT_FINISHED;
+		}
+		cmd_flag |= CMD_FLAG_DMA;
+		break;
+
+	case ATA_PROTOCOL_FPDMA:
+		cmd_flag |= (CMD_FLAG_DMA | CMD_FLAG_NCQ);
+		break;
+
+	case ATA_PROTOCOL_RTN_INFO:
+		req->Scsi_Status = REQ_STATUS_SUCCESS;
+		return MV_QUEUE_COMMAND_RESULT_FINISHED;
+
+	default:
+		scsi_ata_check_condition(req, SCSI_SK_ILLEGAL_REQUEST,
+		SCSI_ASC_INVALID_FEILD_IN_CDB, 0);
+
+		return MV_QUEUE_COMMAND_RESULT_FINISHED;
+	}
+	 req->Cmd_Flag=cmd_flag;
+	return 1;
+}
+#endif
+MV_QUEUE_COMMAND_RESULT
+PrepareAndSendCommand(
+	IN PCore_Driver_Extension pCore,
+	IN PMV_Request pReq
+	)
+{
+	PDomain_Device pDevice = NULL;
+	PDomain_Port pPort = NULL;
+	MV_BOOLEAN isPATA = MV_FALSE;
+	MV_U8 i, tag;
+//	MV_U8 count=0;
+	ATA_TaskFile taskFile;
+	MV_BOOLEAN ret;
+
+	/* Associate this request to the corresponding device and port */
+	pDevice = &pCore->Ports[PATA_MapPortId(pReq->Device_Id)].Device[PATA_MapDeviceId(pReq->Device_Id)];
+	pPort = pDevice->PPort;
+
+	if ( !(pDevice->Status&DEVICE_STATUS_FUNCTIONAL) )
+	{
+		pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+		return MV_QUEUE_COMMAND_RESULT_FINISHED;
+	}
+
+	/* Set the Cmd_Flag to indicate which type of commmand it is. */
+	if ( !Category_CDB_Type(pDevice, pReq) )
+	{
+		pReq->Scsi_Status = REQ_STATUS_INVALID_REQUEST;
+		/* Invalid request and can be returned to OS now. */
+		return MV_QUEUE_COMMAND_RESULT_FINISHED;
+	}
+#ifdef SUPPORT_ATA_SECURITY_CMD
+	else if(pReq->Cdb[0]==ATA_16)
+		{
+			ret=mv_ata_pass_through(pDevice,pReq);
+			if(ret==MV_QUEUE_COMMAND_RESULT_FINISHED)
+				return MV_QUEUE_COMMAND_RESULT_FINISHED;
+		}
+#endif
+	MV_DASSERT( pPort!=NULL );
+	if ( pPort->Running_Slot!=0 )	/* Some requests are running. */
+	{
+
+#ifdef COMMAND_ISSUE_WORKROUND
+		if(mv_core_check_is_reseeting(pCore)){
+			MV_DPRINT(("HBA is resetting, wait...\n"));
+			MV_DumpRequest(pReq, 0);
+			return MV_QUEUE_COMMAND_RESULT_FULL;
+		}
+#endif
+
+		if ((pDevice->Outstanding_Req >= MAX_SLOT_NUMBER - 2)
+			|| (pPort->Running_Slot == 0xFFFFFFFFL)){
+			MV_DPRINT(("running slot[0x%x], req=%d is full.\n",pPort->Running_Slot, pDevice->Outstanding_Req));
+			return MV_QUEUE_COMMAND_RESULT_FULL;
+		}
+
+		if (pReq->Cmd_Flag & CMD_FLAG_SMART)
+			return MV_QUEUE_COMMAND_RESULT_FULL;
+
+		if (	( (pReq->Cmd_Flag&CMD_FLAG_NCQ) && !(pPort->Setting&PORT_SETTING_NCQ_RUNNING) )
+				||  ( !(pReq->Cmd_Flag&CMD_FLAG_NCQ) && (pPort->Setting&PORT_SETTING_NCQ_RUNNING) )
+				|| (pReq->Scsi_Status==REQ_STATUS_RETRY)
+				|| (pPort->Setting&PORT_SETTING_DURING_RETRY)
+			)
+		{
+			return MV_QUEUE_COMMAND_RESULT_FULL;
+		}
+
+		if((pReq->Cmd_Flag&CMD_FLAG_NCQ)&&(pPort->Setting&PORT_SETTING_NCQ_RUNNING)&&(pPort->Running_Slot==0xffffffffL))
+		{
+			MV_PRINT("NCQ TAG is run out\n");
+			return MV_QUEUE_COMMAND_RESULT_FULL;
+		}
+
+		if((!(pReq->Cmd_Flag&CMD_FLAG_NCQ))&&(!(pPort->Setting&PORT_SETTING_NCQ_RUNNING))&&(pPort->Running_Slot==0xffffffffL))
+		{
+			MV_PRINT("DMA TAG is run out\n");
+			return MV_QUEUE_COMMAND_RESULT_FULL;
+		}
+
+
+		/* In order for request sense to immediately follow the error request. */
+		if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+			return MV_QUEUE_COMMAND_RESULT_FULL;
+
+		if ((pPort->Type==PORT_TYPE_PATA)&&
+			(pPort->Port_State==PORT_STATE_INIT_DONE)){
+			for (i=0; i<2; i++){
+				if ((&pPort->Device[i]!=pDevice)&& //check the other device
+				    (pPort->Device[i].Device_Type&DEVICE_TYPE_ATAPI)&&
+				    (pPort->Device[i].Status & DEVICE_STATUS_FUNCTIONAL)){
+						return MV_QUEUE_COMMAND_RESULT_FULL;
+				}
+			}//end of for
+		}
+
+		/* One request at a time */
+		if ( (pReq->Scsi_Status==REQ_STATUS_RETRY)
+			|| (pPort->Setting&PORT_SETTING_DURING_RETRY)
+			)
+			return MV_QUEUE_COMMAND_RESULT_FULL;
+	}
+
+	if ( Tag_IsEmpty(&pPort->Tag_Pool) )
+		return MV_QUEUE_COMMAND_RESULT_FULL;
+
+	isPATA = (pPort->Type==PORT_TYPE_PATA)?1:0;
+
+	/* Get one slot for this request. */
+	tag = (MV_U8)Tag_GetOne(&pPort->Tag_Pool);
+
+	if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+		ret = ATAPI_CDB2TaskFile(pDevice, pReq, &taskFile);
+	else
+		ret = ATA_CDB2TaskFile(pDevice, pReq, tag, &taskFile);
+	if ( !ret )
+	{
+		pReq->Scsi_Status = REQ_STATUS_INVALID_REQUEST;
+		Tag_ReleaseOne(&pPort->Tag_Pool, tag);
+		/* Invalid request and can be returned to OS now. */
+		return MV_QUEUE_COMMAND_RESULT_FINISHED;
+	}
+
+	if ( !isPATA )
+		SATA_PrepareCommandHeader(pPort, pReq, tag);
+	else
+		PATA_PrepareCommandHeader(pPort, pReq, tag);
+
+
+	if ( !isPATA )
+		SATA_PrepareCommandTable(pPort, pReq, tag, &taskFile);
+	else
+		PATA_PrepareCommandTable(pPort, pReq, tag, &taskFile);
+
+	/* in some cases, when preparing command table, a consolidated
+	   request would cause PRD entries to run out. In this case, we
+	   return this request to re-try without consolidating */
+	/* This is assuming that REQ_STATUS_BUSY is ONLY used for these cases */
+	if (pReq->Scsi_Status == REQ_STATUS_BUSY)
+		return MV_QUEUE_COMMAND_RESULT_FINISHED;
+
+	SATA_SendFrame(pPort, pReq, tag);
+	/* Request is sent to the hardware and not finished yet. */
+	return MV_QUEUE_COMMAND_RESULT_SENT;
+}
+
+void Core_HandleWaitingList(PCore_Driver_Extension pCore)
+{
+	PMV_Request pReq = NULL;
+	MV_QUEUE_COMMAND_RESULT result;
+#ifdef SUPPORT_HOT_PLUG
+	PDomain_Device pDevice;
+	MV_U8 portId, deviceId;
+#endif
+#if defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX)
+	MV_U32 timeout;
+#endif /* efined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX) */
+
+	/* Get the request header */
+	while ( !List_Empty(&pCore->Waiting_List) )
+	{
+		pReq = (PMV_Request) List_GetFirstEntry(&pCore->Waiting_List,
+							MV_Request,
+							Queue_Pointer);
+		if ( NULL == pReq ) {
+			MV_ASSERT(0);
+			break;
+		}
+
+		if (pReq->Cdb[0]==0xEC){
+			MV_DPRINT(("Catch the 0xEC cmd.\n"));
+		}
+
+#if defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX)
+		pReq->eh_flag = 0;
+//		hba_init_timer(pReq);
+#endif /* defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX) */
+
+		/* During reset, we still have internal requests need to
+		 *be handled. */
+
+		// Internal request is always at the beginning.
+		if ( (pCore->Need_Reset)&&(pReq->Cmd_Initiator!=pCore) )
+		{
+			/* Return the request back. */
+			List_Add(&pReq->Queue_Pointer, &pCore->Waiting_List);
+			return;
+		}
+
+#ifdef SUPPORT_HOT_PLUG
+		/* hot plug - device is gone, reject this request */
+		if ( pReq->Device_Id != VIRTUAL_DEVICE_ID )
+		{
+			portId = PATA_MapPortId(pReq->Device_Id);
+			deviceId = PATA_MapDeviceId(pReq->Device_Id);
+			pDevice = &pCore->Ports[portId].Device[deviceId];
+
+			if ( !(pDevice->Status & DEVICE_STATUS_FUNCTIONAL) )
+			{
+				pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+				CompleteRequest(pCore, pReq, NULL);
+				return;
+			}
+
+			/* Reset is not done yet. */
+			if ( pDevice->State!=DEVICE_STATE_INIT_DONE )
+			{
+				/* check if it is the reset commands */
+				if ( !Core_IsInternalRequest(pCore, pReq) )
+				{
+					List_Add(&pReq->Queue_Pointer, &pCore->Waiting_List); /* Return the request back. */
+					return;
+				}
+				else
+				{
+					/*
+					 * Cannot be the request sense.
+					 * It's not pushed back.
+					 */
+					MV_ASSERT( !SCSI_IS_REQUEST_SENSE(pReq->Cdb[0]) );
+				}
+			}
+		}
+#endif /* SUPPORT_HOT_PLUG */
+
+		/* Whether we can handle this request without hardware access? */
+		if ( HandleInstantRequest(pCore, pReq) )
+		{
+			CompleteRequest(pCore, pReq, NULL);
+			continue;
+		}
+
+		/* handle the cmd which data length is > 128k
+		 * We suppose the data length was multiples of 128k first.
+		 * If not, we will still verify multiples of 128k since
+		 * no data transfer.
+		 */
+		if(pReq->Cdb[0] == SCSI_CMD_VERIFY_10)
+		{
+			PDomain_Device pDevice = &pCore->Ports[PATA_MapPortId(pReq->Device_Id)].Device[PATA_MapDeviceId(pReq->Device_Id)];
+			MV_U32 sectors = SCSI_CDB10_GET_SECTOR(pReq->Cdb);
+
+			if((!(pDevice->Capacity&DEVICE_CAPACITY_48BIT_SUPPORTED)) && (sectors > MV_MAX_TRANSFER_SECTOR)){
+				MV_ASSERT(!pReq->Splited_Count );
+				pReq->Splited_Count = (MV_U8)((sectors + MV_MAX_TRANSFER_SECTOR -1)/MV_MAX_TRANSFER_SECTOR) - 1;
+				sectors = MV_MAX_TRANSFER_SECTOR;
+				SCSI_CDB10_SET_SECTOR(pReq->Cdb, sectors);
+			}
+		}
+
+		result = PrepareAndSendCommand(pCore, pReq);
+
+		switch ( result )
+		{
+			case MV_QUEUE_COMMAND_RESULT_FINISHED:
+				CompleteRequest(pCore, pReq, NULL);
+				break;
+
+			case MV_QUEUE_COMMAND_RESULT_FULL:
+				List_Add(&pReq->Queue_Pointer, &pCore->Waiting_List);
+				return;
+
+			case MV_QUEUE_COMMAND_RESULT_SENT:
+			{
+				portId = PATA_MapPortId(pReq->Device_Id);
+				deviceId = PATA_MapDeviceId(pReq->Device_Id);
+				pDevice = &pCore->Ports[portId].Device[deviceId];
+				pDevice->Outstanding_Req++;
+#if defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX)
+				/*
+				 * timeout to 15 secs if the port has just
+				 * been reset.
+				 */
+				if ( pReq->eh_flag ) {
+					timeout = HBA_REQ_TIMER_AFTER_RESET;
+					pReq->eh_flag = 0;
+				} else {
+					timeout = HBA_REQ_TIMER;
+				}
+
+				if (pDevice->Device_Type & DEVICE_TYPE_ATAPI)
+					timeout = timeout * 20 + 5;
+			#ifdef SUPPORT_ATA_SMART
+				if(pReq->Cdb[0]==SCSI_CMD_SND_DIAG)
+					timeout = timeout * 10;
+			#endif
+				hba_init_timer(pReq);
+
+				hba_add_timer(pReq,
+					      timeout,
+					      __core_req_timeout_handler);
+				/*in captive mode, maybe timeout.*/
+			#ifdef SUPPORT_ATA_SMART
+				if(pReq->Cdb[0]== SCSI_CMD_MARVELL_SPECIFIC &&
+					pReq->Cdb[1]== CDB_CORE_MODULE &&
+						pReq->Cdb[2] == CDB_CORE_ATA_SMART_IMMEDIATE_OFFLINE){
+							hba_remove_timer(pReq);
+				}
+			#endif
+#elif defined(SUPPORT_ERROR_HANDLING)
+#ifdef SUPPORT_TIMER
+				/* start timer for error handling */
+				if( pDevice->Timer_ID == NO_CURRENT_TIMER )
+				{
+					// if no timer is running right now
+					if (pDevice->Device_Type&DEVICE_TYPE_ATAPI){
+//						MV_DASSERT(pDevice->Outstanding_Req==1);
+						if (pReq->Time_Out!=0){
+							pDevice->Timer_ID = Timer_AddRequest( pCore, pReq->Time_Out*2, Core_ResetChannel, pDevice, NULL );
+						}else {
+							pDevice->Timer_ID = Timer_AddRequest( pCore, REQUEST_TIME_OUT, Core_ResetChannel, pDevice, NULL );
+						}
+					}else {
+						pDevice->Timer_ID = Timer_AddRequest( pCore, REQUEST_TIME_OUT, Core_ResetChannel, pDevice, NULL );
+					}
+				}
+#endif /* SUPPORT_TIMER */
+#endif /* defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX) */
+#if 0
+				{
+					MV_U8 i;
+					PMV_Request pTmpRequest = NULL;
+					PDomain_Port pPort = pDevice->PPort;
+					/* When there is reset command, other commands won't come here. */
+					if ( SCSI_IS_READ(pReq->Cdb[0]) || SCSI_IS_WRITE(pReq->Cdb[0]) )
+					{
+						for ( i=0; i<MAX_SLOT_NUMBER; i++ )
+						{
+							pTmpRequest = pPort->Running_Req[i];
+							if ( pTmpRequest && (pTmpRequest->Device_Id==pReq->Device_Id) )
+							{
+								MV_DASSERT( !SCSI_IS_INTERNAL(pTmpRequest->Cdb[0]) );
+							}
+						}
+
+					}
+				}
+#endif /* 0 */
+				break;
+			}
+			default:
+				MV_ASSERT(MV_FALSE);
+		}
+	}
+
+#ifdef SUPPORT_CONSOLIDATE
+	{
+		MV_U8 i,j;
+		PDomain_Port pPort;
+
+		if ( pCore->Is_Dump ) return;
+
+		/*
+		* If there is no more request we can do,
+		* force command consolidate to run the holding request.
+		*/
+		for ( i=0; i<MAX_PORT_NUMBER; i++ )
+		{
+			pPort = &pCore->Ports[i];
+			for ( j=0; j<MAX_DEVICE_PER_PORT; j++ )
+			{
+				if ( (pPort->Device[j].Status&DEVICE_STATUS_FUNCTIONAL)
+					&& (pPort->Device[j].Outstanding_Req==0) )
+				{
+					Consolid_PushSendRequest(pCore, i*MAX_DEVICE_PER_PORT+j);
+				}
+			}
+		}
+	}
+#endif /* SUPPORT_CONSOLIDATE */
+}
+
+/*
+ * Interrupt service routine and related funtion
+ * We can split this function to two functions.
+ * One is used to check and clear interrupt, called in ISR.
+ * The other is used in DPC.
+ */
+void SATA_PortHandleInterrupt(
+	IN PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort
+	);
+void PATA_PortHandleInterrupt(
+	IN PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort
+	);
+void SATA_HandleSerialError(
+	IN PDomain_Port pPort,
+	IN MV_U32 serialError
+	);
+void SATA_HandleHotplugInterrupt(
+	IN PDomain_Port pPort,
+	IN MV_U32 intStatus
+	);
+
+MV_BOOLEAN Core_InterruptServiceRoutine(MV_PVOID This)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	MV_U32	irqStatus;
+	MV_U8 i;
+	PDomain_Port pPort = NULL;
+
+	/* Get interrupt status */
+	irqStatus = MV_REG_READ_DWORD(pCore->Mmio_Base, HOST_IRQ_STAT);
+	irqStatus &= pCore->Port_Map;
+#ifndef  SUPPORT_TASKLET
+	if (!irqStatus) {
+		return MV_FALSE;
+	}
+
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		/* no interrupt for this port. */
+		if (!(irqStatus&(1<<i)))
+			continue;
+
+		pPort = &pCore->Ports[i];
+		if ( pPort->Type==PORT_TYPE_PATA )
+			PATA_PortHandleInterrupt(pCore, pPort);
+		else
+			SATA_PortHandleInterrupt(pCore, pPort);
+	}
+
+	/* If we need to do hard reset. And the controller is idle now. */
+	if ((pCore->Need_Reset) && (!pCore->Resetting))
+	{
+		if (Core_WaitingForIdle(pCore))
+			Core_ResetHardware(pCore);
+	}
+
+	Core_HandleWaitingList(pCore);
+#else
+	pCore->Saved_ISR_Status=irqStatus;
+#endif
+	return MV_TRUE;
+}
+
+#ifdef  SUPPORT_TASKLET
+MV_BOOLEAN Core_HandleServiceRoutine(MV_PVOID This)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	MV_U8 i;
+	PDomain_Port pPort = NULL;
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		/* no interrupt for this port. */
+		if (!(pCore->Saved_ISR_Status&(1<<i)))
+			continue;
+
+		pPort = &pCore->Ports[i];
+		if ( pPort->Type==PORT_TYPE_PATA )
+			PATA_PortHandleInterrupt(pCore, pPort);
+		else
+			SATA_PortHandleInterrupt(pCore, pPort);
+	}
+
+	/* If we need to do hard reset. And the controller is idle now. */
+	if ((pCore->Need_Reset) && (!pCore->Resetting))
+	{
+		if (Core_WaitingForIdle(pCore))
+			Core_ResetHardware(pCore);
+	}
+
+	Core_HandleWaitingList(pCore);
+
+	return MV_TRUE;
+}
+#endif
+
+void SATA_HandleSerialError(
+	IN PDomain_Port pPort,
+	IN MV_U32 serialError
+	)
+{
+	MV_DPRINT(("Error: port=%d  Serial error=0x%x.\n", pPort->Id, serialError));
+}
+
+void SATA_ResetPort(PCore_Driver_Extension pCore, MV_U8 portId);
+
+#ifdef COMMAND_ISSUE_WORKROUND
+
+void SATA_ResetPort(PCore_Driver_Extension pCore, MV_U8 portId);
+MV_BOOLEAN ResetController(PCore_Driver_Extension pCore);
+void InitChip(PCore_Driver_Extension pCore);
+void mvHandleDevicePlugin_BH(MV_PVOID ext);
+
+#define CORE_ERROR_HANDLE_RESET_DEVICE		1
+#define CORE_ERROR_HANDLE_RESET_PORT			2
+#define CORE_ERROR_HANDLE_RESET_HBA			3
+
+
+MV_U8 mv_core_reset_port(PDomain_Port pPort)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pPort->Core_Extension;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_U32 temp=0,count=0;
+	MV_U32 old_stat;
+	MV_U32 issue_reg=0;
+	MV_U8 ret=MV_TRUE;
+
+	//mvDisableIntr(portMmio, old_stat);
+
+	/* Toggle should before we clear the channel interrupt status but not the global interrupt. */
+	MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+#if 1
+	{
+		MV_U16 counter;
+		{
+			/*Doing PHY reset*/
+			MV_U32 SControl = MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);
+			SControl &= ~0x000000FF;
+#ifdef FORCE_1_5_G
+			SControl |= 0x11;
+#else
+			SControl |= 0x21;
+#endif
+			MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+			MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);	/* flush */
+			HBA_SleepMillisecond(pCore, 2);
+//			hba_msleep(2);
+
+			SControl &= ~0x0000000F;
+			MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+			MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);	/* flush */
+			HBA_SleepMillisecond(pCore, 10);
+//			hba_msleep(10);
+		}
+
+		/*Waiting PHY Ready*/
+		counter = 200;
+		while(((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT) & 0x0f) != 0x03) && (counter > 0)) {
+			HBA_SleepMillisecond(pCore, 10);
+			//hba_msleep(10);
+			counter --;
+		}
+
+		if (counter > 0) {
+			/*Some HD update status too slow, so the workaround to wait HD ready*/
+			counter = 200;
+			while (((MV_REG_READ_DWORD(portMmio, PORT_TFDATA) & 0xff) != 0x50) && (counter > 0)) {
+				HBA_SleepMillisecond(pCore, 10);
+				//hba_msleep(10);
+				counter --;
+			}
+		}
+	}
+#endif
+
+	/* Always turn the PM bit on - otherwise won't work! */
+	temp = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+	MV_REG_WRITE_DWORD(portMmio, PORT_CMD, temp | MV_BIT(17));
+	temp=MV_REG_READ_DWORD(portMmio, PORT_CMD);	/* flush */
+#if 1
+	/*Clear Busy bit and error bit, do spin-up device, power on device*/
+	temp = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+	MV_REG_WRITE_DWORD(portMmio, PORT_CMD, temp | MV_BIT(3) | MV_BIT(2) | MV_BIT(1));
+	temp=MV_REG_READ_DWORD(portMmio, PORT_CMD);	/* flush */
+#endif
+	/* hardware workaround - send dummy FIS first to clear FIFO */
+	temp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp & ~PORT_CMD_START);
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp |PORT_CMD_START);
+	Tag_Init( &pPort->Tag_Pool, pPort->Tag_Pool.Size );
+	//sendDummyFIS( pPort );
+
+	// start command handling on this port
+	temp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp & ~PORT_CMD_START);
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp | PORT_CMD_START);
+	HBA_SleepMillisecond(pCore, 2000);
+	//hba_msleep(2000);
+	// reset the tag stack - to guarantee soft reset is issued at slot 0
+	Tag_Init( &pPort->Tag_Pool, pPort->Tag_Pool.Size );
+
+	// make sure CI is cleared before moving on
+	issue_reg = MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE);
+	while (issue_reg != 0 && count < 10) {
+		count++;
+		temp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+		MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp & ~PORT_CMD_START);
+		MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp | PORT_CMD_START);
+		HBA_SleepMillisecond(pCore, 1000);
+		//hba_msleep(1000);
+		issue_reg = MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE);
+	}
+
+	//mvEnableIntr(portMmio, old_stat);
+
+	temp = MV_REG_READ_DWORD(portMmio, PORT_IRQ_STAT);
+	if(temp){
+		MV_DPRINT(("port reset but port status can not be clean[0x%x] on port[%d].\n",MV_REG_READ_DWORD( portMmio, PORT_IRQ_STAT), pPort->Id));
+		MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_STAT, temp);
+	}
+
+	if(MV_REG_READ_DWORD( portMmio, PORT_CMD) & PORT_CMD_LIST_ON){
+		MV_PRINT("port reset but command running can not be clean[0x%x] on port[%d].\n",MV_REG_READ_DWORD( portMmio, PORT_CMD_ISSUE), pPort->Id);
+		ret = MV_FALSE;
+	}
+
+	if (issue_reg != 0)
+	{
+		MV_PRINT("port reset but CI can not be clean[0x%x] on port[%d].\n",MV_REG_READ_DWORD( portMmio, PORT_CMD_ISSUE), pPort->Id);
+		ret = MV_FALSE;
+	}
+
+	return ret;
+}
+
+
+MV_U8 mv_core_reset_hba(PDomain_Port pPort)
+{
+
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pPort->Core_Extension;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_U32 tmp;
+	pCore->error_handle_state = CORE_ERROR_HANDLE_RESET_HBA;
+	MV_REG_WRITE_DWORD(mmio, HOST_CTL, 0);
+	MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_MASK, 0);
+
+	//Reset port command start.
+	tmp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp & ~PORT_CMD_START);
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp | PORT_CMD_START);
+	HBA_SleepMillisecond(pCore, 500);
+	//hba_msleep(500);
+
+	if(ResetController(pCore) == MV_FALSE) {
+		MV_DPRINT(("Reset controller failed."));
+		return MV_FALSE;
+
+	}
+	InitChip(pCore);
+
+
+	MV_DPRINT(("Finished reset hba for port[%d], CMD=0x%x.\n",pPort->Id, MV_REG_READ_DWORD(portMmio, PORT_CMD)));
+	//mv_core_dump_reg(pPort);
+	return MV_TRUE;
+}
+
+void mv_core_reset_command_in_timer(PDomain_Port pPort)
+{
+	unsigned long flags;
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pPort->Core_Extension;
+	MV_DPRINT(("start handle reset command in timer for port[%d].\n",pPort->Id));
+	spin_lock_irqsave(&pCore->desc->hba_desc->global_lock, flags);
+	mv_core_reset_command(pPort);
+	spin_unlock_irqrestore(&pCore->desc->hba_desc->global_lock, flags);
+
+}
+
+MV_U8 mv_core_check_is_reseeting(MV_PVOID core_ext)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)core_ext;
+	return	pCore->resetting_command;
+}
+
+
+void mv_core_init_reset_para(PDomain_Port pPort)
+{
+
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pPort->Core_Extension;
+	pCore->resetting_command = MV_FALSE;
+	pPort->command_callback = NULL;
+	pPort->timer_para = NULL;
+	pPort->Hot_Plug_Timer = 0;
+	pPort->find_disk = MV_FALSE;
+//	pPort->reset_cmd_times= 0;
+
+}
+void mv_core_put_back_request(PDomain_Port pPort)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pPort->Core_Extension;
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 i,j;
+	PMV_Request pReq = NULL;
+	PDomain_Device pDevice=NULL;
+	/* put all the running requests back into waiting list */
+	for ( i=0; i<MAX_SLOT_NUMBER; i++ )
+	{
+		pReq = pPort->Running_Req[i];
+		if (pReq) {
+			/*
+			 * If this channel has multiple devices, pReq is
+			 * not the internal request of pDevice
+			 */
+			if ( !Core_IsInternalRequest(pCore, pReq) )
+			{
+				List_AddTail(&pReq->Queue_Pointer, &pCore->Waiting_List);
+			}
+			else
+			{
+				/* Can be reset command or request sense command */
+				if ( SCSI_IS_REQUEST_SENSE(pReq->Cdb[0]) )
+				{
+					MV_ASSERT( pReq->Org_Req!=NULL );
+					if ( pReq->Org_Req )
+						List_AddTail( &((PMV_Request)pReq->Org_Req)->Queue_Pointer, &pCore->Waiting_List);
+				}
+			}
+
+			hba_remove_timer(pReq);
+			pReq->eh_flag = 1;
+			mv_core_reset_running_slot(pPort, i);
+		}
+	}
+	MV_DASSERT(pPort->Running_Slot == 0);
+
+	/* reset device related variables */
+	for ( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+	{
+		pDevice = &pPort->Device[i];
+
+//		pDevice->Device_Type = 0;
+//		pDevice->Need_Notify = MV_FALSE;
+#ifdef SUPPORT_TIMER
+		if( pDevice->Timer_ID != NO_CURRENT_TIMER )
+		{
+			Timer_CancelRequest( pCore, pDevice->Timer_ID );
+			pDevice->Timer_ID = NO_CURRENT_TIMER;
+		}
+#endif /* SUPPORT_TIMER */
+		pDevice->Outstanding_Req = 0;
+
+		/*
+		 * Go through the waiting list. If there is some reset
+		 * request, remove that request.
+		 */
+		mvRemoveDeviceWaitingList(pCore, pDevice->Id, MV_FALSE);
+	}
+
+	// reset the tag stack - to guarantee soft reset is issued at slot 0
+	Tag_Init( &pPort->Tag_Pool, pPort->Tag_Pool.Size );
+
+	for( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+	{
+		if( (pPort->Device[i].Status & DEVICE_STATUS_FUNCTIONAL) &&
+			(pPort->Device[i].Internal_Req != NULL) )
+		{
+			if (pCore->Total_Device_Count && (pPort->Port_State == PORT_STATE_INIT_DONE))
+				pCore->Total_Device_Count--;
+			ReleaseInternalReqToPool( pCore, pPort->Device[i].Internal_Req );
+			pPort->Device[i].Internal_Req = NULL;
+		}
+	}
+}
+
+
+#define MAX_WAIT_TIMES	3
+void  mv_core_reset_command(PDomain_Port pPort)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pPort->Core_Extension;
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 tmp;
+	MV_U32 port_id, slot_id;
+	PMV_Request pReq = NULL;
+	PDomain_Port pCheckPort;
+
+	Timer_CancelRequest(pPort, NULL);
+	MV_DPRINT(("Reset HBA times=%d, Hot_Plug_Timer=%d.\n",pPort->reset_hba_times, pPort->Hot_Plug_Timer));
+
+	/* check disk exist */
+	if ((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT) & 0xf) == 0)
+		return;
+
+	pPort->Hot_Plug_Timer++;
+
+	pCore->resetting_command = MV_TRUE;
+	for(port_id=0;port_id<MAX_PORT_NUMBER;port_id++){
+		pCheckPort = &pCore->Ports[port_id];
+		if((pCheckPort != pPort) && pCheckPort->Running_Slot){
+			if(pPort->Hot_Plug_Timer < (MAX_WAIT_TIMES -1)){
+				MV_DPRINT(("Wait port[%d] running request[0x%x].\n",pCheckPort->Id, pCheckPort->Running_Slot));
+				Timer_AddRequest(pPort, 1, mv_core_reset_command_in_timer, pPort, NULL);
+				return;
+			} else {
+				MV_DPRINT(("Abort port[%d] running request[0x%x].\n",pCheckPort->Id, pCheckPort->Running_Slot));
+				mv_core_put_back_request(pCheckPort);
+			}
+		}
+	}
+
+
+	if(pPort->Hot_Plug_Timer > MAX_WAIT_TIMES){
+		MV_DPRINT(("ERROR: Wait too long time for command running bit, failed detecting port[%d].\n",pPort->Id));
+		mv_core_init_reset_para(pPort);
+		return;
+	}
+
+	pPort->reset_hba_times++;
+	if(pPort->reset_hba_times > MAX_RESET_TIMES)
+	{
+		MV_DPRINT(("Has reset command on port [%d] more than %d.\n",pPort->Id, MAX_RESET_TIMES));
+		mv_core_init_reset_para(pPort);
+		return;
+	}
+
+	pPort->Port_State = PORT_STATE_IDLE;
+	mv_core_reset_hba(pPort);
+	MV_DPRINT(("Re-enable AHCI mode on port[%d].\n",pPort->Id));
+	if(pPort->command_callback){
+		MV_DPRINT(("reset hba callback on port[%d].\n",pPort->Id));
+		pPort->command_callback( pPort);
+	}
+	return;
+}
+
+void mv_core_dump_reg(PDomain_Port pPort)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pPort->Core_Extension;
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_DPRINT(("Global status=0x%x.\n", MV_REG_READ_DWORD(mmio, HOST_IRQ_STAT)));
+	MV_DPRINT(("Global control=0x%x.\n", MV_REG_READ_DWORD(mmio, HOST_CTL)));
+	MV_DPRINT(("Port[%d] command =0x%x.\n",pPort->Id,  MV_REG_READ_DWORD( portMmio, PORT_CMD )));
+	MV_DPRINT(("Port[%d] command issue =0x%x.\n",pPort->Id,  MV_REG_READ_DWORD( portMmio, PORT_CMD_ISSUE)));
+	MV_DPRINT(("Port[%d] irq status=0x%x.\n",pPort->Id,  MV_REG_READ_DWORD(portMmio, PORT_IRQ_STAT)));
+}
+#endif	//COMMAND_ISSUE_WORKROUND
+
+#ifdef SUPPORT_HOT_PLUG
+void Device_SoftReset(PDomain_Port pPort, PDomain_Device pDevice);
+
+void mvRemoveDeviceWaitingList( MV_PVOID This, MV_U16 deviceId, MV_BOOLEAN returnOSRequest )
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	PMV_Request pReq = NULL;
+	List_Head *pPos;
+	List_Head remove_List;
+	MV_U8 count = 0, myCount=0, i;
+	PDomain_Device pDevice;
+	MV_U8 portNum = PATA_MapPortId(deviceId);
+	MV_U8 deviceNum = PATA_MapDeviceId(deviceId);
+	pDevice = &pCore->Ports[portNum].Device[deviceNum];
+
+	LIST_FOR_EACH(pPos, &pCore->Waiting_List) {
+		count++;
+	}
+
+	if (count!=0){
+		MV_LIST_HEAD_INIT(&remove_List);
+	}
+
+	/*
+	 * If returnOSRequest is MV_FALSE, actually we just remove the
+	 * internal reset command.
+	 */
+	while ( count>0 )
+	{
+		pReq = (PMV_Request)List_GetFirstEntry(&pCore->Waiting_List, MV_Request, Queue_Pointer);
+
+		if ( pReq->Device_Id==deviceId )
+		{
+			if ( !Core_IsInternalRequest(pCore, pReq) )
+			{
+				if ( returnOSRequest ) {
+					pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+					List_AddTail(&pReq->Queue_Pointer, &remove_List);
+					myCount++;
+				} else {
+					List_AddTail(&pReq->Queue_Pointer, &pCore->Waiting_List);
+				}
+			}
+			else
+			{
+				/* Reset command or request sense */
+				if ( SCSI_IS_REQUEST_SENSE(pReq->Cdb[0]) )
+				{
+					MV_ASSERT( pReq->Org_Req!=NULL );
+					pReq = (PMV_Request)pReq->Org_Req;
+					if ( pReq ) {
+						if ( returnOSRequest ) {
+							pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+							List_AddTail(&pReq->Queue_Pointer, &remove_List);
+							myCount++;
+						} else {
+							List_AddTail(&pReq->Queue_Pointer, &pCore->Waiting_List);
+						}
+					}
+				} else {
+					/* Reset command is removed. */
+				}
+			}
+		}
+		else
+		{
+			List_AddTail(&pReq->Queue_Pointer, &pCore->Waiting_List);
+		}
+		count--;
+	}//end of while
+
+	for (i=0; i<myCount; i++){
+		pReq = (PMV_Request)List_GetFirstEntry(&remove_List, MV_Request, Queue_Pointer);
+		MV_DASSERT(pReq && (pReq->Scsi_Status==REQ_STATUS_NO_DEVICE));
+		CompleteRequest(pCore, pReq, NULL);
+	}//end of for
+}
+
+void mvRemovePortWaitingList( MV_PVOID This, MV_U8 portId )
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	PMV_Request pReq;
+	List_Head *pPos;
+	List_Head remove_List;
+	MV_U8 count = 0, myCount=0, i;
+
+	LIST_FOR_EACH(pPos, &pCore->Waiting_List) {
+		count++;
+	}
+
+	if (count!=0){
+		MV_LIST_HEAD_INIT(&remove_List);
+	}
+
+	while ( count>0 )
+	{
+		pReq = (PMV_Request)List_GetFirstEntry(&pCore->Waiting_List, MV_Request, Queue_Pointer);
+		if ( PATA_MapPortId(pReq->Device_Id) == portId )
+		{
+			if ( pReq->Cmd_Initiator==pCore ) {
+				if ( SCSI_IS_READ(pReq->Cdb[0]) || SCSI_IS_WRITE(pReq->Cdb[0]) ) {
+					/* Command consolidate, should return */
+					pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+					List_AddTail(&pReq->Queue_Pointer, &remove_List);
+					myCount++;
+				} else if ( SCSI_IS_REQUEST_SENSE(pReq->Cdb[0]) ) {
+					/* Request sense */
+					MV_ASSERT( pReq->Org_Req!=NULL );
+					pReq = (PMV_Request)pReq->Org_Req;
+					if ( pReq ) {
+						pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+						List_AddTail(&pReq->Queue_Pointer, &remove_List);
+						myCount++;
+					}
+				} else {
+					/* Reset command. Ignore. */
+				}
+			} else {
+				pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+				List_AddTail(&pReq->Queue_Pointer, &remove_List);
+				myCount++;
+			}
+		}
+		else
+		{
+			List_AddTail(&pReq->Queue_Pointer, &pCore->Waiting_List);
+		}
+		count--;
+	}//end of while
+
+	for (i=0; i<myCount; i++){
+		pReq = (PMV_Request)List_GetFirstEntry(&remove_List, MV_Request, Queue_Pointer);
+		MV_DASSERT(pReq && (pReq->Scsi_Status==REQ_STATUS_NO_DEVICE));
+		CompleteRequest(pCore, pReq, NULL);
+	}//end of for
+
+}
+#ifdef HOTPLUG_ISSUE_WORKROUND
+void mvHandleDeviceUnplugReset (MV_PVOID pport, MV_PVOID temp);
+#endif
+void mvHandleDeviceUnplug (PCore_Driver_Extension pCore, PDomain_Port pPort)
+{
+	PMV_Request pReq;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U8 i;
+	MV_U32 temp;
+
+	#ifdef HOTPLUG_ISSUE_WORKROUND
+	PDomain_Device pDevice = &pPort->Device[0];
+	MV_U32 SControl = 0;
+	#endif
+#ifdef COMMAND_ISSUE_WORKROUND
+	MV_DPRINT(("Check port[%d] running slot[0x%x].\n",pPort->Id, pPort->Running_Slot));
+	Timer_CancelRequest(pPort, NULL);
+#endif
+	if( !SATA_PortDeviceDetected(pPort) )
+	{
+		/* clear the start bit in cmd register,
+		   stop the controller from handling anymore requests */
+		temp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+		MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp & ~PORT_CMD_START);
+
+		/* Device is gone. Return the Running_Req */
+		for ( i=0; i<MAX_SLOT_NUMBER; i++ )
+		{
+			pReq =  pPort->Running_Req[i];
+			if ( pReq !=NULL )
+			{
+				pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+				CompleteRequestAndSlot(pCore, pPort, pReq, NULL, i);
+			}
+		}
+
+		if( pPort->Type == PORT_TYPE_PM )
+		{
+			pPort->Setting &= ~PORT_SETTING_PM_FUNCTIONAL;
+			pPort->Setting &= ~PORT_SETTING_PM_EXISTING;
+		}
+
+		SATA_PortReportNoDevice( pCore, pPort );
+		#ifdef HOTPLUG_ISSUE_WORKROUND
+		if( pPort->Type != PORT_TYPE_PM ){
+
+			mvDisableIntr(portMmio, pPort->old_stat);
+
+			SControl = MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);
+			SControl &= ~0x0000000F;
+			SControl |= 0x4;    // Disable PHY
+			MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+			MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);	/* flush */
+			HBA_SleepMillisecond(pCore, 10);
+
+			MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+			MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);	/* flush */
+			HBA_SleepMillisecond(pCore, 10);
+
+			pDevice->Status = DEVICE_STATUS_UNPLUG;
+			MV_DPRINT(("######### Device UNPLUG on PORT irq_mask=0x%x#########\n",pPort->old_stat));
+
+			Timer_AddRequest( pPort, 8, mvHandleDeviceUnplugReset, pPort, NULL);
+			}
+		#endif
+	}
+	else
+	{
+		MV_DPRINT(("===== Not detect that device is out =====\n"));
+	}
+}
+
+void sendDummyFIS( PDomain_Port pPort )
+{
+	MV_U16 tag = Tag_GetOne(&pPort->Tag_Pool);
+	PMV_Command_Header header = SATA_GetCommandHeader(pPort, tag);
+	PMV_Command_Table pCmdTable = Port_GetCommandTable(pPort, tag);
+	PSATA_FIS_REG_H2D pFIS = (PSATA_FIS_REG_H2D)pCmdTable->FIS;
+#if 1//ndef _OS_LINUX
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+#endif
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 old_stat;
+	MV_U32 temp=0, count=0;
+	MV_DASSERT( tag == 0 );
+
+	mvDisableIntr(portMmio, old_stat);
+
+	MV_ZeroMemory(header, sizeof(MV_Command_Header));
+	MV_ZeroMemory(pCmdTable, sizeof(MV_Command_Table));
+
+	header->FIS_Length = 0;
+	header->Reset = 0;
+	header->PM_Port = 0xE;
+
+	header->Table_Address = pPort->Cmd_Table_DMA.parts.low + SATA_CMD_TABLE_SIZE*tag;
+	header->Table_Address_High = pPort->Cmd_Table_DMA.parts.high;
+
+	pFIS->FIS_Type = SATA_FIS_TYPE_REG_H2D;
+	pFIS->PM_Port = 0;
+	pFIS->Control = 0;
+
+	MV_REG_WRITE_DWORD(portMmio, PORT_CMD_ISSUE, 1<<tag);
+	MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE);	/* flush */
+
+	HBA_SleepMicrosecond(pCore, 10);
+	//hba_msleep(10);
+
+	// make sure CI is cleared before moving on
+	do {
+		temp = MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE) & (1<<tag);
+		count++;
+		HBA_SleepMillisecond(pCore, 10);
+		//hba_msleep(10);
+	} while (temp != 0 && count < 1000);
+
+	Tag_ReleaseOne(&pPort->Tag_Pool, tag);
+	mvEnableIntr(portMmio, old_stat);
+
+	if (temp != 0)
+	{
+//		MV_DPRINT(("DummyFIS:CI can not be clean[0x%x] on port[%d].\n",MV_REG_READ_DWORD( portMmio, PORT_CMD_ISSUE), pPort->Id));
+	}
+}
+
+
+
+
+void mvHandleDevicePlugin (PCore_Driver_Extension pCore, PDomain_Port pPort)
+{
+	PDomain_Device pDevice = &pPort->Device[0];
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U8 i;
+	MV_U32 temp=0;
+	MV_DPRINT(("Find plug in device on port[%d].\n",pPort->Id));
+
+	if( pCore->Total_Device_Count >= MAX_DEVICE_SUPPORTED ){
+		MV_DPRINT(("has many device[%d].\n", pCore->Total_Device_Count ));
+		return;
+	}
+#ifdef COMMAND_ISSUE_WORKROUND
+	pPort->reset_hba_times = 0;
+	mv_core_init_reset_para(pPort);
+#endif
+	#ifdef HOTPLUG_ISSUE_WORKROUND
+	if ( pDevice->Status == DEVICE_STATUS_UNPLUG )
+	{
+		MV_DPRINT(("######## Cancel hot plug INT #########"));
+	    return;
+	}
+	#endif
+	// start command handling on this port
+	mv_core_reset_port(pPort);
+	// reset the tag stack - to guarantee soft reset is issued at slot 0
+	Tag_Init( &pPort->Tag_Pool, pPort->Tag_Pool.Size );
+
+#ifdef COMMAND_ISSUE_WORKROUND
+	pPort->error_state = PORT_ERROR_AT_PLUGIN;
+	if(MV_REG_READ_DWORD( portMmio, PORT_CMD) & PORT_CMD_LIST_ON)
+	{
+		MV_PRINT("Find command running BIT is set on port[%d], reset HBA in timer handler.\n",pPort->Id);
+		pPort->Hot_Plug_Timer = 0;
+		pPort->command_callback = mvHandleDevicePlugin_BH;
+		mv_core_reset_command(pPort);
+		return;
+	}
+#endif
+	mvHandleDevicePlugin_BH(pPort);
+}
+
+void mvHandleDevicePlugin_BH(MV_PVOID  ext)
+{
+	PDomain_Port pPort = (PDomain_Port)ext;
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)pPort->Core_Extension;
+	PDomain_Device pDevice = &pPort->Device[0];
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U8 i;
+	MV_U32 temp;
+	MV_U32 tmp, old_stat;
+
+	// do software reset
+	MV_DPRINT(("Detected device plug-in, doing soft reset\n"));
+
+	if (! (SATA_PortSoftReset( pCore, pPort )) ){
+		goto start_waiting_command;
+	}
+	#ifdef HOTPLUG_ISSUE_WORKROUND
+	if ( pDevice->Status == DEVICE_STATUS_UNPLUG )
+	{
+	   MV_DPRINT(("######## Cancel hot plug  BH #########"));
+	    return;
+	}
+	#endif
+	if( pPort->Type == PORT_TYPE_PM )
+	{
+		/* need to send notifications for all of these devices */
+		for (i=0; i<MAX_DEVICE_PER_PORT; i++)
+		{
+			pDevice = &pPort->Device[i];
+			pDevice->Id = (pPort->Id)*MAX_DEVICE_PER_PORT + i;
+			pDevice->Need_Notify = MV_TRUE;
+			pDevice->State = DEVICE_STATE_IDLE;
+			pDevice->Device_Type = 0;
+			pDevice->Reset_Count = 0;
+		}
+
+		/*SATA_InitPM( pPort );*/
+		SATA_PortReset( pPort, MV_TRUE);
+	}
+	else
+	{
+		/* not a PM - turn off the PM bit in command register */
+		temp = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+		MV_REG_WRITE_DWORD(portMmio, PORT_CMD, temp & (~MV_BIT(17)));
+		temp=MV_REG_READ_DWORD(portMmio, PORT_CMD);	/* flush */
+
+		if( SATA_PortDeviceDetected(pPort) )
+		{
+			if ( SATA_PortDeviceReady(pPort) )
+			{
+				MV_U32 signature;
+
+				signature = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_SIG);
+				if ( signature==0xEB140101 )				/* ATAPI signature */
+					pDevice->Device_Type |= DEVICE_TYPE_ATAPI;
+				else
+				#ifdef HOTPLUG_ISSUE_WORKROUND
+					if(signature==0x00000101)
+				#endif
+					{
+						MV_DASSERT( signature==0x00000101 );	/* ATA signature */
+						pDevice->Device_Type &= ~DEVICE_TYPE_ATAPI;
+					}
+				#ifdef HOTPLUG_ISSUE_WORKROUND
+					else{
+						SATA_PortReportNoDevice(pCore,pPort);
+						return;
+						}
+				#endif
+
+				pDevice->Internal_Req = GetInternalReqFromPool(pCore);
+				if( pDevice->Internal_Req == NULL )
+				{
+					MV_DPRINT(("ERROR: Unable to get an internal request buffer\n"));
+					/* can't initialize without internal buffer - just set this disk down */
+					pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+					pDevice->State = DEVICE_STATE_INIT_DONE;
+					goto start_waiting_command;
+				}
+				else
+				{
+					{
+						pDevice->Status = DEVICE_STATUS_EXISTING|DEVICE_STATUS_FUNCTIONAL;
+						pDevice->State = DEVICE_STATE_RESET_DONE;
+						pDevice->Id = (pPort->Id)*MAX_DEVICE_PER_PORT;
+						if(pPort->error_state == PORT_ERROR_AT_PLUGIN){
+							pDevice->Need_Notify = MV_TRUE;
+							pPort->Device_Number++;
+						}
+						pDevice->Reset_Count = 0;
+					}
+				}
+
+				mvDeviceStateMachine (pCore, pDevice);
+			}
+		}
+	}
+
+start_waiting_command:
+	MV_DPRINT(("Finshed mvHandleDevicePlugin_BH on port[%d].\n",pPort->Id));
+	Core_HandleWaitingList(pCore);
+
+}
+
+#ifdef SUPPORT_PM
+void mvHandlePMUnplug (PCore_Driver_Extension pCore, PDomain_Device pDevice)
+{
+	PMV_Request pReq;
+	PDomain_Port pPort = pDevice->PPort;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	List_Head *pPos;
+	MV_U8 i, count;
+	MV_U32 temp, cmdIssue;
+	MV_BOOLEAN valid;
+	#ifdef RAID_DRIVER
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_RAID);
+	#else
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_HBA);
+	#endif
+
+	pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+	pPort->Device_Number--;
+
+	cmdIssue = MV_REG_READ_DWORD( portMmio, PORT_CMD_ISSUE );
+
+	/* toggle the start bit in cmd register */
+	temp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp & ~MV_BIT(0));
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, temp | MV_BIT(0));
+	HBA_SleepMillisecond( pCore, 100 );
+	//hba_msleep(100);
+
+	/* check for requests that are not finished, clear the port,
+	 * then re-send again */
+	for ( i=0; i<MAX_SLOT_NUMBER; i++ )
+	{
+		pReq = pPort->Running_Req[i];
+
+		if( pReq != NULL )
+		{
+			if( pReq->Device_Id == pDevice->Id )
+			{
+				pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+				CompleteRequestAndSlot(pCore, pPort, pReq, NULL, i);
+			}
+			else if ( cmdIssue & (1<<i) )
+			{
+				if( PrepareAndSendCommand( pCore, pReq ) == MV_QUEUE_COMMAND_RESULT_SENT )
+				{
+#ifdef SUPPORT_ERROR_HANDLING
+#ifdef SUPPORT_TIMER
+					/* start timer for error handling */
+					if( pDevice->Timer_ID == NO_CURRENT_TIMER )
+					{
+						// if no timer is running right now
+						pDevice->Timer_ID = Timer_AddRequest( pCore, REQUEST_TIME_OUT, Core_ResetChannel, pDevice, NULL );
+					}
+#endif /* SUPPORT_TIMER */
+#endif /* SUPPORT_ERROR_HANDLING */
+					pDevice->Outstanding_Req++;
+				}
+				else
+					MV_DASSERT(MV_FALSE);		// shouldn't happens
+			}
+		}
+	}
+
+	count = 0;
+	LIST_FOR_EACH(pPos, &pCore->Waiting_List) {
+		count++;
+	}
+	while ( count>0 )
+	{
+		pReq = (PMV_Request)List_GetFirstEntry(&pCore->Waiting_List, MV_Request, Queue_Pointer);
+
+		if ( pReq->Device_Id == pDevice->Id )
+		{
+			pReq->Scsi_Status = REQ_STATUS_NO_DEVICE;
+			CompleteRequest(pCore, pReq, NULL);
+		}
+		else
+		{
+			List_AddTail(&pReq->Queue_Pointer, &pCore->Waiting_List);
+		}
+		count--;
+	}
+
+	/* clear x bit */
+	valid = MV_FALSE;
+	do
+	{
+		mvPMDevReWrReg( pPort, MV_Read_Reg, MV_SATA_PSCR_SERROR_REG_NUM, 0, pDevice->PM_Number, MV_TRUE );
+		temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+
+		if (((temp >> 16) & 0xF0) == 0xF0)
+			valid = MV_TRUE;
+
+		temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+	} while (valid == MV_FALSE);
+
+	mvPMDevReWrReg( pPort, MV_Write_Reg, MV_SATA_PSCR_SERROR_REG_NUM, temp, pDevice->PM_Number, MV_TRUE);
+
+	if( pDevice->Internal_Req != NULL )
+	{
+		pCore->Total_Device_Count--;
+		ReleaseInternalReqToPool( pCore, pDevice->Internal_Req );
+		pDevice->Internal_Req = NULL;
+	}
+
+	{
+		struct mod_notif_param param;
+		param.lo = pDevice->Id;
+#ifdef RAID_DRIVER
+		RAID_ModuleNotification(pUpperLayer, EVENT_DEVICE_REMOVAL,
+					&param);
+#else
+		HBA_ModuleNotification(pUpperLayer,
+				       EVENT_DEVICE_REMOVAL,
+				       &param);
+#endif /* RAID_DRIVER */
+	}
+}
+
+extern MV_BOOLEAN mvDeviceStateMachine(
+	PCore_Driver_Extension pCore,
+	PDomain_Device pDevice
+	);
+
+void mvHandlePMPlugin (PCore_Driver_Extension pCore, PDomain_Device pDevice)
+{
+	PDomain_Port pPort = pDevice->PPort;
+
+	if( pCore->Total_Device_Count >= MAX_DEVICE_SUPPORTED )
+		return;
+
+	pDevice->Need_Notify = MV_TRUE;
+	pDevice->Device_Type = 0;
+	HBA_SleepMillisecond(pCore, 1000);
+	SATA_InitPMPort( pPort, pDevice->PM_Number );
+	mvDeviceStateMachine(pCore, pDevice);
+}
+#endif	/* #ifdef SUPPORT_PM */
+#endif	/* #ifdef SUPPORT_HOT_PLUG */
+
+void sata_hotplug(MV_PVOID data,MV_U32 intStatus)
+{
+#ifdef SUPPORT_HOT_PLUG
+	PDomain_Port pPort = (PDomain_Port)data;
+	PDomain_Device pDevice = &pPort->Device[0];
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	MV_U8 i, plugout=0, plugin=0;
+	MV_U32 temp;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_BOOLEAN valid;
+	MV_U32 hotPlugDevice = intStatus & PORT_IRQ_PHYRDY;
+	MV_U32 hotPlugPM = (intStatus & PORT_IRQ_ASYNC_NOTIF) || (intStatus & PORT_IRQ_SDB_FIS);
+
+	intStatus &= ~(PORT_IRQ_D2H_REG_FIS|PORT_IRQ_SDB_FIS|PORT_IRQ_PIO_DONE);
+#ifdef _OS_LINUX
+	/*fix Thor - Linux non-raid driver - hotplug
+	Thor-Lite spec define bit25,26 as plug-out/plug-in irq status, Thor didn't define, but also will set these
+	two bits when doing hot-plug. So clear them here if they're set.
+	If power is not stable, hot-plug will result to resetting port, meantime,PORT_IRQ_SIGNATURE_FIS will be set,
+	so also need clear it if it is set.*/
+	intStatus &= ~(PORT_IRQ_SIGNATURE_FIS|(1L<<26)|(1L<<25));
+#endif
+	/* if a hard drive or a PM is plugged in/out of the controller */
+	if( hotPlugDevice )
+	{
+		intStatus &= ~PORT_IRQ_PHYRDY;
+		/* use Phy status to determine if this is a plug in/plug out */
+		//hba_msleep(500);
+		HBA_SleepMillisecond(pCore, 500);
+		if ((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT) & 0xf) == 0)
+			plugout = MV_TRUE;
+		else
+			plugin = MV_TRUE;
+
+		/* following are special cases, so we take care of these first */
+		if( plugout )
+		{
+			if ( (pPort->Type != PORT_TYPE_PM ) && (pDevice->Status & DEVICE_STATUS_EXISTING) &&
+			     !(pDevice->Status & DEVICE_STATUS_FUNCTIONAL) )
+			{
+				/* a bad drive was unplugged */
+				pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+				MV_DPRINT(("bad drive was unplugged\n"));
+			}
+
+			if ( (pPort->Setting & PORT_SETTING_PM_EXISTING) &&
+			     !(pPort->Setting & PORT_SETTING_PM_FUNCTIONAL) )
+			{
+				/* a bad PM was unplugged */
+				pPort->Setting &= ~PORT_SETTING_PM_EXISTING;
+				MV_DPRINT(("bad PM was unplugged\n"));
+			}
+
+			mvHandleDeviceUnplug( pCore, pPort );
+			return;
+		}
+
+		if ( ((pPort->Type == PORT_TYPE_PM) && (pPort->Setting & PORT_SETTING_PM_FUNCTIONAL)) ||
+		     ((pPort->Type != PORT_TYPE_PM) && (pDevice->Status & DEVICE_STATUS_FUNCTIONAL))
+			)
+		{
+			if( plugout ){
+				mvHandleDeviceUnplug( pCore, pPort );
+				return;
+			}
+		}
+		else
+		{
+			if( plugin ){
+				mvHandleDevicePlugin( pCore, pPort );
+				return;
+			}
+		}
+	}
+
+	/* if a drive was plugged in/out of a PM */
+	if ( hotPlugPM )
+	{
+		intStatus &= ~PORT_IRQ_ASYNC_NOTIF;
+		intStatus &= ~PORT_IRQ_SDB_FIS;
+
+		valid = MV_FALSE;
+		do
+		{
+			mvPMDevReWrReg( pPort, MV_Read_Reg, MV_SATA_GSCR_ERROR_REG_NUM, 0, 0xF, MV_TRUE );
+			temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+			if ((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT)& 0xf) != 0x3)
+			{
+				mvHandleDeviceUnplug( pCore, pPort );
+				MV_DPRINT(("PM Lost connection 1\n"));
+				return;
+			}
+
+			if (((temp >> 16) & 0xF0) == 0xF0)
+				valid = MV_TRUE;
+
+			temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+		} while (valid == MV_FALSE);
+
+		if (temp == 0)
+			return;
+
+		// better solution???
+		for (i=0; i<MAX_DEVICE_PER_PORT; i++)
+		{
+			if( temp & MV_BIT(i) )
+			{
+				pDevice = &pPort->Device[i];
+				pDevice->PM_Number = i;
+				break;
+			}
+		}
+
+		/* make sure it's a hot plug SDB */
+		valid = MV_FALSE;
+		do
+		{
+			mvPMDevReWrReg( pPort, MV_Read_Reg, MV_SATA_PSCR_SERROR_REG_NUM, 0, pDevice->PM_Number, MV_TRUE );
+			temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+			if ((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT)& 0xf) != 0x3)
+			{
+				mvHandleDeviceUnplug( pCore, pPort );
+				MV_DPRINT(("PM Lost connection 2\n"));
+				return;
+			}
+
+			if (((temp >> 16) & 0xF0) == 0xF0)
+				valid = MV_TRUE;
+
+			temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+		} while (valid == MV_FALSE);
+
+		if( !( (temp & MV_BIT(16)) || (temp & MV_BIT(26)) ) )
+			return;
+
+		/* check phy status to determine plug in/plug out */
+		HBA_SleepMillisecond(pCore, 500);
+		//hba_msleep(500);
+
+		valid = MV_FALSE;
+		do
+		{
+			mvPMDevReWrReg(pPort, MV_Read_Reg, MV_SATA_PSCR_SSTATUS_REG_NUM, 0, pDevice->PM_Number, MV_TRUE);
+			temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+			if ((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT)& 0xf) != 0x3)
+			{
+				mvHandleDeviceUnplug( pCore, pPort );
+				MV_DPRINT(("PM Lost connection 3\n"));
+				return;
+			}
+
+			if (((temp >> 16) & 0xF0) == 0xF0)
+				valid = MV_TRUE;
+
+			temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+		} while (valid == MV_FALSE);
+
+		if( (temp & 0xF) == 0 )
+		{
+			plugout = MV_TRUE;
+			MV_DPRINT(("PM port %d plugout\n", pDevice->PM_Number));
+		}
+		else
+		{
+			if ((temp & 0xF) == 3)
+			{
+			plugin = MV_TRUE;
+				MV_DPRINT(("PM port %d plugin\n", pDevice->PM_Number));
+			}
+			else
+			{
+				/*
+				On Sil3726, we'll see this condition.
+				Solution:
+				1. Unplug this unstable device.
+				2. Reset Channel, detect current devices to update status
+				*/
+				MV_DPRINT(("PM device connection not established, reset channel\n"));
+
+				mvHandlePMUnplug(pCore, pDevice);
+				Core_ResetChannel(pDevice,NULL);
+				return;
+			}
+		}
+
+		if ( plugout && (pDevice->Status & DEVICE_STATUS_EXISTING) &&
+			 !(pDevice->Status & DEVICE_STATUS_FUNCTIONAL) )
+		{
+			// a bad drive was unplugged
+			pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+
+			/* clear x bit */
+			valid = MV_FALSE;
+			do
+			{
+				mvPMDevReWrReg( pPort, MV_Read_Reg, MV_SATA_PSCR_SERROR_REG_NUM, 0, pDevice->PM_Number, MV_TRUE );
+				temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+				if ((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT)& 0xf) != 0x3)
+				{
+//					mvHandleDeviceUnplug( pCore, pPort );
+					MV_DPRINT(("PM Lost connection 5\n"));
+					return;
+				}
+
+				if (((temp >> 16) & 0xF0) == 0xF0)
+					valid = MV_TRUE;
+
+				temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+			} while (valid == MV_FALSE);
+
+			mvPMDevReWrReg( pPort, MV_Write_Reg, MV_SATA_PSCR_SERROR_REG_NUM, temp, pDevice->PM_Number, MV_TRUE);
+
+			MV_DPRINT(("bad drive was unplugged\n"));
+			mvHandlePMUnplug(pCore, pDevice);
+			return;
+		}
+
+		if( pDevice->Status & DEVICE_STATUS_FUNCTIONAL )
+		{
+			if (plugout)
+				mvHandlePMUnplug(pCore, pDevice);
+		}
+		else
+		{
+			if (plugin)
+				mvHandlePMPlugin( pCore, pDevice );
+		}
+	}
+#endif /* SUPPORT_HOT_PLUG */
+}
+#ifdef HOTPLUG_ISSUE_WORKROUND
+void mvHandleDeviceUnplugReset (MV_PVOID pport, MV_PVOID ptemp)
+{
+
+	PDomain_Port pPort = (PDomain_Port)pport;
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PMV_Request pReq;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U8 i;
+	MV_U32 temp, temp2;
+	PDomain_Device pDevice = &pPort->Device[0];
+
+
+	MV_U32 SControl = MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);
+	SControl &= ~0x0000000F;    //Enable PHY
+	mvEnableIntr(portMmio, pPort->old_stat);
+	MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+	MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);	/* flush */
+	HBA_SleepMillisecond(pCore, 10);
+
+	MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+	MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);	/* flush */
+	HBA_SleepMillisecond(pCore, 10);
+
+	pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+	MV_DPRINT(("######### ReEnable PHY&INT by Unplug Timer #########\n"));
+
+}
+#endif
+void SATA_HandleHotplugInterrupt(
+	IN PDomain_Port pPort,
+	IN MV_U32 intStatus
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	MV_U32 hotPlugDevice = intStatus & PORT_IRQ_PHYRDY;
+	MV_U32 hotPlugPM = (intStatus & PORT_IRQ_ASYNC_NOTIF) | (intStatus & PORT_IRQ_SDB_FIS);
+	struct mod_notif_param event_param;
+
+	#ifdef HOTPLUG_ISSUE_WORKROUND
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	PDomain_Device pDevice = &pPort->Device[0];
+	MV_U32 plugout=0,plugin=0;
+	#endif
+	if (hotPlugDevice | hotPlugPM) {
+		event_param.p_param = pPort;
+		event_param.event_id = intStatus;
+		#ifdef HOTPLUG_ISSUE_WORKROUND
+		HBA_SleepMillisecond(pCore, 500);
+		if ((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT) & 0xf) == 0)
+			plugout= MV_TRUE;
+		else
+			plugin= MV_TRUE;
+
+		/* following are special cases, so we take care of these first */
+		if( plugout )
+		{
+
+			if ( (pPort->Type != PORT_TYPE_PM ) && (pDevice->Status & DEVICE_STATUS_EXISTING) &&
+			     !(pDevice->Status & DEVICE_STATUS_FUNCTIONAL) )
+			{
+				/* a bad drive was unplugged */
+				pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+				MV_DPRINT(("bad drive was unplugged\n"));
+			}
+
+			if ( (pPort->Setting & PORT_SETTING_PM_EXISTING) &&
+			     !(pPort->Setting & PORT_SETTING_PM_FUNCTIONAL) )
+			{
+				/* a bad PM was unplugged */
+				pPort->Setting &= ~PORT_SETTING_PM_EXISTING;
+				MV_DPRINT(("bad PM was unplugged\n"));
+			}
+
+			mvHandleDeviceUnplug( pCore, pPort );
+			return;
+		}
+
+		if ( ((pPort->Type == PORT_TYPE_PM) && (pPort->Setting & PORT_SETTING_PM_FUNCTIONAL)) ||
+		     ((pPort->Type != PORT_TYPE_PM) && (pDevice->Status & DEVICE_STATUS_FUNCTIONAL))
+			)
+		{
+			if(plugout ){
+				mvHandleDeviceUnplug( pCore, pPort );
+				return;
+			}
+		}
+		if(plugin)
+		#endif
+			HBA_ModuleNotification(pCore, EVENT_HOT_PLUG, &event_param);
+	}
+
+	if (intStatus) {
+		MV_DPRINT(("Error: port=%d intStatus=0x%x.\n", pPort->Id, intStatus));
+		/* clear global before channel */
+		MV_REG_WRITE_DWORD(pCore->Mmio_Base, HOST_IRQ_STAT, (1L<<pPort->Id));
+		intStatus = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_IRQ_STAT);
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_IRQ_STAT, intStatus);
+	}
+}
+
+void mvCompleteSlots( PDomain_Port pPort, MV_U32 completeSlot, PATA_TaskFile taskFiles )
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+#ifdef MV_DEBUG
+	MV_LPVOID port_mmio = pPort->Mmio_Base;
+#endif
+	PDomain_Device pDevice;
+	PMV_Request pReq = NULL, pOrgReq = NULL;
+	MV_U8 slotId;
+
+	/* Complete finished commands. All of them are finished successfully.
+	 * There are three situations code will come here.
+	 * 1. No error for both NCQ and Non-NCQ.
+	 * 2. Under NCQ, some requests are completed successfully. At lease one is not.
+	 *	For the error command, by specification, SActive isn't cleared.
+	 * 3. Under non-NCQ, since no interrupt coalescing, no succesful request.
+	 *  Hardware will return one request is completed. But software clears it above. */
+
+	for ( slotId=0; slotId<MAX_SLOT_NUMBER; slotId++ )
+	{
+		if ( !(completeSlot&(1L<<slotId)) )
+			continue;
+
+	//	MV_DASSERT( (MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE)&(1<<slotId))==0 );
+	//	MV_DASSERT( (MV_REG_READ_DWORD(port_mmio, PORT_SCR_ACT)&(1<<slotId))==0 );
+
+		completeSlot &= ~(1L<<slotId);
+
+		/* This slot is finished. */
+		pReq = pPort->Running_Req[slotId];
+		MV_DASSERT( pReq );
+		pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+
+		if ( pReq->Scsi_Status==REQ_STATUS_RETRY )
+		{
+			MV_PRINT("Retried request[0x%p] is finished on port[%d]\n", pReq, pPort->Id);
+			MV_DumpRequest(pReq, MV_FALSE);
+		}
+
+		if ( Core_IsInternalRequest(pCore, pReq)&&(pReq->Org_Req) )
+		{
+			/* This internal request is used to request sense. */
+			MV_ASSERT( pDevice->Device_Type&DEVICE_TYPE_ATAPI );
+			pOrgReq = pReq->Org_Req;
+			/* Copy sense from the scratch buffer to the request sense buffer. */
+			MV_CopyMemory(
+					pOrgReq->Sense_Info_Buffer,
+					pReq->Data_Buffer,
+					MV_MIN(pOrgReq->Sense_Info_Buffer_Length, pReq->Data_Transfer_Length)
+					);
+			pOrgReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+#if defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX)
+			/* remove internal req's timer */
+			hba_remove_timer(pReq);
+#endif
+			pReq = pOrgReq;
+		}
+		else
+		{
+			pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		}
+
+		CompleteRequestAndSlot(pCore, pPort, pReq, taskFiles, slotId);
+
+		if ( completeSlot==0 )
+			break;
+	}
+}
+
+void SATA_PortHandleInterrupt(
+	IN PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort
+	)
+{
+	PDomain_Device pDevice = &pPort->Device[0];
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_LPVOID port_mmio = pPort->Mmio_Base;
+	MV_U32 orgIntStatus, intStatus, serialError, commandIssue, serialActive, temp;
+	PMV_Request pReq = NULL, pOrgReq = NULL;
+	MV_U32 completeSlot = 0;
+	MV_U16 slotId;
+	MV_U8 i,j;
+	MV_BOOLEAN tfError = MV_FALSE,hasError = MV_FALSE, finalError = MV_FALSE,reset_port=MV_FALSE;
+	MV_U32 errorSlot = 0;
+	ATA_TaskFile	taskFiles;
+#ifdef MV_DEBUG
+	MV_U32 orgSerialError, orgCommandIssue, orgSerialActive, orgCompleteSlot, orgRunningSlot;
+#endif
+
+#ifdef SUPPORT_SCSI_PASSTHROUGH
+	readTaskFiles(pPort, pDevice, &taskFiles);
+#endif
+
+
+	/* Read port interrupt status register */
+	orgIntStatus = MV_REG_READ_DWORD(port_mmio, PORT_IRQ_STAT);
+	intStatus = orgIntStatus;
+
+	if ( pPort->Setting&PORT_SETTING_NCQ_RUNNING )
+	{
+		serialActive = MV_REG_READ_DWORD(port_mmio, PORT_SCR_ACT);
+		completeSlot =  (~serialActive) & pPort->Running_Slot;
+	}
+	else
+	{
+		commandIssue = MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE);
+		completeSlot = (~commandIssue) & pPort->Running_Slot;
+	}
+
+#ifdef MV_DEBUG
+	orgCommandIssue = commandIssue;
+	orgSerialActive = serialActive;
+	orgCompleteSlot = completeSlot;
+	orgRunningSlot = pPort->Running_Slot;
+#endif
+
+	intStatus &= ~(PORT_IRQ_D2H_REG_FIS|PORT_IRQ_SDB_FIS|PORT_IRQ_PIO_DONE);	/* Used to check request is done. */
+	intStatus &= ~(PORT_IRQ_DMAS_FIS|PORT_IRQ_PIOS_FIS);						/* Needn't care. */
+
+	/* Error handling */
+	if (
+			(intStatus&PORT_IRQ_TF_ERR)
+		||	(intStatus&PORT_IRQ_LINK_RECEIVE_ERROR)
+		||	(intStatus&PORT_IRQ_LINK_TRANSMIT_ERROR)
+		)
+	{
+		MV_DPRINT(("Interrupt Error: 0x%x orgIntStatus: 0x%x completeSlot=0x%x on port[%d].\n",
+			intStatus, orgIntStatus, completeSlot, pPort->Id));
+		mv_core_dump_reg( pPort);
+		//if (intStatus&PORT_IRQ_TF_ERR)
+		{
+			/* Don't do error handling when receive link error.
+			 * Wait until we got the Task File Error */
+
+			/* read serial error only when there is error */
+			serialError = MV_REG_READ_DWORD(port_mmio, PORT_SCR_ERR);
+			MV_REG_WRITE_DWORD(port_mmio, PORT_SCR_ERR, serialError);
+
+			/* Handle serial error interrupt */
+			if ( serialError )
+			{
+				SATA_HandleSerialError(pPort, serialError);
+			}
+
+#ifdef MV_DEBUG
+			orgSerialError = serialError;
+#endif
+
+			/* read errorSlot only when there is error */
+			errorSlot = MV_REG_READ_DWORD(port_mmio, PORT_CMD);
+
+			hasError = MV_TRUE;
+			errorSlot = (errorSlot>>8)&0x1F;
+
+			if ( pPort->Setting&PORT_SETTING_DURING_RETRY ){
+				pPort->Setting &= ~PORT_SETTING_DURING_RETRY;
+				MV_DPRINT(("Start reset port[%d], reset time=%d.\n",pPort->Id, pPort->reset_hba_times));
+				if(pPort->reset_hba_times < MAX_RESET_TIMES)
+					reset_port = MV_TRUE;
+				else
+					finalError = MV_TRUE;
+
+			}
+			else
+			{
+				/* if the error request is any internal requests, we don't retry
+				 *     1) read log ext - don't retry
+				 *	   2) any initialization requests such as identify - buffer
+				 *		  will conflict when we try to send read log ext to retry
+				 *	   3) request sense - included in the ATAPI condition below
+				 */
+				pReq = pPort->Running_Req[errorSlot];
+				//if ( pReq != NULL && Core_IsInternalRequest(pCore, pReq) )
+				//	finalError = MV_TRUE;
+
+				/* For ATAPI device, we don't do retry. OS already has done a lot.
+				* ATAPI device: one request at a time. */
+				if ( completeSlot==((MV_U32)1L<<errorSlot) )
+				{
+					pReq = pPort->Running_Req[errorSlot];
+					MV_ASSERT( pReq!=NULL );
+					pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+					if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+						finalError = MV_TRUE;
+				} else{
+					MV_U32 slotId=0;
+					for ( slotId=0; slotId<MAX_SLOT_NUMBER; slotId++ )
+					{
+						if ( !(completeSlot&(1L<<slotId)) )
+							continue;
+						pReq = pPort->Running_Req[slotId];
+						MV_ASSERT( pReq!=NULL );
+						pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+						if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI ){
+							if(!(MV_REG_READ_DWORD(mmio,PORT_TFDATA)&0x01)){
+								hasError = MV_FALSE;
+							}
+						}else{
+							printk("has error is true\n");
+							tfError = MV_TRUE;
+						}
+					}
+				}
+			#ifdef SUPPORT_ATA_SECURITY_CMD
+				if ((intStatus&PORT_IRQ_TF_ERR)&&(tfError != MV_TRUE)) {
+
+					if((MV_REG_READ_DWORD(port_mmio, PORT_TFDATA)&0x451) && pReq){
+
+						MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+						MV_REG_WRITE_DWORD(port_mmio, PORT_IRQ_STAT, orgIntStatus);
+
+						intStatus &= ~(PORT_IRQ_TF_ERR|PORT_IRQ_LINK_RECEIVE_ERROR|PORT_IRQ_LINK_TRANSMIT_ERROR);
+
+						pReq->Scsi_Status = REQ_STATUS_ABORT;
+						CompleteRequestAndSlot(pCore, pPort, pReq, &taskFiles, (MV_U8)errorSlot);
+						return;
+						}
+					}
+			#endif
+			}
+		}
+		intStatus &= ~(PORT_IRQ_TF_ERR|PORT_IRQ_LINK_RECEIVE_ERROR|PORT_IRQ_LINK_TRANSMIT_ERROR);
+	}
+
+	/* If NO device ,then only handle hotplug Interrupt. */
+	if((pPort->Type != PORT_TYPE_PM) && (pDevice->Status & DEVICE_STATUS_NO_DEVICE))
+	{
+		SATA_HandleHotplugInterrupt(pPort, intStatus);
+		return;
+	}
+
+	/* Final Error: we give up this error request. Only one request is running.
+	 * And during retry we won't use NCQ command. */
+	if ( finalError )
+	{
+		MV_DPRINT(("Final error, abort port[%d],running slot=0x%x.\n", pPort->Id,pPort->Running_Slot));
+		MV_ASSERT( !(pPort->Setting&PORT_SETTING_NCQ_RUNNING) );
+//		MV_DASSERT( completeSlot==((MV_U32)1L<<errorSlot) );
+		MV_DASSERT( pPort->Running_Slot==completeSlot );
+
+		/* clear global before channel */
+		MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+		MV_REG_WRITE_DWORD(port_mmio, PORT_IRQ_STAT, orgIntStatus);
+
+
+		/* This is the failed request. */
+		pReq = pPort->Running_Req[errorSlot];
+		if((!errorSlot)&&(!pReq)&&(pPort->Setting&PORT_SETTING_DURING_RETRY)){
+			for(errorSlot=0;errorSlot<32;errorSlot++){
+				if((pPort->Running_Slot>>errorSlot)&0x01)
+					break;
+			}
+			pReq = pPort->Running_Req[errorSlot];
+		}
+
+		MV_ASSERT( pReq!=NULL );
+		pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+
+		if( Core_IsInternalRequest(pCore, pReq) )
+		{
+			if( pReq->Org_Req )
+			{
+				/* This internal request is used to request sense. */
+				MV_ASSERT( pDevice->Device_Type&DEVICE_TYPE_ATAPI );
+				pOrgReq = pReq->Org_Req;
+				pOrgReq->Scsi_Status = REQ_STATUS_ERROR;
+#ifdef _OS_LINUX
+				/* remove internal req's timer */
+				hba_remove_timer(pReq);
+#endif
+				pReq = pOrgReq;
+			}
+			else if( pReq->Cdb[2] == CDB_CORE_READ_LOG_EXT )
+			{
+				pReq->Scsi_Status = REQ_STATUS_ERROR;
+			}
+			else
+			{
+				/* This internal request is initialization request like identify */
+				MV_DASSERT( pDevice->State != DEVICE_STATE_INIT_DONE );
+				pReq->Scsi_Status = REQ_STATUS_ERROR;
+			}
+		}
+		else
+		{
+			if ( pReq->Cmd_Flag&CMD_FLAG_PACKET )
+			{
+				pReq->Scsi_Status = REQ_STATUS_REQUEST_SENSE;
+			}
+			else
+			{
+				MV_DPRINT(("Finally SATA error for Req 0x%x.\n", pReq->Cdb[0]));
+				pReq->Scsi_Status = REQ_STATUS_ERROR;
+			}
+		}
+
+		CompleteRequestAndSlot(pCore, pPort, pReq, &taskFiles, (MV_U8)errorSlot);
+
+		/* Handle interrupt status register */
+		if ( intStatus )
+		{
+			SATA_HandleHotplugInterrupt(pPort, intStatus);
+		}
+
+		return;
+	}
+
+
+	/* The Second time to hit the error.  */
+	if ( reset_port){
+		MV_DPRINT(("retry request failed, try reset HBA for port[%d], running slot=0x%x.\n", pPort->Id, pPort->Running_Slot));
+//		mv_core_dump_reg(pPort);
+		/* Toggle the port start bit to clear up the hardware to prepare for the retry. */
+		temp = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_CMD);
+		temp &= ~PORT_CMD_START;
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_CMD, temp );
+		HBA_SleepMillisecond(pCore, 1);
+		temp |= PORT_CMD_START;
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_CMD, temp );
+		HBA_SleepMillisecond(pCore, 100);
+
+		/* Toggle should before we clear the channel interrupt status but not the global interrupt. */
+		MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+
+		MV_DASSERT( MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE)==0 );
+		MV_DASSERT( MV_REG_READ_DWORD(port_mmio, PORT_IRQ_STAT)==0 );
+		//MV_DASSERT( orgIntStatus == 0 );
+		MV_DASSERT( (MV_REG_READ_DWORD(mmio, HOST_IRQ_STAT)&(1L<<pPort->Id))==0 );
+		pPort->Hot_Plug_Timer = 0;
+		pPort->timer_para = pDevice;
+		pPort->command_callback = Core_ResetChannel_BH;
+		//Reset the device.
+		pCore->Total_Device_Count--;
+		pPort->error_state = PORT_ERROR_AT_RUNTIME;
+		mv_core_reset_command(pPort);
+		return;
+	}
+
+	/* The first time to hit the error. Under error condition, figure out all the successful requests. */
+	if ( hasError )
+	{
+		MV_DPRINT(("Has error, reset port[%d], running slot=0x%x.\n", pPort->Id, pPort->Running_Slot));
+		MV_ASSERT( !finalError );
+		if ( pPort->Setting&PORT_SETTING_NCQ_RUNNING )
+		{
+			/* For NCQ command, if error happens on one slot.
+			 * This slot is not completed. SActive is not cleared. */
+		}
+		else
+		{
+			/* For Non-NCQ command, last command is the error command.
+			 * ASIC will stop whenever there is an error.
+			 * And we only have one request if there is no interrupt coalescing or NCQ. */
+			//MV_DASSERT( completeSlot==((MV_U32)1L<<errorSlot) );
+
+			/* The error command is finished but we clear it to make it to be retried. */
+			completeSlot=0;
+		}
+		/* Now all the completed commands are completed successfully. */
+
+		/* Reset this port to prepare for the retry. At least one request will be retried. */
+
+		MV_ASSERT( finalError==MV_FALSE );
+
+		/* Toggle the port start bit to clear up the hardware to prepare for the retry. */
+		temp = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_CMD);
+		temp &= ~PORT_CMD_START;
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_CMD, temp );
+		HBA_SleepMillisecond(pCore, 1);
+		temp |= PORT_CMD_START;
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_CMD, temp );
+		HBA_SleepMillisecond(pCore, 1);
+		/* Toggle should before we clear the channel interrupt status but not the global interrupt. */
+		MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+
+		if(pPort->Type==PORT_TYPE_SATA){
+			//MV_DPRINT(("***pPort->Running_Slot=0x%x pDevice.Outstanding_Req=%d***\n", pPort->Running_Slot, pPort->Device[0].Outstanding_Req));
+		}
+
+		/* Abort all the others requests and retry. */
+		for ( slotId=0; slotId<MAX_SLOT_NUMBER; slotId++ )
+		{
+			pReq = pPort->Running_Req[slotId];
+			if ( !(completeSlot&(1L<<slotId)) && pReq )
+			{
+				pReq->Cmd_Flag &= 0xFF;	/* Remove NCQ setting. */
+				pReq->Scsi_Status = REQ_STATUS_RETRY;
+
+				/* Put requests to the queue head but don't run them. Should run ReadLogExt first. */
+				mv_core_reset_running_slot(pPort, slotId);
+#ifdef _OS_LINUX
+				pReq->eh_flag = 1;
+				hba_remove_timer(pReq);
+#endif
+				List_Add(&pReq->Queue_Pointer, &pCore->Waiting_List);		/* Add to the header. */
+
+				if (pPort->Type==PORT_TYPE_SATA){
+					pPort->Device[0].Outstanding_Req--;
+				}else {//PM case
+					for (j=0; j<MAX_DEVICE_PER_PORT; j++){
+						if (pPort->Device[j].Id == pReq->Device_Id){
+							pPort->Device[j].Outstanding_Req--;
+							break;
+						}
+					}//end of for
+					MV_DASSERT(j==MAX_DEVICE_PER_PORT);
+				}
+
+				//MV_PRINT("Abort error requests....\n");
+				//MV_DumpRequest(pReq, MV_FALSE);
+			}
+		}
+
+#ifdef SUPPORT_TIMER
+		if(pPort->Type==PORT_TYPE_SATA)
+		{
+			MV_DASSERT(pPort->Running_Slot==0);
+			MV_DASSERT(pPort->Device[0].Outstanding_Req==0);
+
+			if( pPort->Device[0].Timer_ID != NO_CURRENT_TIMER )
+			{
+				Timer_CancelRequest( pCore, pPort->Device[0].Timer_ID );
+				pPort->Device[0].Timer_ID = NO_CURRENT_TIMER;
+
+			}
+		}else {//PM case
+			MV_DASSERT(pPort->Type==PORT_TYPE_PM);
+			MV_DASSERT(pPort->Running_Slot==0);
+			for (j=0; j<MAX_DEVICE_PER_PORT; j++){
+				MV_DASSERT(pPort->Device[j].Outstanding_Req==0);
+				if( pPort->Device[j].Timer_ID != NO_CURRENT_TIMER )
+				{
+					Timer_CancelRequest( pCore, pPort->Device[j].Timer_ID );
+					pPort->Device[j].Timer_ID = NO_CURRENT_TIMER;
+
+				}
+			}
+		}
+#endif /* SUPPORT_TIMER */
+
+		MV_DASSERT( MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE)==0 );
+		MV_DASSERT( MV_REG_READ_DWORD(port_mmio, PORT_IRQ_STAT)==0 );
+		//MV_DASSERT( orgIntStatus == 0 );
+		MV_DASSERT( (MV_REG_READ_DWORD(mmio, HOST_IRQ_STAT)&(1L<<pPort->Id))==0 );
+
+		/* Send ReadLogExt command to clear the outstanding commands on the device.
+		 * This request will be put to the queue head because it's Cmd_Initiator is Core Driver.
+		 * Consider the port multiplier. */
+		for ( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+		{
+			pDevice = &pPort->Device[i];
+			if (
+				!(pDevice->Device_Type&DEVICE_TYPE_ATAPI)
+				&& (pDevice->Capacity&DEVICE_CAPACITY_READLOGEXT_SUPPORTED)
+				&& (pPort->Setting&PORT_SETTING_NCQ_RUNNING)
+				)
+			{
+				if(pDevice->Internal_Req){
+					Device_IssueReadLogExt(pPort, pDevice);
+				} else {
+					Core_ResetChannel(pDevice,NULL);
+					return;
+				}
+			}
+			else
+			{
+				Core_HandleWaitingList(pCore);
+			}
+		}
+
+		/* Needn't run interrupt_handle_bottom_half except the hot plug.
+		 * Toggle start bit will clear all the interrupt. So don't clear interrupt again.
+		 * Otherwise it'll clear Read Log Ext interrupt.
+		 * If Device_IssueReadLogExt is called, needn't run Core_HandleWaitingList. */
+
+	}
+
+
+	/* clear global before channel */
+	MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+	MV_REG_WRITE_DWORD(port_mmio, PORT_IRQ_STAT, orgIntStatus);
+
+	/* handle completed slots */
+	if( completeSlot )
+		mvCompleteSlots( pPort, completeSlot, &taskFiles );
+
+	/* Handle interrupt status register */
+	if ( intStatus )
+	{
+		SATA_HandleHotplugInterrupt(pPort, intStatus);
+	}
+}
+
+void PATA_PortHandleInterrupt(
+	IN PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort
+	)
+{
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_LPVOID port_mmio = pPort->Mmio_Base;
+	MV_U32 intStatus, orgIntStatus, commandIssue, taskFile=0, stateMachine, portCommand;
+	MV_U32 temp;
+	PMV_Request pReq = NULL, pOrgReq = NULL;
+	MV_U32 completeSlot = 0;
+	MV_U16 slotId = 0;
+	MV_BOOLEAN hasOneAlready = MV_FALSE;
+	MV_BOOLEAN hasError = MV_FALSE, needReset = MV_FALSE;
+	PDomain_Device pDevice=NULL;
+	ATA_TaskFile	taskFiles;
+
+	/* Read port interrupt status register */
+	intStatus = MV_REG_READ_DWORD(port_mmio, PORT_IRQ_STAT);
+	orgIntStatus = intStatus;
+
+	/*
+	 * Workaround for PATA non-data command.
+	 * PATA non-data command, CI is not ready yet when interrupt is triggered.
+	 */
+	commandIssue = MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE);
+	completeSlot = (~commandIssue) & pPort->Running_Slot;
+
+/* Thor Lite D0 and Thor B0 */
+//if ( (pCore->Device_Id!=DEVICE_ID_THOR_4S1P_NEW) && (pCore->Revision_Id!=0xB0) && (pCore->Revision_Id!=0xB1) && (pCore->Revision_Id!=0xB2) )
+if ( (pCore->Device_Id!=DEVICE_ID_THOR_4S1P_NEW) && (pCore->Revision_Id<0xB0) )
+{
+	temp=1000;
+	while ( (completeSlot==0) && (temp>0) )
+	{
+		HBA_SleepMillisecond(pCore, 2);
+		commandIssue = MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE);
+		completeSlot = (~commandIssue) & pPort->Running_Slot;
+		temp--;
+	}
+
+	if ( (completeSlot==0)&&(pPort->Running_Slot!=0) )
+	{
+		MV_DPRINT(("INT but no request completed: 0x%x CI: 0x%x Running: 0x%x\n",
+			intStatus, commandIssue, pPort->Running_Slot));
+		/*
+		 * Workaround:
+		 * If ATAPI read abort happens, got one interrupt but CI is not cleared.
+		 */
+		stateMachine = MV_REG_READ_DWORD(port_mmio, PORT_INTERNAL_STATE_MACHINE);
+		if ( stateMachine==0x60007013 )
+		{
+			pCore->Need_Reset = 1;
+			needReset = MV_TRUE;
+
+			/* Actually one request is finished. We need figure out which one it is. */
+			portCommand = MV_REG_READ_DWORD(port_mmio, PORT_CMD);
+			MV_DASSERT( portCommand&MV_BIT(15) );	/* Command is still running */
+			portCommand = (portCommand>>8)&0x1F;
+			MV_ASSERT( portCommand<MAX_SLOT_NUMBER );
+			MV_DPRINT(("Read abort happens on slot %d.\n", portCommand));
+			completeSlot |= (1<<portCommand);
+		}
+	}
+}
+
+	/* Handle interrupt status register */
+	intStatus &= ~MV_BIT(0); intStatus &= ~MV_BIT(2);
+#ifdef ENABLE_PATA_ERROR_INTERRUPT
+	hasError = (intStatus!=0) ? MV_TRUE : MV_FALSE;
+
+	/*
+	 * Workaround:
+	 * If error interrupt bit is set. We cannot clear it.
+	 * Try to use PORT_CMD PORT_CMD_PATA_START bit to clear the error interrupt but didn't work.
+	 * So we have to disable PATA error interrupt.
+	 */
+#endif
+
+	/* Complete finished commands */
+	for ( slotId=0; slotId<MAX_SLOT_NUMBER; slotId++ )
+	{
+
+		if ( !(completeSlot&(1L<<slotId)) )
+			continue;
+
+		completeSlot &= ~(1L<<slotId);
+		MV_DASSERT( completeSlot==0 );
+
+		/* This slot is finished. */
+		pReq = pPort->Running_Req[slotId];
+		MV_DASSERT(pReq);
+
+#if defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX)
+		hba_remove_timer(pReq);
+#endif /* defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX) */
+		mv_core_reset_running_slot(pPort, slotId);
+		MV_DASSERT( (MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_CMD_ISSUE)&(1<<slotId))==0 );
+
+		pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+
+	#ifndef ENABLE_PATA_ERROR_INTERRUPT
+		/*
+		 * Workaround:
+		 * Sometimes we got error interrupt bit but the status is still 0x50.
+		 * In this case, the command is completed without error.
+		 * So we have to check the task status to make sure it's really an error or not.
+		 */
+		HBA_SleepMicrosecond(pCore, 2);
+		if ( !pDevice->Is_Slave )
+			taskFile = MV_REG_READ_DWORD(port_mmio, PORT_MASTER_TF0);
+		else
+			taskFile = MV_REG_READ_DWORD(port_mmio, PORT_SLAVE_TF0);
+
+
+		if ( taskFile&MV_BIT(0) )
+		{
+			hasError = MV_TRUE;
+			MV_DPRINT(("PATA request returns with error 0x%x.\n", taskFile));
+		}
+
+		#ifdef MV_DEBUG
+		if ( !(taskFile&MV_BIT(0)) && ( intStatus ) )
+		{
+			MV_DPRINT(("Error interrupt is set but status is 0x50.\n"));
+
+		}
+		#endif
+	#endif
+
+		//if ( (hasError)&&(pCore->Device_Id!=DEVICE_ID_THOR_4S1P_NEW)&&(pCore->Revision_Id!=0xB0)&&(pCore->Revision_Id!=0xB1)&&(pCore->Revision_Id!=0xB2) )
+		if ( (hasError)&&(pCore->Device_Id!=DEVICE_ID_THOR_4S1P_NEW)&&(pCore->Revision_Id<0xB0) )
+		{
+
+			if ( pDevice->Device_Type==DEVICE_TYPE_ATAPI )
+			{
+				/*
+				 * Workaround:
+				 * Write request if device abort, hardware state machine got wrong.
+				 * Need do reset to recover.
+				 * If the error register is 0x40, we think the error happens.
+				 * Suppose this problem only happens on ODD. HDD won't write abort.
+				 */
+
+				taskFile = taskFile>>24;  /* Get the error register */
+				if ( taskFile==0x40 )
+				{
+					pCore->Need_Reset = 1;
+					needReset = MV_TRUE;
+				}
+			}
+		}
+		if ( Core_IsInternalRequest(pCore, pReq)&&(pReq->Org_Req) )
+		{
+			/* This internal request is used to request sense. */
+			pOrgReq = pReq->Org_Req;
+			if ( hasError )
+			{
+				MV_ASSERT( hasOneAlready==MV_FALSE );
+				hasOneAlready = MV_TRUE;
+				pOrgReq->Scsi_Status = REQ_STATUS_ERROR;
+			}
+			else
+			{
+				/* Copy sense from the scratch buffer to the request sense buffer. */
+				MV_CopyMemory(
+						pOrgReq->Sense_Info_Buffer,
+						pReq->Data_Buffer,
+						MV_MIN(pOrgReq->Sense_Info_Buffer_Length, pReq->Data_Transfer_Length)
+						);
+				pOrgReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+			}
+			pReq = pOrgReq;
+		}
+		else
+
+		{
+			if ( hasError )
+			{
+
+				MV_ASSERT( hasOneAlready==MV_FALSE );
+				hasOneAlready = MV_TRUE;
+
+				if ( needReset )
+				{
+					/* Get sense data using legacy mode or fake a sense data here. */
+					PATA_LegacyPollSenseData(pCore, pReq);
+					pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+				}
+				else
+				{
+					if ( pReq->Cmd_Flag&CMD_FLAG_PACKET )
+						pReq->Scsi_Status = REQ_STATUS_REQUEST_SENSE;
+					else
+						pReq->Scsi_Status = REQ_STATUS_ERROR;
+				}
+			}
+			else
+			{
+				pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+
+			}
+		}
+
+#ifdef SUPPORT_SCSI_PASSTHROUGH
+		readTaskFiles(pPort, pDevice, &taskFiles);
+#endif
+
+		pDevice->Outstanding_Req--;
+#ifdef SUPPORT_ERROR_HANDLING
+#ifdef SUPPORT_TIMER
+		/* request for this device came back, so we cancel the timer */
+		Timer_CancelRequest( pCore, pDevice->Timer_ID );
+		pDevice->Timer_ID = NO_CURRENT_TIMER;
+
+		/* if there are more outstanding requests, we send a new timer */
+		if ( pDevice->Outstanding_Req > 0 )
+		{
+			pDevice->Timer_ID = Timer_AddRequest( pCore, REQUEST_TIME_OUT, Core_ResetChannel, pDevice, NULL );
+		}
+#endif /* SUPPORT_TIMER */
+#endif /* SUPPORT_ERROR_HANDLING */
+
+		CompleteRequest(pCore, pReq, &taskFiles);
+
+		if ( completeSlot==0 )
+			break;
+	}
+
+	/*
+	 * Clear the interrupt. It'll re-start the hardware to handle the next slot.
+	 * I clear the interrupt after I've checked the CI register.
+	 * Currently we handle one request everytime in case if there is an error I don't know which one it is.
+	 */
+	MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+	MV_REG_WRITE_DWORD(port_mmio, PORT_IRQ_STAT, orgIntStatus);
+
+
+	/* If there is more requests on the slot, we have to push back there request. */
+	if ( needReset )
+	{
+		for ( slotId=0; slotId<MAX_SLOT_NUMBER; slotId++ )
+		{
+			pReq = pPort->Running_Req[slotId];
+			if ( pReq )
+			{
+				List_Add(&pReq->Queue_Pointer, &pCore->Waiting_List);
+				mv_core_reset_running_slot(pPort, slotId);
+			}
+		}
+		MV_DASSERT(pPort->Running_Slot == 0);
+	}
+}
+
+void Device_MakeRequestSenseRequest(
+	IN PCore_Driver_Extension pCore,
+	IN PDomain_Device pDevice,
+	IN PMV_Request pNewReq,
+	IN PMV_Request pOrgReq
+	)
+{
+	PMV_SG_Table pSGTable = &pNewReq->SG_Table;
+	//MV_U8 senseSize = SATA_SCRATCH_BUFFER_SIZE;
+	MV_U8 senseSize = 18;
+
+	MV_ZeroMvRequest(pNewReq);
+
+	pNewReq->Device_Id = pDevice->Id;
+
+	pNewReq->Scsi_Status = REQ_STATUS_PENDING;
+	pNewReq->Cmd_Initiator = pCore;
+
+	pNewReq->Data_Transfer_Length = senseSize;
+	pNewReq->Data_Buffer = pDevice->Scratch_Buffer;
+
+	pNewReq->Org_Req = pOrgReq;
+
+	pNewReq->Cmd_Flag = CMD_FLAG_DATA_IN;
+#ifdef USE_DMA_FOR_ALL_PACKET_COMMAND
+	pNewReq->Cmd_Flag |=CMD_FLAG_DMA;
+#endif
+
+	pNewReq->Completion = NULL;
+
+	/* Make the SG table. */
+	SGTable_Init(pSGTable, 0);
+	SGTable_Append(
+		pSGTable,
+		pDevice->Scratch_Buffer_DMA.parts.low,
+		pDevice->Scratch_Buffer_DMA.parts.high,
+		senseSize
+		);
+	MV_DASSERT( senseSize%2==0 );
+
+	/* Request Sense request */
+	pNewReq->Cdb[0]=SCSI_CMD_REQUEST_SENSE;
+	pNewReq->Cdb[4]=senseSize;
+
+	/* Fixed sense data format is 18 bytes. */
+	MV_ZeroMemory(pNewReq->Data_Buffer, senseSize);
+}
+
+void CompleteRequest(
+	IN PCore_Driver_Extension pCore,
+	IN PMV_Request pReq,
+	IN PATA_TaskFile pTaskFile
+	)
+{
+#ifdef SUPPORT_SCSI_PASSTHROUGH
+	PHD_Status pHDStatus;
+#endif
+	PDomain_Port pPort = &pCore->Ports[PATA_MapPortId(pReq->Device_Id)];
+	PDomain_Device pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+
+	//Some of the command, we need read the received FIS like smart command.
+
+	if(pReq->Splited_Count)
+	{
+		MV_DASSERT( pReq->Cdb[0]==SCSI_CMD_VERIFY_10 );
+		if(pReq->Scsi_Status == REQ_STATUS_SUCCESS)
+		{
+			MV_U32 sectors;
+			MV_LBA lba;
+
+			pReq->Splited_Count--;
+
+			lba.value = SCSI_CDB10_GET_LBA(pReq->Cdb) + MV_MAX_TRANSFER_SECTOR;
+			sectors = MV_MAX_TRANSFER_SECTOR;
+			SCSI_CDB10_SET_LBA(pReq->Cdb, lba.value);
+			SCSI_CDB10_SET_SECTOR(pReq->Cdb, sectors);
+
+			pReq->Scsi_Status = REQ_STATUS_PENDING;
+
+			Core_ModuleSendRequest(pCore, pReq);
+
+			return;
+		}
+		else
+			pReq->Splited_Count = 0;
+	}
+
+#if defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX)
+	hba_remove_timer(pReq);
+#endif /* defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX) */
+
+	if ( pReq->Scsi_Status==REQ_STATUS_REQUEST_SENSE )
+	{
+		/* Use the internal request to request sense. */
+		Device_MakeRequestSenseRequest(pCore, pDevice, pDevice->Internal_Req, pReq);
+		/* pReq is linked to the */
+		Core_ModuleSendRequest(pCore, pDevice->Internal_Req);
+
+		return;
+	}
+
+
+#ifdef SUPPORT_SCSI_PASSTHROUGH
+	if (pTaskFile != NULL)
+	{
+		if (pReq->Scsi_Status == REQ_STATUS_SUCCESS)
+		{
+			if (pReq->Cdb[0] == SCSI_CMD_MARVELL_SPECIFIC && pReq->Cdb[1] == CDB_CORE_MODULE)
+			{
+				if (pReq->Cdb[2] == CDB_CORE_DISABLE_WRITE_CACHE)
+					pDevice->Setting &= ~DEVICE_SETTING_WRITECACHE_ENABLED;
+				else if (pReq->Cdb[2] == CDB_CORE_ENABLE_WRITE_CACHE)
+					pDevice->Setting |= DEVICE_SETTING_WRITECACHE_ENABLED;
+				else if (pReq->Cdb[2] == CDB_CORE_DISABLE_SMART)
+					pDevice->Setting &= ~DEVICE_SETTING_SMART_ENABLED;
+				else if (pReq->Cdb[2] == CDB_CORE_ENABLE_SMART)
+					pDevice->Setting |= DEVICE_SETTING_SMART_ENABLED;
+				else if (pReq->Cdb[2] == CDB_CORE_SMART_RETURN_STATUS)
+				{
+					pHDStatus = (PHD_Status)pReq->Data_Buffer;
+					if (pHDStatus == NULL)
+					{
+#ifdef SUPPORT_EVENT
+					if (pTaskFile->LBA_Mid == 0xF4 && pTaskFile->LBA_High == 0x2C)
+						core_generate_event( pCore, EVT_ID_HD_SMART_THRESHOLD_OVER, pDevice->Id, SEVERITY_WARNING, 0, NULL );
+#endif
+					}
+					else
+					{
+						if (pTaskFile->LBA_Mid == 0xF4 && pTaskFile->LBA_High == 0x2C)
+							pHDStatus->SmartThresholdExceeded = MV_TRUE;
+						else
+							pHDStatus->SmartThresholdExceeded = MV_FALSE;
+					}
+				}
+			}
+
+
+			if (pReq->Cdb[0] == SCSI_CMD_MARVELL_SPECIFIC && pReq->Cdb[1] == CDB_CORE_MODULE
+			    && pReq->Cdb[2] == CDB_CORE_OS_SMART_CMD)
+			{
+				pReq->Cdb[6] = pTaskFile->LBA_Mid;
+				pReq->Cdb[7] = pTaskFile->LBA_High;
+				pReq->Cdb[5] = pTaskFile->LBA_Low;
+				pReq->Cdb[9] = pTaskFile->Device;
+				pReq->Cdb[8] = pTaskFile->Sector_Count;
+			}
+
+		}
+		else
+		{
+			if (pReq->Sense_Info_Buffer != NULL)
+				((MV_PU8)pReq->Sense_Info_Buffer)[0] = REQ_STATUS_ERROR;
+			pReq->Scsi_Status = REQ_STATUS_ERROR_WITH_SENSE;
+#ifdef SUPPORT_EVENT
+			if ( (pReq->Cdb[0] == SCSI_CMD_MARVELL_SPECIFIC) &&
+				 (pReq->Cdb[1] == CDB_CORE_MODULE) &&
+				 (pReq->Cdb[2] == CDB_CORE_SMART_RETURN_STATUS) )
+				core_generate_event(pCore, EVT_ID_HD_SMART_POLLING_FAIL, pDevice->Id, SEVERITY_WARNING,  0,  NULL );
+#endif
+		}
+	}
+#endif
+
+	/* Do something if necessary to return back the request. */
+	if ( (pReq->Cdb[0]==SCSI_CMD_MARVELL_SPECIFIC) && (pReq->Cdb[1]==CDB_CORE_MODULE) )
+	{
+		if ( pReq->Cdb[2]==CDB_CORE_SHUTDOWN )
+		{
+			if ( pReq->Device_Id<MAX_DEVICE_NUMBER-1 )
+			{
+				pReq->Device_Id++;
+				pReq->Scsi_Status = REQ_STATUS_PENDING;
+				Core_ModuleSendRequest(pCore, pReq);
+				return;
+			}
+			else
+			{
+				pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+			}
+		}
+	}
+
+	if(pReq->Completion)
+		pReq->Completion(pReq->Cmd_Initiator, pReq);
+}
+
+void CompleteRequestAndSlot(
+	IN PCore_Driver_Extension pCore,
+	IN PDomain_Port pPort,
+	IN PMV_Request pReq,
+	IN PATA_TaskFile pTaskFile,
+	IN MV_U8 slotId
+	)
+{
+#ifdef SUPPORT_ERROR_HANDLING
+	PDomain_Device pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+#endif
+	mv_core_reset_running_slot(pPort, slotId);
+	//MV_DASSERT( (MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_CMD_ISSUE)&(1<<slotId))==0 );
+
+	if ( pPort->Type!=PORT_TYPE_PATA )
+	{
+		//MV_DASSERT( (MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_SCR_ACT)&(1<<slotId))==0 );
+	}
+
+	pDevice->Outstanding_Req--;
+#ifdef SUPPORT_ERROR_HANDLING
+#ifdef SUPPORT_TIMER
+	/* request for this device came back, so we cancel the timer */
+	Timer_CancelRequest( pCore, pDevice->Timer_ID );
+	pDevice->Timer_ID = NO_CURRENT_TIMER;
+
+	/* if there are more outstanding requests, we send a new timer */
+	if ( pDevice->Outstanding_Req > 0 )
+	{
+		MV_DASSERT(pPort->Running_Slot!=0);
+		pDevice->Timer_ID = Timer_AddRequest( pCore, REQUEST_TIME_OUT, Core_ResetChannel, pDevice, NULL );
+	}
+#endif /* SUPPORT_TIMER */
+#endif /* SUPPORT_ERROR_HANDLING */
+	CompleteRequest(pCore, pReq, pTaskFile);
+}
+
+void Port_Monitor(PDomain_Port pPort)
+{
+	MV_U8 i;
+	MV_PRINT("Port_Monitor: Running_Slot=0x%x.\n", pPort->Running_Slot);
+
+	for ( i=0; i<MAX_SLOT_NUMBER; i++ )
+	{
+		if ( pPort->Running_Req[i]!=NULL )
+			MV_DumpRequest(pPort->Running_Req[i], MV_FALSE);
+	}
+}
+
+void Core_ModuleMonitor(MV_PVOID This)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	PMV_Request pReq = NULL;
+	PList_Head head = &pCore->Waiting_List;
+	PDomain_Port pPort = NULL;
+	MV_U8 i;
+
+	MV_PRINT("Core_ModuleMonitor Waiting_List:\n");
+	for (pReq = LIST_ENTRY((head)->next, MV_Request, Queue_Pointer);	\
+	     &pReq->Queue_Pointer != (head); 	\
+	     pReq = LIST_ENTRY(pReq->Queue_Pointer.next, MV_Request, Queue_Pointer))
+	{
+		MV_DumpRequest(pReq, MV_FALSE);
+	}
+
+	for ( i=0; i<pCore->Port_Num; i++ )
+	{
+		MV_PRINT("Port[%d]:\n", i);
+		pPort = &pCore->Ports[i];
+		Port_Monitor(pPort);
+	}
+}
+
+void Core_ModuleReset(MV_PVOID This)
+{
+	MV_U32 extensionSize = 0;
+
+	extensionSize = ( ROUNDING(sizeof(Core_Driver_Extension),8)
+#ifdef SUPPORT_CONSOLIDATE
+					+ ROUNDING(sizeof(Consolidate_Extension),8) + ROUNDING(sizeof(Consolidate_Device),8)*MAX_DEVICE_NUMBER
+#endif
+					);
+
+	/* Re-initialize all the variables even discard all the requests. */
+	Core_ModuleInitialize(This, extensionSize, 32);
+
+}
+#ifdef __MM_SE__
+
+struct mv_module_ops __core_mod_ops = {
+	.module_id              = MODULE_CORE,
+	.get_res_desc           = Core_ModuleGetResourceQuota,
+	.module_initialize      = Core_ModuleInitialize,
+	.module_start           = Core_ModuleStart,
+	.module_stop            =  Core_ModuleShutdown,
+	.module_notification    = Core_ModuleNotification,
+	.module_sendrequest     = Core_ModuleSendRequest,
+	.module_reset           =  Core_ModuleReset,
+	.module_monitor         = Core_ModuleMonitor,
+	.module_service_isr     = Core_InterruptServiceRoutine,
+#ifdef RAID_DRIVER
+	.module_send_xor_request= Core_ModuleSendXORRequest,
+#endif /* RAID_DRIVER */
+};
+
+struct mv_module_ops *mv_core_register_module(void)
+{
+	return &__core_mod_ops;
+}
+
+#endif /* _OS_LINUX */
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_exp.h
@@ -0,0 +1,67 @@
+#if !defined(CORE_EXPOSE_H)
+#define CORE_EXPOSE_H
+
+#ifdef __MM_SE__
+#define __ext_to_core(_ext)       ((PCore_Driver_Extension) (_ext))
+
+#define core_start_cmpl_notify(_ext)                                      \
+           {                                                              \
+		   __ext_to_core(_ext)->desc->status = MV_MOD_STARTED;    \
+		   HBA_ModuleStarted(__ext_to_core(_ext)->desc);          \
+	   }
+
+
+#define core_generate_event(ext, eid, did, slv, pc, ptr)                  \
+   {                                                                      \
+	struct mod_notif_param _param = {ptr, 0, 0, eid, did, slv, pc};   \
+	__ext_to_core(ext)->desc->parent->ops->module_notification(       \
+                                            ext->desc->parent->extension, \
+					    EVENT_LOG_GENERATED,          \
+					    &_param);                     \
+   }
+
+
+#else /* __MM_SE__ */
+#define core_generate_event(ext, eid, did, slv, pc, ptr)                \
+   {                                                                    \
+       struct mod_notif_param param = {ptr, 0, 0, eid, did, slv, pc};   \
+       HBA_ModuleNotification(ext, EVENT_LOG_GENERATED, &param);        \
+   }
+
+#define core_start_cmpl_notify(_ext)  	HBA_ModuleStarted(_ext)
+#endif /* __MM_SE__ */
+
+
+MV_U32 Core_ModuleGetResourceQuota(enum Resource_Type type, MV_U16 maxIo);
+void Core_ModuleInitialize(MV_PVOID, MV_U32, MV_U16);
+void Core_ModuleStart(MV_PVOID);
+void Core_ModuleShutdown(MV_PVOID);
+void Core_ModuleNotification(MV_PVOID, enum Module_Event, struct mod_notif_param *);
+void Core_ModuleSendRequest(MV_PVOID, PMV_Request);
+void Core_ModuleMonitor(MV_PVOID);
+void Core_ModuleReset(MV_PVOID pExtension);
+
+MV_BOOLEAN Core_InterruptServiceRoutine(MV_PVOID This);
+
+#ifdef  SUPPORT_TASKLET
+MV_BOOLEAN Core_HandleServiceRoutine(MV_PVOID This);
+#endif
+
+#ifdef SUPPORT_ERROR_HANDLING
+#ifdef _OS_LINUX
+#define REQUEST_TIME_OUT			10		// in multiples of TIMER_INTERVAL, see hba_timer.h
+#else
+#define REQUEST_TIME_OUT			30		// in multiples of TIMER_INTERVAL, see hba_timer.h
+#endif
+#endif
+
+void mvRemoveDeviceWaitingList( MV_PVOID This, MV_U16 deviceId, MV_BOOLEAN returnOSRequest );
+void mvRemovePortWaitingList( MV_PVOID This, MV_U8 portId );
+MV_U8 mv_core_check_is_reseeting(MV_PVOID core_ext);
+void Core_FillSenseData(PMV_Request pReq, MV_U8 senseKey, MV_U8 adSenseCode);
+void RAID_ModuleNotification(MV_PVOID This,
+			     enum Module_Event event,
+			     struct mod_notif_param *param);
+
+void sata_hotplug(MV_PVOID data,MV_U32 intStatus);
+#endif /* CORE_EXPOSE_H */
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_init.c
@@ -0,0 +1,2818 @@
+#include "mv_include.h"
+
+#include "core_exp.h"
+
+#include "core_init.h"
+#include "core_inter.h"
+
+#include "core_thor.h"
+
+#include "core_sata.h"
+
+static void Device_IssueIdentify(PDomain_Port pPort, PDomain_Device pDevice);
+static void Device_IssueSetUDMAMode(PDomain_Port pPort, PDomain_Device pDevice);
+static void Device_IssueSetPIOMode(PDomain_Port pPort, PDomain_Device pDevice);
+static void Device_EnableWriteCache(PDomain_Port pPort, PDomain_Device pDevice);
+static void Device_EnableReadAhead(PDomain_Port pPort, PDomain_Device pDevice);
+
+extern void Core_HandleWaitingList(PCore_Driver_Extension pCore);
+
+static MV_BOOLEAN mvChannelStateMachine(
+	PCore_Driver_Extension pCore,
+	PDomain_Port pPort
+	);
+
+MV_BOOLEAN mvDeviceStateMachine(
+	PCore_Driver_Extension pCore,
+	PDomain_Device pDevice
+	);
+
+#ifdef COMMAND_ISSUE_WORKROUND
+MV_U8 mv_core_reset_port(PDomain_Port pPort);
+void mv_core_dump_reg(PDomain_Port pPort);
+void mv_core_reset_command_in_timer(PDomain_Port pPort);
+void mvHandleDevicePlugin_BH(MV_PVOID  ext);
+void  mv_core_reset_command(PDomain_Port pPort);
+void mv_core_init_reset_para(PDomain_Port pPort);
+#endif
+
+
+#ifdef SUPPORT_HOT_PLUG
+void Device_SoftReset(PDomain_Port pPort, PDomain_Device pDevice)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+
+	pDevice->State = DEVICE_STATE_RESET_DONE;
+	mvDeviceStateMachine (pCore, pDevice);
+}
+#endif	/* #ifdef SUPPORT_HOT_PLUG */
+
+PMV_Request GetInternalReqFromPool( PCore_Driver_Extension pCore )
+{
+	if( !List_Empty(&pCore->Internal_Req_List) )
+		return ((PMV_Request)List_GetFirstEntry(&pCore->Internal_Req_List, MV_Request, Queue_Pointer));
+	else
+		return NULL;
+}
+
+void ReleaseInternalReqToPool( PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	MV_ZeroMvRequest(pReq);
+	List_AddTail( &pReq->Queue_Pointer, &pCore->Internal_Req_List );
+}
+
+/*
+ * Initialize this port including possible device hard or soft reset.
+ */
+
+#ifdef SUPPORT_PM
+void mvPMDevReWrReg(
+	PDomain_Port pPort,
+	MV_U8 read,
+	MV_U8 PMreg,
+	MV_U32 regVal,
+	MV_U8 PMport,
+	MV_BOOLEAN control)
+{
+	/* Always use the last slot for PM */
+	/* This will only be a problem if two PM requests are sent at the same time */
+	/* using the last slot, which should not happen in current driver, */
+	/* since our PM operations are done by polling. */
+	MV_U16 tag = MAX_SLOT_NUMBER - 1;		/* always use the last slot */
+	PMV_Command_Header header = SATA_GetCommandHeader(pPort, tag);
+	PMV_Command_Table pCmdTable = Port_GetCommandTable(pPort, tag);
+	PSATA_FIS_REG_H2D pFIS = (PSATA_FIS_REG_H2D)pCmdTable->FIS;
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 old_stat, loop=5000;
+
+	mvDisableIntr(portMmio, old_stat);
+
+	MV_ZeroMemory(header, sizeof(MV_Command_Header));
+	MV_ZeroMemory(pCmdTable, sizeof(MV_Command_Table));
+
+	header->FIS_Length = FIS_REG_H2D_SIZE_IN_DWORD;
+	header->PM_Port = control? 0xF : PMport;
+
+	*((MV_U16 *) header) = CPU_TO_LE_16( *((MV_U16 *) header) );
+	header->Table_Address = CPU_TO_LE_32(pPort->Cmd_Table_DMA.parts.low + SATA_CMD_TABLE_SIZE*tag);
+	header->Table_Address_High = CPU_TO_LE_32(pPort->Cmd_Table_DMA.parts.high);
+
+	pFIS->FIS_Type = SATA_FIS_TYPE_REG_H2D;
+	pFIS->PM_Port = control? 0xF : PMport;
+	pFIS->Command =  (read)?MV_ATA_COMMAND_PM_READ_REG : MV_ATA_COMMAND_PM_WRITE_REG;
+	pFIS->Features = PMreg;
+	pFIS->Device = PMport;
+	pFIS->C = 1;
+
+	if (!read)
+	{
+		pFIS->LBA_Low =  (MV_U8)((regVal & 0xff00) >> 8);
+		pFIS->LBA_Mid = (MV_U8)((regVal & 0xff0000) >> 16);
+		pFIS->LBA_High = (MV_U8)((regVal & 0xff000000) >> 24) ;
+		pFIS->Sector_Count = (MV_U8)(regVal & 0xff);
+	}
+
+	MV_DASSERT( (MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE)&(1<<tag))==0 );
+
+	MV_REG_WRITE_DWORD(portMmio, PORT_CMD_ISSUE, 1<<tag);
+	MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE);	/* flush */
+
+	//temp = MV_REG_READ_DWORD(portMmio, PORT_SCR_ERR);
+	//MV_REG_WRITE_DWORD(portMmio, PORT_SCR_ERR, temp);
+	//temp = MV_REG_READ_DWORD(portMmio, PORT_IRQ_STAT);
+	//MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_STAT, temp);
+	//MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+
+	// make sure CI is cleared before moving on
+	loop = 5000;
+	while(loop > 0) {
+		if( (MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE)&(1<<tag)) == 0 )
+			break;
+		if( (MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT)& 0xf) != 0x3)
+			break;
+		HBA_SleepMillisecond(pCore, 10);
+		loop--;
+	}
+
+	if ( (MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE)&(1<<tag)) != 0 )
+		MV_DPRINT(("read/write PM register: CI not cleared!\n"));
+
+	mvEnableIntr(portMmio, old_stat);
+
+}
+MV_U8 mvGetSataDeviceType(PDomain_Port pPort)
+{
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 tmp;
+
+	tmp = MV_REG_READ_DWORD(portMmio, PORT_SIG);
+
+	if(tmp == 0x96690101)
+	{
+		MV_DPRINT(("Port Multiplier detected.\n"));
+		return PORT_TYPE_PM;
+	}
+
+	return 0;
+}
+#endif /* SUPPORT_PM */
+
+
+MV_BOOLEAN SATA_DoSoftReset(PDomain_Port pPort, MV_U8 PMPort)
+{
+	MV_U16 tag = Tag_GetOne(&pPort->Tag_Pool);
+	PMV_Command_Header header = SATA_GetCommandHeader(pPort, tag);
+	PMV_Command_Table pCmdTable = Port_GetCommandTable(pPort, tag);
+	PSATA_FIS_REG_H2D pFIS = (PSATA_FIS_REG_H2D)pCmdTable->FIS;
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 old_stat;
+	MV_U32 temp = 0, count = 0;
+	MV_U8 reset = 1;
+
+	if( PMPort == 0xF )
+		MV_DASSERT( tag == 0 );
+
+	mvDisableIntr(portMmio, old_stat);
+
+	do
+	{
+		MV_ZeroMemory(header, sizeof(MV_Command_Header));
+		MV_ZeroMemory(pCmdTable, sizeof(MV_Command_Table));
+
+		header->FIS_Length = FIS_REG_H2D_SIZE_IN_DWORD;
+		header->Reset = (reset)?1:0;
+		header->PM_Port = PMPort;
+
+		*((MV_U16 *) header) = CPU_TO_LE_16( *((MV_U16 *) header) );
+		header->Table_Address = CPU_TO_LE_32(pPort->Cmd_Table_DMA.parts.low + SATA_CMD_TABLE_SIZE*tag);
+		header->Table_Address_High = CPU_TO_LE_32(pPort->Cmd_Table_DMA.parts.high);
+
+		pFIS->FIS_Type = SATA_FIS_TYPE_REG_H2D;
+		pFIS->PM_Port = PMPort;
+//		pFIS->Device = 0x40;
+		pFIS->Control = (reset)?MV_BIT(2):0;
+
+		MV_REG_WRITE_DWORD(portMmio, PORT_CMD_ISSUE, 1<<tag);
+		MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE);	/* flush */
+
+		HBA_SleepMillisecond(pCore, 2);
+
+
+		reset = reset ^ 1; /*SRST CLEAR*/
+
+		count = 0;
+		// make sure CI is cleared before moving on
+		do {
+			temp = MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE) & (1<<tag);
+			count++;
+			HBA_SleepMillisecond(pCore, 10);
+		} while (temp != 0 && count < 3000);
+
+	}while(reset==0);
+
+	Tag_ReleaseOne(&pPort->Tag_Pool, tag);
+	mvEnableIntr(portMmio, old_stat);
+#if 0//def MV_DEBUG
+	if(pPort->command_callback && pPort->reset_hba_times < MAX_RESET_TIMES){
+		pPort->Hot_Plug_Timer = 0;
+		pPort->command_callback = mvHandleDevicePlugin_BH;
+		SmallTimer_AddRequest(pPort, 10, mv_core_reset_command_in_timer, pPort, NULL);
+		pCore->resetting_command = MV_TRUE;
+		return MV_FALSE;
+	}
+
+#endif
+
+	if (temp != 0)
+	{
+		MV_PRINT("Softrest:CI can not be clean[0x%x] on port[%d].\n",MV_REG_READ_DWORD( portMmio, PORT_CMD ), pPort->Id);
+		mv_core_dump_reg(pPort);
+		if(pPort->reset_hba_times > MAX_RESET_TIMES){
+			MV_DPRINT(("Port resetted more than %d times, shut down the port.\n",MAX_RESET_TIMES));
+			return MV_FALSE;
+		}
+		pPort->Hot_Plug_Timer = 0;
+		pPort->command_callback = mvHandleDevicePlugin_BH;
+		pCore->resetting_command = MV_TRUE;
+		SmallTimer_AddRequest(pPort, 10, mv_core_reset_command_in_timer, pPort, NULL);
+		return MV_FALSE;
+	}
+
+	return MV_TRUE;
+}
+
+#ifdef SUPPORT_PM
+MV_BOOLEAN SATA_SoftResetDevice(PDomain_Port pPort, MV_U8 portNum)
+{
+#ifndef _OS_LINUX
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+#endif
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 status1, status2, loop = 20;
+
+	if (! (SATA_DoSoftReset(pPort, portNum)) )
+		return MV_FALSE;
+
+	while(loop>0)
+	{
+		status1 = MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT) & 0xf;
+		status2 = MV_REG_READ_DWORD(portMmio, PORT_TFDATA) & 0xff;
+		if (((status1 & 0xf) == 0x3) && ((status2 & 0x80) == 0))
+		{
+			MV_DPRINT(("loop = %x\n", loop));
+			pPort->Type = mvGetSataDeviceType( pPort );
+			return MV_TRUE;
+		}
+		HBA_SleepMicrosecond(pCore, 1000);
+		loop--;
+	}
+	MV_DPRINT(("Did not detect device after soft reset\n"));
+	return MV_FALSE;
+}
+
+MV_BOOLEAN SATA_SoftResetPMDevice(PDomain_Port pPort, MV_U8 portNum)
+{
+#ifndef _OS_LINUX
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+#endif
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 status, loop = 5000;
+//	MV_U32 PMPort;
+
+
+	if (! (SATA_DoSoftReset(pPort, portNum)) )
+		return MV_FALSE;
+
+
+	while(loop>0)
+	{
+		status = MV_REG_READ_DWORD(portMmio, PORT_TFDATA) & 0xff;
+		/*PMPort = (MV_REG_READ_DWORD(portMmio, PORT_TFDATA) & 0xf00000) >> 20;*/
+
+		if ( ((status & 0x80) == 0)/* && (PMPort == portNum)*/ )
+		{
+
+			MV_DPRINT(("loop = %x\n", loop));
+			return MV_TRUE;
+		}
+		HBA_SleepMicrosecond(pCore, 1000);
+
+		loop--;
+	}
+
+	MV_DPRINT(("Did not detect device after soft reset\n"));
+	return MV_FALSE;
+}
+
+MV_BOOLEAN PMPortDeviceDetected(PDomain_Port pPort, MV_U8 portNum)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 read_result, temp;
+	MV_BOOLEAN valid;
+	MV_U32 loop = 100;
+
+	while(loop>0)
+	{
+		/*Detect the sata device*/
+		valid = MV_FALSE;
+		do
+		{
+			mvPMDevReWrReg(pPort, MV_Read_Reg, MV_SATA_PSCR_SSTATUS_REG_NUM, 0, portNum, MV_TRUE);
+			temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+
+			if (((temp >> 16) & 0xF0) == 0xF0)
+				valid = MV_TRUE;
+
+			read_result = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+		} while (valid == MV_FALSE);
+
+		if( (read_result & 0xFFF) == 0x123 || (read_result & 0xFFF) == 0x113 )
+		{
+			MV_DPRINT(("the device detected on PM port %d, Port %d ", portNum, pPort->Id));
+
+			/* clears X bit in SError */
+			mvPMDevReWrReg( pPort, MV_Write_Reg, MV_SATA_PSCR_SERROR_REG_NUM, 0xFFFFFFFF, portNum, MV_TRUE);
+			return MV_TRUE;
+		}
+		HBA_SleepMillisecond(pCore, 1);
+		loop--;
+	}
+	return MV_FALSE;
+}
+#endif /* SUPPORT_PM */
+
+void SATA_InitPMPort (PDomain_Port pPort, MV_U8 portNum)
+{
+	PDomain_Device pDevice = &pPort->Device[portNum];
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 signature, tmp;
+#ifdef SUPPORT_ERROR_HANDLING
+	#ifdef RAID_DRIVER
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_RAID);
+	#else
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_HBA);
+	#endif
+	struct mod_notif_param param;
+#endif
+	if( PMPortDeviceDetected(pPort, portNum) )
+	{
+		/*
+		This delay is for SiliconImage PM
+		especiall ATAPI device
+		*/
+		HBA_SleepMillisecond(pCore, 50);
+		if ( SATA_SoftResetPMDevice(pPort, portNum) )
+		{
+			signature = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_PM_FIS_0);
+
+			if ( signature==0xEB140101 )				/* ATAPI signature */
+				pDevice->Device_Type |= DEVICE_TYPE_ATAPI;
+			else
+				MV_DASSERT( signature==0x00000101 );	/* ATA signature */
+
+			pDevice->Internal_Req = GetInternalReqFromPool(pCore);
+			if( pDevice->Internal_Req == NULL )
+			{
+				MV_DPRINT(("ERROR: Unable to get an internal request buffer\n"));
+				// can't initialize without internal buffer - just set this disk down
+				pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+				pDevice->State = DEVICE_STATE_INIT_DONE;
+			}
+			else
+			{
+				pDevice->Status = DEVICE_STATUS_EXISTING|DEVICE_STATUS_FUNCTIONAL;
+				pDevice->State = DEVICE_STATE_RESET_DONE;
+				pDevice->PM_Number = portNum;
+				pPort->Device_Number++;
+			}
+		}
+		else
+		{
+			MV_DPRINT(("soft reset failed on PM port %d\n", portNum));
+
+#ifdef SUPPORT_ERROR_HANDLING
+			if( pDevice->Status & DEVICE_STATUS_FUNCTIONAL )
+			{
+				MV_ASSERT( pDevice->Internal_Req!=NULL );
+				pCore->Total_Device_Count--;
+				ReleaseInternalReqToPool( pCore, pDevice->Internal_Req );
+				pDevice->Internal_Req = NULL;
+				param.lo = pDevice->Id;
+				#ifdef RAID_DRIVER
+				RAID_ModuleNotification(pUpperLayer, EVENT_DEVICE_REMOVAL, &param);
+				#else
+				HBA_ModuleNotification(pUpperLayer,
+						       EVENT_DEVICE_REMOVAL,
+						       &param);
+				#endif
+			}
+#endif
+			pDevice->Status = DEVICE_STATUS_EXISTING;
+			pDevice->State = DEVICE_STATE_INIT_DONE;
+			pDevice->Need_Notify = MV_FALSE;
+
+			/* toggle the start bit in cmd register */
+			tmp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+			MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp & ~MV_BIT(0));
+			MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp | MV_BIT(0));
+			HBA_SleepMillisecond( pCore, 100 );
+		}
+	}
+	else
+		pDevice->Need_Notify = MV_FALSE;
+}
+
+void SATA_InitPM (PDomain_Port pPort)
+{
+	PDomain_Device pDevice;
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 numPMPorts, temp;
+	MV_U8 i;
+	MV_BOOLEAN valid;
+
+	pPort->Setting |= PORT_SETTING_PM_EXISTING;
+	pPort->Setting |= PORT_SETTING_PM_FUNCTIONAL;
+
+	/* fill in various information about the PM */
+
+	/* check how many ports the PM has */
+	valid = MV_FALSE;
+	do
+	{
+		mvPMDevReWrReg(pPort, MV_Read_Reg, MV_SATA_GSCR_INFO_REG_NUM, 0, 0xF, MV_TRUE);
+		temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+
+		if (((temp >> 16) & 0xF0) == 0xF0)
+			valid = MV_TRUE;
+
+		numPMPorts = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 ) & 0xF;
+	} while (valid == MV_FALSE);
+
+	if ( numPMPorts > MAX_DEVICE_PER_PORT )
+		numPMPorts = MAX_DEVICE_PER_PORT;
+	else if ( numPMPorts < MAX_DEVICE_PER_PORT )
+	{
+		for( i=(MV_U8)numPMPorts; i<MAX_DEVICE_PER_PORT; i++ )
+		{
+			pPort->Device[i].Status = DEVICE_STATUS_NO_DEVICE;
+			pPort->Device[i].State = DEVICE_STATE_INIT_DONE;
+		}
+	}
+	pPort->PM_Num_Ports = (MV_U8)numPMPorts;
+
+	/* vendor ID & device ID */
+	valid = MV_FALSE;
+	do
+	{
+		mvPMDevReWrReg( pPort, MV_Read_Reg, MV_SATA_GSCR_ID_REG_NUM, 0, 0xF, MV_TRUE );
+		temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+
+		if (((temp >> 16) & 0xF0) == 0xF0)
+			valid = MV_TRUE;
+
+		temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+	} while (valid == MV_FALSE);
+
+	pPort->PM_Vendor_Id = (MV_U16)temp;
+	pPort->PM_Device_Id = (MV_U16)(temp >> 16);
+
+	/* product & spec revisions */
+	valid = MV_FALSE;
+	do
+	{
+		mvPMDevReWrReg( pPort, MV_Read_Reg, MV_SATA_GSCR_REVISION_REG_NUM, 0, 0xF, MV_TRUE );
+		temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+
+		if (((temp >> 16) & 0xF0) == 0xF0)
+			valid = MV_TRUE;
+
+		temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+	} while (valid == MV_FALSE);
+
+	pPort->PM_Product_Revision = (MV_U8)((temp & 0xFF00) >> 8);
+	if ( temp & MV_BIT(3) )
+		pPort->PM_Spec_Revision = 12;
+	else if ( temp & MV_BIT(2) )
+		pPort->PM_Spec_Revision = 11;
+	else if ( temp & MV_BIT(1) )
+		pPort->PM_Spec_Revision = 10;
+	else
+		pPort->PM_Spec_Revision = 0;
+
+	/* enable asychronous notification bit for hot plug */
+	valid = MV_FALSE;
+	do
+	{
+		mvPMDevReWrReg( pPort, MV_Read_Reg, MV_SATA_GSCR_FEATURES_ENABLE_REG_NUM, 0, 0xF, MV_TRUE );
+		temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+
+		if (((temp >> 16) & 0xF0) == 0xF0)
+			valid = MV_TRUE;
+
+		temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+	} while (valid == MV_FALSE);
+
+	mvPMDevReWrReg( pPort, MV_Write_Reg, MV_SATA_GSCR_FEATURES_ENABLE_REG_NUM,
+					temp | MV_BIT(3), 0xF, MV_TRUE );
+
+	/* enable N & X bit in SError for hot plug */
+	valid = MV_FALSE;
+	do
+	{
+		mvPMDevReWrReg( pPort, MV_Read_Reg, MV_SATA_GSCR_ERROR_ENABLE_REG_NUM, 0, 0xF, MV_TRUE );
+		temp = MV_REG_READ_DWORD( portMmio, PORT_TFDATA);
+
+		if (((temp >> 16) & 0xF0) == 0xF0)
+			valid = MV_TRUE;
+
+		temp = MV_REG_READ_DWORD( portMmio, PORT_PM_FIS_0 );
+	} while (valid == MV_FALSE);
+
+	mvPMDevReWrReg( pPort, MV_Write_Reg, MV_SATA_GSCR_ERROR_ENABLE_REG_NUM,
+					MV_BIT(16) | MV_BIT(26), 0xF, MV_TRUE );
+
+	for( i=0; i<numPMPorts; i++ )
+	{
+		pDevice = &pPort->Device[i];
+		pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+		pDevice->State = DEVICE_STATE_INIT_DONE;
+
+
+		/*
+		When enter hibernation, do not enable device port again
+		for saving hibernation time
+		*/
+		if(!pCore->Is_Dump){
+			/*enable the device port*/
+			mvPMDevReWrReg(pPort, MV_Write_Reg, MV_SATA_PSCR_SCONTROL_REG_NUM, 0x01, i, MV_TRUE);
+			HBA_SleepMillisecond(pCore, 1);
+			mvPMDevReWrReg(pPort, MV_Write_Reg, MV_SATA_PSCR_SCONTROL_REG_NUM, 0x00, i, MV_TRUE);
+			HBA_SleepMillisecond(pCore, 1);
+		}
+
+		SATA_InitPMPort( pPort, i );
+	}
+
+	/* Wait for each port to finish setting flags before starting state machine*/
+	for( i=0; i<numPMPorts; i++ )
+	{
+		pDevice = &pPort->Device[i];
+		if( pDevice->Status & DEVICE_STATUS_FUNCTIONAL )
+			mvDeviceStateMachine( pCore, pDevice );
+	}
+
+	if( pPort->Device_Number == 0 )
+		mvDeviceStateMachine( pCore, &pPort->Device[0] );
+}
+
+MV_BOOLEAN SATA_PortSoftReset( PCore_Driver_Extension pCore, PDomain_Port pPort )
+{
+	PDomain_Device pDevice = &pPort->Device[0];
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 tmp;
+	MV_U8 i;
+
+	if (! (SATA_SoftResetDevice(pPort, 0xF)) )
+	{
+		/* toggle the start bit in cmd register */
+		tmp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+		MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp & ~MV_BIT(0));
+		MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp | MV_BIT(0));
+		HBA_SleepMillisecond( pCore, 100 );
+
+		if( (pPort->Type != PORT_TYPE_PM) && (pDevice->Status & DEVICE_STATUS_FUNCTIONAL)
+#ifdef COMMAND_ISSUE_WORKROUND
+			&& !mv_core_check_is_reseeting(pCore)
+#endif
+			){
+			SATA_PortReportNoDevice( pCore, pPort );
+		}
+		/* had trouble detecting device on this port, so we report existing
+		   but not functional */
+		pDevice->Status = DEVICE_STATUS_EXISTING;
+		pDevice->State = DEVICE_STATE_INIT_DONE;
+
+		/* set the rest of the device on this port */
+		for (i=1; i<MAX_DEVICE_PER_PORT; i++)
+		{
+			pDevice = &pPort->Device[i];
+			pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+			pDevice->State = DEVICE_STATE_INIT_DONE;
+		}
+
+		mvDeviceStateMachine(pCore, pDevice);
+		return MV_FALSE;
+	}
+
+	/* toggle the start bit in cmd register to make sure hardware
+	   is clean after soft reset */
+	tmp = MV_REG_READ_DWORD( portMmio, PORT_CMD );
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp & ~MV_BIT(0));
+	MV_REG_WRITE_DWORD( portMmio, PORT_CMD, tmp | MV_BIT(0));
+	HBA_SleepMillisecond( pCore, 100 );
+
+	return MV_TRUE;
+}
+
+void SATA_PortReportNoDevice (PCore_Driver_Extension pCore, PDomain_Port pPort)
+{
+	PDomain_Device pDevice;
+	MV_U8 temp, i;
+#ifdef RAID_DRIVER
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_RAID);
+#else
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_HBA);
+#endif /* RAID_DRIVER */
+	struct mod_notif_param param;
+
+	mvRemovePortWaitingList( pCore, pPort->Id );
+
+	/* if PM - clear all the device attached to the port */
+	if( pPort->Type == PORT_TYPE_PM )
+		temp = MAX_DEVICE_PER_PORT-1;
+	else
+		temp = 0;
+
+	for( i=0; i<=temp; i++ )
+	{
+		pDevice = &pPort->Device[i];
+
+		if( pDevice->Status & DEVICE_STATUS_FUNCTIONAL )
+		{
+			if( pDevice->Internal_Req != NULL )
+			{
+				pCore->Total_Device_Count--;
+				ReleaseInternalReqToPool( pCore, pDevice->Internal_Req );
+				pDevice->Internal_Req = NULL;
+			}
+			param.lo = pDevice->Id;
+#ifdef RAID_DRIVER
+			RAID_ModuleNotification(pUpperLayer, EVENT_DEVICE_REMOVAL, &param);
+#else
+			HBA_ModuleNotification(pUpperLayer,
+					       EVENT_DEVICE_REMOVAL,
+					       &param);
+#endif /* RAID_DRIVER */
+			pPort->Device_Number--;
+		}
+
+		pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+		pDevice->State = DEVICE_STATE_INIT_DONE;
+	}
+}
+
+#ifdef SUPPORT_PM
+void mvHandlePMUnplug (PCore_Driver_Extension pCore, PDomain_Device pDevice);
+#endif
+
+void SATA_PortReset(
+	PDomain_Port pPort,
+	MV_BOOLEAN hardReset
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PDomain_Device pDevice = &pPort->Device[0];
+	MV_U32 signature;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 tmp, old_stat;
+	MV_U8 i;
+	MV_U8 skip = MV_FALSE;
+
+	/* No running commands at this moment */
+//	MV_ASSERT( pPort->Running_Slot==0 );
+//	MV_ASSERT( pPort->Port_State==PORT_STATE_IDLE );
+
+	pPort->Device_Number = 0;
+
+	MV_DPRINT(("Enter SATA_PortReset.\n"));
+	/* If we already reached the max number of devices supported,
+	   disregard the rest */
+
+	{
+#ifdef SUPPORT_PM
+		/*The timeout retry count more than CORE_MAX_RESET_COUNT*/
+		if( pPort->Type == PORT_TYPE_PM ){
+			MV_BOOLEAN RetryCountOverflow = MV_FALSE;
+			for( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+			{
+				if((pPort->Device[i].Reset_Count > CORE_MAX_RESET_COUNT)
+				    && (pPort->Device[i].Status & DEVICE_STATUS_FUNCTIONAL )){
+					mvHandlePMUnplug(pCore, &pPort->Device[i]);
+					MV_DPRINT(("Remove device %d on port %d is gone\n",i ,pPort->Id));
+					RetryCountOverflow = MV_TRUE;
+				}
+			}
+			if(RetryCountOverflow)
+				return;
+		}
+		else
+#endif
+		{
+			if((pDevice->Reset_Count > CORE_MAX_RESET_COUNT)
+			    &&  (pDevice->Status & DEVICE_STATUS_FUNCTIONAL)){
+				SATA_PortReportNoDevice( pCore, pPort );
+				MV_DPRINT(("Remove port %d\n",pPort->Id));
+				return;
+			}
+		}
+	}
+
+
+	if( pCore->Total_Device_Count >= MAX_DEVICE_SUPPORTED )
+	{
+		for( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+		{
+			pPort->Device[i].State = DEVICE_STATE_INIT_DONE;
+			pPort->Device[i].Status = DEVICE_STATUS_NO_DEVICE;
+		}
+		MV_DPRINT(("We have too many devices %d.\n", pCore->Total_Device_Count));
+		return;
+	}
+
+	if ( hardReset )
+	{
+
+			MV_U32 SControl,CounterForHardReset = 1000;
+			SControl = MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);
+			SControl &= ~0x0000000F;
+			SControl |= 0x01;
+			MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+			HBA_SleepMillisecond(pCore, 1);
+			SControl &= ~0x0000000F;
+			SControl |= 0x00;
+			MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+			while(((MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT)&0x0F) != 0x03) && CounterForHardReset>0){
+				HBA_SleepMillisecond(pCore, 1);
+				CounterForHardReset--;
+			}
+
+	}
+
+#ifdef FORCE_1_5_G
+	/* It'll trigger OOB. Looks like PATA hardware reset.
+	 * Downgrade 3G to 1.5G
+	 * If Port Multiplier is attached, only the PM is downgraded. */
+	{
+		SStatus = MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT);
+		if ( (SStatus&0xF0)==0x20 )
+		{
+			/* 3G */
+			SControl = MV_REG_READ_DWORD(portMmio, PORT_SCR_CTL);
+			SControl &= ~0x000000FF;
+			SControl |= 0x11;
+			MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+            HBA_SleepMillisecond(pCore, 2);
+			SControl &= ~0x000000FF;
+			SControl |= 0x10;
+			MV_REG_WRITE_DWORD(portMmio, PORT_SCR_CTL, SControl);
+			HBA_SleepMillisecond(pCore, 2);
+		}
+	}
+#endif
+
+	if( !SATA_PortDeviceDetected(pPort) )
+	{
+#if defined(SUPPORT_ERROR_HANDLING)
+		if( pPort->Setting & PORT_SETTING_PM_FUNCTIONAL )
+		{
+			pPort->Setting &= ~PORT_SETTING_PM_FUNCTIONAL;
+			pPort->Setting &= ~PORT_SETTING_PM_EXISTING;
+			MV_DPRINT(("PM on port %d is gone\n", pPort->Id));
+			SATA_PortReportNoDevice( pCore, pPort );
+		}
+		else if( pDevice->Status & DEVICE_STATUS_FUNCTIONAL )
+		{
+			MV_DPRINT(("device on port %d is gone\n", pPort->Id));
+			SATA_PortReportNoDevice( pCore, pPort );
+		}
+#endif
+
+		// fixed: have to set each device individually - or hot plug will have problem
+		for (i=0; i<MAX_DEVICE_PER_PORT; i++)
+		{
+			pDevice = &pPort->Device[i];
+			pDevice->State = DEVICE_STATE_INIT_DONE;
+		}
+
+		mvDeviceStateMachine(pCore, pDevice);
+		return;
+	}
+
+	MV_DPRINT(("find device on port %d.\n", pPort->Id));
+	if( !SATA_PortDeviceReady(pPort) )
+	{
+
+#if defined(SUPPORT_ERROR_HANDLING)
+		if( pPort->Setting & PORT_SETTING_PM_FUNCTIONAL )
+		{
+			pPort->Setting &= ~PORT_SETTING_PM_FUNCTIONAL;
+			MV_DPRINT(("PM on port %d is non-functional\n", pPort->Id));
+			SATA_PortReportNoDevice( pCore, pPort );
+		}
+		else if( pDevice->Status & DEVICE_STATUS_FUNCTIONAL )
+		{
+			MV_DPRINT(("device on port %d is non-functional\n", pPort->Id));
+			SATA_PortReportNoDevice( pCore, pPort );
+			pDevice->Status = DEVICE_STATUS_EXISTING;
+		}
+#endif
+		for (i=0; i<MAX_DEVICE_PER_PORT; i++)
+		{
+			pDevice = &pPort->Device[i];
+			pDevice->State = DEVICE_STATE_INIT_DONE;
+		}
+
+		mvDeviceStateMachine(pCore, pDevice);
+		return;
+	}
+	MV_DPRINT(("find device ready on port %d.\n", pPort->Id));
+
+
+#ifdef SUPPORT_PM
+	/* link error work around */
+	mvDisableIntr( portMmio, old_stat );
+	MV_REG_WRITE_DWORD( pPort->Mmio_Base, PORT_VSR_ADDR, 0x5 );
+	tmp = MV_REG_READ_DWORD( pPort->Mmio_Base, PORT_VSR_DATA );
+	MV_REG_WRITE_DWORD( pPort->Mmio_Base, PORT_VSR_DATA, tmp | MV_BIT(26));
+	HBA_SleepMillisecond( pCore, 1 );
+	mvEnableIntr( portMmio, old_stat );
+	if(!pCore->Is_Dump)
+		HBA_SleepMillisecond(pCore, 1000);
+
+	if ( (pCore->State!=CORE_STATE_STARTED) &&
+		 (pCore->Flag_Fastboot_Skip & FLAG_SKIP_PM) )
+		 skip = MV_TRUE;
+
+	if(!skip) /*Not skip in running time*/
+	{
+		MV_DPRINT(("SATA_PortReset not skipped\n"));
+
+		/* Always turn the PM bit on - otherwise won't work! */
+		tmp = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+		MV_REG_WRITE_DWORD(portMmio, PORT_CMD, tmp | MV_BIT(17));
+		tmp=MV_REG_READ_DWORD(portMmio, PORT_CMD);	/* flush */
+
+
+		if(!pCore->Is_Dump) {
+			HBA_SleepMillisecond(pCore, 2000);
+			//hba_msleep(2000);
+		}
+
+		if (! (SATA_PortSoftReset( pCore, pPort )) )
+		{
+#if defined(SUPPORT_ERROR_HANDLING) || defined(_OS_LINUX)
+			if( pPort->Setting & PORT_SETTING_PM_FUNCTIONAL )
+			{
+				pPort->Setting &= ~PORT_SETTING_PM_FUNCTIONAL;
+				SATA_PortReportNoDevice( pCore, pPort );
+			}
+#endif
+			return;
+		}
+	}
+	else {
+		MV_DPRINT(("SATA_PortReset is skipped.\n"));
+	}
+
+	if( pPort->Type == PORT_TYPE_PM )
+	{
+		SATA_InitPM( pPort );
+	}
+	else
+#endif
+	{
+
+#ifdef SUPPORT_PM
+	/* not a PM - turn off the PM bit in command register */
+	tmp = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+	MV_REG_WRITE_DWORD(portMmio, PORT_CMD, tmp & (~MV_BIT(17)));
+	tmp=MV_REG_READ_DWORD(portMmio, PORT_CMD);	/* flush */
+#endif
+
+	signature = MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_SIG);
+
+
+
+	if ( signature==0xEB140101 )				/* ATAPI signature */
+	{
+		pDevice->Device_Type |= DEVICE_TYPE_ATAPI;
+	}
+	else
+	{
+		MV_DASSERT( signature==0x00000101 );	/* ATA signature */
+	}
+
+	/* set the rest of the devices on this port */
+	for ( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+	{
+		pPort->Device[i].Status = DEVICE_STATUS_NO_DEVICE;
+		pPort->Device[i].State = DEVICE_STATE_INIT_DONE;
+	}
+
+	/* Device is ready */
+	pPort->Device[0].Internal_Req = GetInternalReqFromPool(pCore);
+	if( pPort->Device[0].Internal_Req == NULL )
+	{
+		MV_DPRINT(("ERROR: Unable to get an internal request buffer\n"));
+		// can't initialize without internal buffer - just set this disk down
+		//pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+		//pDevice->State = DEVICE_STATE_INIT_DONE;
+	}
+	else
+	{
+		pPort->Device[0].Status = DEVICE_STATUS_EXISTING|DEVICE_STATUS_FUNCTIONAL;
+		pPort->Device[0].State = DEVICE_STATE_RESET_DONE;
+		pPort->Device_Number = 1;	/* We have one device here. */
+	}
+
+	mvDeviceStateMachine(pCore, &pPort->Device[0]);/*This is single drive.*/
+	}
+}
+
+MV_VOID PATA_MakeControllerCommandBlock(
+	MV_PU16 pControllerCmd,
+	MV_U8 address,
+	MV_U8 data,
+	MV_BOOLEAN master,
+	MV_BOOLEAN write
+	)
+{
+	*pControllerCmd = 0L;
+
+	if ( write )
+		*((MV_PU8)pControllerCmd) = data;
+	*pControllerCmd |= (address<<8);
+	if ( !master )
+		*pControllerCmd |= MV_BIT(13);
+	if ( !write )
+		*pControllerCmd |= MV_BIT(14);
+}
+
+/* Poll the ATA register using enhanced mode. Exp register and Data is not included. */
+MV_BOOLEAN PATA_PollControllerCommand(
+	PDomain_Port pPort,
+	MV_U8 slot,
+	MV_U8 registerAddress,
+	MV_U8 registerData,
+	MV_BOOLEAN master,
+	MV_BOOLEAN write,
+	PATA_TaskFile pTaskFile
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_LPVOID port_mmio = pPort->Mmio_Base;
+	/* Cannot use Is_Slave to judge whether it's a master device. The flag may be not ready yet. */
+
+	PMV_PATA_Command_Header header = PATA_GetCommandHeader(pPort, slot);
+	PMV_Command_Table pCmdTable = Port_GetCommandTable(pPort, slot);
+	MV_PU16 pCmdTableU16 = (MV_PU16)pCmdTable;
+	MV_U32 loop = 1000, i;
+	MV_U32 temp = 0;
+
+	/* Always use the first slot */
+	MV_ASSERT( (pPort->Running_Slot&(1L<<slot))==0 );
+	MV_ZeroMemory( pTaskFile, sizeof(ATA_TaskFile) );
+	MV_ZeroMemory(header, sizeof(MV_PATA_Command_Header));
+
+	/* Command list */
+	header->Controller_Command = 1;
+	header->PIO_Sector_Count = 1;	/* How many command */
+
+	if ( !master )
+		header->Is_Slave = 1;
+
+	header->Table_Address = pPort->Cmd_Table_DMA.parts.low;
+	header->Table_Address_High = pPort->Cmd_Table_DMA.parts.high;
+
+	PATA_MakeControllerCommandBlock(pCmdTableU16++, registerAddress, registerData, master, write);
+
+	MV_REG_WRITE_DWORD(port_mmio, PORT_CMD_ISSUE, MV_BIT(0));
+	MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE);	/* flush */
+
+	/* Loop command issue to check whether it's finished. Hardware won't trigger interrupt. */
+	while ( loop>0 )
+	{
+		/* check interrupt */
+		temp = MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE);
+
+		if ( temp ==0 )	/* It's done. */
+		{
+			/* Anyway it's still better to clear the interrupt. */
+			temp = MV_REG_READ_DWORD(port_mmio, PORT_IRQ_STAT);
+			MV_REG_WRITE_DWORD(port_mmio, PORT_IRQ_STAT, temp);
+			MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, (1L<<pPort->Id));
+
+			if ( master )
+			{
+				for (i=0; i<1000; i++)/*Workaround for WD PATA disk*/
+				{
+					temp = MV_REG_READ_DWORD(port_mmio, PORT_MASTER_TF0);
+					if(temp == 0x7F) break;
+					if((temp & 0x80) == 0) break;
+					HBA_SleepMillisecond(pCore, 1);
+				}
+
+				pTaskFile->Command = (MV_U8)temp;
+				pTaskFile->Device = (MV_U8)(temp>>8);
+				pTaskFile->Features = (MV_U8)(temp>>24);
+				temp = MV_REG_READ_DWORD(port_mmio, PORT_MASTER_TF1);
+				pTaskFile->LBA_Low = (MV_U8)(temp>>8);
+				pTaskFile->Sector_Count = (MV_U8)(temp>>24);
+				temp = MV_REG_READ_DWORD(port_mmio, PORT_MASTER_TF2);
+				pTaskFile->LBA_High = (MV_U8)(temp>>8);
+				pTaskFile->LBA_Mid = (MV_U8)(temp>>24);
+			}
+			else
+			{
+				for (i=0; i<1000; i++)/*Workaround for WD PATA disk*/
+				{
+					temp = MV_REG_READ_DWORD(port_mmio, PORT_SLAVE_TF0);
+					if(temp == 0x7F) break;
+					if((temp & 0x80) == 0) break;
+					HBA_SleepMillisecond(pCore, 1);
+				}
+
+				pTaskFile->Command = (MV_U8)temp;
+				pTaskFile->Device = (MV_U8)(temp>>8);
+				pTaskFile->Features = (MV_U8)(temp>>24);
+				temp = MV_REG_READ_DWORD(port_mmio, PORT_SLAVE_TF1);
+				pTaskFile->LBA_Low = (MV_U8)(temp>>8);
+				pTaskFile->Sector_Count = (MV_U8)(temp>>24);
+				temp = MV_REG_READ_DWORD(port_mmio, PORT_SLAVE_TF2);
+				pTaskFile->LBA_High = (MV_U8)(temp>>8);
+				pTaskFile->LBA_Mid = (MV_U8)(temp>>24);
+			}
+			return MV_TRUE;
+		}
+		HBA_SleepMillisecond(pCore, 1);
+		loop--;
+	}
+
+	/* If this command is not completed and hardware is not cleared, we'll have trouble. */
+	MV_DASSERT( MV_REG_READ_DWORD(port_mmio, PORT_CMD_ISSUE)==0 );
+
+	return MV_FALSE;
+}
+
+MV_BOOLEAN PATA_PortDeviceWaitForBusy(
+	PDomain_Port pPort,
+	MV_BOOLEAN master
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	ATA_TaskFile taskFile;
+	/* PATA_PollControllerCommand will poll for 1 millisecond each time, so total 5 seconds ,1 second=1000 millisecond*/
+	MV_U32 retry = 5000;
+	do {
+		if ( master )
+			PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE, 0xA0, master, MV_TRUE, &taskFile);
+		else
+			PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE, 0xB0, master, MV_TRUE, &taskFile);
+		HBA_SleepMillisecond(pCore, 1);
+		retry--;
+	} while ( (taskFile.Command&MV_BIT(7)) && (retry>0) );
+
+#if 1
+	if ( taskFile.Command&MV_BIT(7) )
+	{
+		MV_DPRINT(("Port %d %s is busy retry=%d.\n",
+			pPort->Id, master?"master":"slave", (5000-retry)));
+	}
+	else
+	{
+		MV_DPRINT(("Port %d %s is not busy retry=%d.\n",
+			pPort->Id, master?"master":"slave", (5000-retry)));
+	}
+#endif
+
+	return ( !(taskFile.Command&MV_BIT(7)) );
+}
+
+MV_BOOLEAN PATA_PortDeviceDetected(PDomain_Port pPort, MV_BOOLEAN master, MV_BOOLEAN * isATAPI)
+{
+	ATA_TaskFile taskFile;
+
+	if ( master )
+		PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE, 0xA0, master, MV_TRUE, &taskFile);
+	else
+		PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE, 0xB0, master, MV_TRUE, &taskFile);
+
+#if 0
+	MV_DPRINT(("PATA_PortDeviceDetected: Sector_Count=0x%x, LBA_Low=0x%x, LBA_Mid=0x%x, LBA_High=0x%x.\n",
+		taskFile.Sector_Count, taskFile.LBA_Low, taskFile.LBA_Mid, taskFile.LBA_High));
+#endif
+
+	if ( //(taskFile.Sector_Count==0x01) &&
+		(taskFile.LBA_Low==0x01)
+		&& (taskFile.LBA_Mid==0x14)
+		&& (taskFile.LBA_High==0xEB) )
+	{
+		/* ATAPI signature found */
+		*isATAPI = MV_TRUE;
+		return MV_TRUE;
+	}
+
+	if ( (taskFile.Sector_Count==0x01)
+		&& (taskFile.LBA_Low==0x01)
+		&& (taskFile.LBA_Mid==0x00)
+		&& (taskFile.LBA_High==0x00) )
+	{
+
+		// if status is 0, conclude that drive is not present
+		if ( taskFile.Command == 0)
+			return MV_FALSE;
+
+		/* ATA signature found */
+		*isATAPI = MV_FALSE;
+		return MV_TRUE;
+	}
+
+	return MV_FALSE;
+}
+
+MV_BOOLEAN PATA_PortDeviceReady(PDomain_Port pPort, MV_BOOLEAN master, MV_BOOLEAN * isATAPI)
+{
+	ATA_TaskFile taskFile;
+
+	if ( master )
+		PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE, 0xA0, master, MV_TRUE, &taskFile);
+	else
+		PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE, 0xB0, master, MV_TRUE, &taskFile);
+
+#if 0
+	MV_DPRINT(("PATA_PortDeviceReady: Sector_Count=0x%x, LBA_Low=0x%x, LBA_Mid=0x%x, LBA_High=0x%x.\n",
+		taskFile.Sector_Count, taskFile.LBA_Low, taskFile.LBA_Mid, taskFile.LBA_High));
+#endif
+
+	if ( /* (taskFile.Sector_Count==0x01) && */
+		(taskFile.LBA_Low==0x01)
+		&& (taskFile.LBA_Mid==0x14)
+		&& (taskFile.LBA_High==0xEB) )
+	{
+		/* ATAPI device */
+		*isATAPI = MV_TRUE;
+
+
+
+		return MV_TRUE;	/* ATAPI is always ready. */
+	}
+
+	if ( (taskFile.Sector_Count==0x01)
+		&& (taskFile.LBA_Low==0x01)
+		&& (taskFile.LBA_Mid==0x00)
+		&& (taskFile.LBA_High==0x00) )
+	{
+		/* ATA device */
+		*isATAPI = MV_FALSE;
+		if ( (taskFile.Command&0x50)==0x50 )
+			return MV_TRUE;
+		else
+            return MV_FALSE;
+
+
+
+	}
+
+	return MV_FALSE;
+}
+
+void PATA_PortReset(
+	PDomain_Port pPort,
+	MV_BOOLEAN hardReset
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PDomain_Device pDevice = NULL;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_BOOLEAN temp, isMaster;
+	ATA_TaskFile taskFile;
+	MV_U8 i;
+	MV_U32 registerValue;
+	MV_U32 retry;
+	MV_U8  skip = MV_FALSE;
+	MV_BOOLEAN working[2];	/* Check whether the master/slave device is functional. */
+	MV_BOOLEAN isATAPI[2];	/* Check whether it's ATAPI device. */
+	MV_BOOLEAN unplug[2];
+#ifdef SUPPORT_ERROR_HANDLING
+#ifdef RAID_DRIVER
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_RAID);
+#else
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_HBA);
+#endif /* RAID_DRIVER */
+	struct mod_notif_param param;
+#endif /* SUPPORT_ERROR_HANDLING */
+
+	/* If we already reached the max number of devices supported,
+	   disregard the rest */
+	if( pCore->Total_Device_Count >= MAX_DEVICE_SUPPORTED )
+	{
+		for( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+		{
+			pPort->Device[i].State = DEVICE_STATE_INIT_DONE;
+			pPort->Device[i].Status = DEVICE_STATUS_NO_DEVICE;
+		}
+		return;
+	}
+
+	unplug[0]=MV_FALSE;
+	unplug[1]=MV_FALSE;
+	pPort->Device_Number = 0;
+	/*
+	 * For PATA device, reset signal is shared between master and slave.
+	 * So both hard reset and soft reset are port based, not device based.
+	 */
+	if ( hardReset )
+	{
+#if 1
+		int loop = 0;
+		registerValue = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+		MV_DASSERT( !(registerValue&PORT_CMD_PATA_HARD_RESET) );
+
+		registerValue |= PORT_CMD_PATA_HARD_RESET;
+		MV_REG_WRITE_DWORD(portMmio, PORT_CMD, registerValue);
+
+		do {
+			loop++;
+			registerValue = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+		} while ( registerValue&PORT_CMD_PATA_HARD_RESET && loop < 5000);
+
+		HBA_SleepMillisecond(pCore, 2500);
+		//hba_msleep(2500);
+		MV_DASSERT( MV_REG_READ_DWORD(portMmio, PORT_CMD_ISSUE)==0 );
+#endif
+	}
+
+
+	if ( (pCore->State!=CORE_STATE_STARTED) &&
+		 (pCore->Flag_Fastboot_Skip & FLAG_SKIP_PATA_DEVICE) )
+		 skip = MV_TRUE;
+
+	if (skip)
+	{
+		for (i=0; i<MAX_DEVICE_PER_PORT; i++)
+		{
+			pDevice = &pPort->Device[i];
+			pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+			pDevice->State = DEVICE_STATE_INIT_DONE;
+		}
+	}
+	else
+	{
+#if 1
+		/* Do soft reset. Soft reset is port based, not device based. */
+		pDevice = &pPort->Device[0];
+		PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE_CONTROL, MV_BIT(2), MV_TRUE, MV_TRUE, &taskFile);
+		HBA_SleepMicrosecond(pCore, 10);	/* At least 5 microseconds. */
+		//hba_msleep(10);
+
+		pDevice = &pPort->Device[0];
+		PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE_CONTROL, 0, MV_TRUE, MV_TRUE, &taskFile);
+		HBA_SleepMillisecond(pCore, 5);		/* At least 2 millisecond. */
+		//hba_msleep(5);
+#endif
+
+		isMaster = MV_TRUE;
+
+		for ( i=2; i<MAX_DEVICE_PER_PORT; i++ )
+		{
+			pDevice = &pPort->Device[i];
+			pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+			pDevice->State = DEVICE_STATE_INIT_DONE;
+		}
+
+		/*
+		 * Check master and slave devices. Master is at device[0], Slave is at device [1].
+		 */
+		/* Slave/Master device. Detect first. After it's totally done, we can send request to the devices. */
+		for ( i=0; i<2; i++ )
+		{
+			pDevice = NULL;/* Shouldn't use pDevice here. */
+
+			/* Wait for busy after the reset */
+			temp = PATA_PortDeviceWaitForBusy(pPort, isMaster);
+
+			/*
+			* Suppose after waiting for 5 seconds for the BSY signal, we only need check the signature once.
+			* But I found one ATAPI device BSY is clear right away.
+			* But the first time we read the signature, it's all 0x7F.
+			* Only after a while, it will return the correct value.
+			*/
+			if ( temp )
+			{
+				/*extend retry times,20 times,sometimes PATA disk is still not ready*/
+				retry = 200;
+				do {
+					temp = PATA_PortDeviceDetected(pPort, isMaster, &isATAPI[i]);
+					temp &= PATA_PortDeviceReady(pPort, isMaster, &isATAPI[i]);
+					retry--;
+					HBA_SleepMillisecond(pCore, 1);
+					//hba_msleep(1);
+				} while ( (retry>0)&&(!temp) );
+
+				if ( !temp )
+				{
+					if ( isMaster )
+						PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE, 0xA0, isMaster, MV_TRUE, &taskFile);
+					else
+						PATA_PollControllerCommand(pPort, 0, ATA_REGISTER_DEVICE, 0xB0, isMaster, MV_TRUE, &taskFile);
+
+					MV_DPRINT(("PATA task file: Command=0x%x, Sector_Count=0x%x, LBA_Low=0x%x, LBA_Mid=0x%x, LBA_High=0x%x retry=%d.\n",
+						taskFile.Command, taskFile.Sector_Count, taskFile.LBA_Low, taskFile.LBA_Mid, taskFile.LBA_High, (200-retry)));
+				}
+				else
+				{
+					MV_DPRINT(("PATA is detected and ready after retry= %d.\n", (200-retry)));
+				}
+			}
+
+			working[i] = temp;
+			isMaster = MV_FALSE;
+
+			pDevice = &pPort->Device[i];
+			if ( isATAPI[i] )
+				pDevice->Device_Type |= DEVICE_TYPE_ATAPI;
+			else
+				pDevice->Device_Type &= ~DEVICE_TYPE_ATAPI;
+			pDevice->Is_Slave = (i==0)?MV_FALSE:MV_TRUE;
+
+			/*
+			 * If the device has been reset for too many times,
+			 * just set down this disk. It's better to set
+			 * MEDIA ERROR to the timeout request.
+			 */
+			if ( pDevice->Reset_Count>CORE_MAX_PATA_RESET_COUNT )
+				working[i] = MV_FALSE;
+
+			if ( !working[i] )
+			{
+				if ( pDevice->Status&DEVICE_STATUS_FUNCTIONAL )
+				{
+					pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+					MV_DPRINT(("Port %d %s is gone.\n", pPort->Id, pDevice->Is_Slave?"slave":"master"));
+					unplug[i] = MV_TRUE;
+				}
+				else
+				{
+					pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+					MV_DPRINT(("Port %d %s not ready.\n", pPort->Id, pDevice->Is_Slave?"slave":"master"));
+				}
+				pDevice->State = DEVICE_STATE_INIT_DONE;
+			}
+			else
+			{
+
+				MV_DPRINT(("Port %d %s ready.\n", pPort->Id, pDevice->Is_Slave?"slave":"master"));
+
+				pDevice->Internal_Req = GetInternalReqFromPool(pCore);
+				if( pDevice->Internal_Req == NULL )
+				{
+					MV_DPRINT(("ERROR: Unable to get an internal request buffer\n"));
+					// can't initialize without internal buffer - just set this disk down
+					pDevice->Status = DEVICE_STATUS_NO_DEVICE;
+					pDevice->State = DEVICE_STATE_INIT_DONE;
+				}
+				else
+				{
+					pDevice->Status = DEVICE_STATUS_EXISTING|DEVICE_STATUS_FUNCTIONAL;
+					pPort->Device_Number++;
+				}
+			}
+		}
+
+
+		/* Set Device State for all devices first */
+		for ( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+		{
+			pDevice = &pPort->Device[i];
+			if ( pDevice->Status & DEVICE_STATUS_FUNCTIONAL )
+			{
+				pDevice->State = DEVICE_STATE_RESET_DONE;
+				/* Don't start mvDeviceStateMachine now.
+				 * It may trigger other devices to send DMA request before resetting is done. */
+			}
+		}
+
+		/* After all the flags are set, we can do some related to the state machine and waiting list. */
+		for ( i=0; i<2; i++ )
+		{
+			pDevice = &pPort->Device[i];
+			if ( unplug[i] )
+			{
+				if( pDevice->Internal_Req!=NULL )
+				{
+					ReleaseInternalReqToPool( pCore, pDevice->Internal_Req );
+					pDevice->Internal_Req = NULL;
+				}
+
+				mvRemoveDeviceWaitingList( pCore, pDevice->Id, MV_TRUE );
+
+			#ifdef SUPPORT_ERROR_HANDLING
+				param.lo = pDevice->Id;
+				#ifdef RAID_DRIVER
+					RAID_ModuleNotification(pUpperLayer, EVENT_DEVICE_REMOVAL, &param);
+				#else
+					HBA_ModuleNotification(pUpperLayer, EVENT_DEVICE_REMOVAL, &param);
+				#endif
+			#endif
+			}
+		}
+
+		/* Then run the status machine.*/
+		/* Cable detection edit: We don't run the master state machine yet, if there is
+		   a slave attached */
+		for ( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+		{
+			pDevice = &pPort->Device[i];
+			if ( pDevice->Status & DEVICE_STATUS_FUNCTIONAL )
+			{
+				if (i != 0)
+					mvDeviceStateMachine(pCore, pDevice);
+				else
+				{
+					if (!(pPort->Device[1].Status & DEVICE_STATUS_FUNCTIONAL))
+						mvDeviceStateMachine(pCore, pDevice);
+				}
+			}
+		}
+	}
+
+	if ( pPort->Device_Number==0 )
+	{
+		/* Just use the first device to make the ball roll. */
+
+		mvDeviceStateMachine(pCore, &pPort->Device[0]);
+	}
+
+}
+
+static MV_BOOLEAN mvChannelStateMachine(
+	PCore_Driver_Extension pCore,
+	PDomain_Port pPort
+	)
+{
+	MV_U8 i;
+	MV_U8 portState;
+	PDomain_Device pDevice;
+	PDomain_Port pOrgPort = pPort;
+	#ifdef RAID_DRIVER
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_RAID);
+	#else
+	MV_PVOID pUpperLayer = HBA_GetModuleExtension(pCore, MODULE_HBA);
+	#endif
+
+	MV_DPRINT(("Start mvChannelStateMachine.\n"));
+
+	if ( pPort==NULL )
+		portState = PORT_STATE_IDLE;
+	else {
+		portState = pPort->Port_State;
+	}
+
+	//Each step: if fail like no device, should go to the end.
+	/* Channel state machine */
+	switch ( portState )
+	{
+		case PORT_STATE_IDLE:
+			/* To do reset */
+			for( i=0; i<pCore->Port_Num; i++ )
+			{
+				pPort = &pCore->Ports[i];
+				MV_DPRINT(("Port %d start mvChannelStateMachine in PORT_STATE_IDLE.\n", pPort->Id));
+				MV_DASSERT( pPort->Port_State==PORT_STATE_IDLE );
+				if ( pPort->Type==PORT_TYPE_PATA ){
+					if(pCore->Is_Dump)
+						PATA_PortReset( pPort, MV_FALSE );
+					else
+						PATA_PortReset( pPort, MV_TRUE );
+				}
+				else
+					SATA_PortReset( pPort, MV_FALSE );
+			}
+			break;
+
+		/*
+		 * Each port will call mvDeviceStateMachine for its devices.
+		 * When all the devices for that port are done, will call mvChannelStateMachine.
+		 */
+
+		case PORT_STATE_INIT_DONE:
+
+			MV_DPRINT(("Discovery port[%d] is finished.\n",pPort->Id));
+
+			/* Check whether all the ports are done. */
+			for ( i=0; i<pCore->Port_Num; i++ )
+			{
+				pPort = &pCore->Ports[i];
+				if ( pPort->Port_State != PORT_STATE_INIT_DONE )
+					return MV_TRUE;
+			}
+
+			MV_DPRINT(("Discovery procedure is finished.\n"));
+			MV_DPRINT(("Check need_reset=0x%x,pCore->State=0x%x.\n",pCore->Need_Reset,pCore->State));
+
+			/* Discovery procedure is finished. */
+			if(pCore->Need_Reset == 0)
+			{
+				if ( pCore->State==CORE_STATE_IDLE )
+				{
+					pCore->State = CORE_STATE_STARTED;
+					core_start_cmpl_notify(pCore);	/* The first time initialization */
+				}
+				else
+				{
+					/* check which device on this port needs to be reported */
+					for (i=0; i<MAX_DEVICE_PER_PORT; i++)
+					{
+						pDevice = &pOrgPort->Device[i];
+						if ( pDevice->Need_Notify )
+						{
+							struct mod_notif_param param;
+							param.lo = pDevice->Id;
+#ifdef RAID_DRIVER
+							RAID_ModuleNotification(pUpperLayer, EVENT_DEVICE_ARRIVAL, &param);
+#else
+							HBA_ModuleNotification(pUpperLayer, EVENT_DEVICE_ARRIVAL, &param);
+#endif
+							pDevice->Need_Notify = MV_FALSE;
+						}
+					}
+				}
+			}
+			else
+			{
+				pCore->Need_Reset = 0;
+				pCore->Resetting = 0;
+
+			}
+			/* Begin to handle request again. */
+			Core_HandleWaitingList(pCore);
+			break;
+	}
+
+	return MV_TRUE;
+}
+
+MV_BOOLEAN mvDeviceStateMachine(
+	PCore_Driver_Extension pCore,
+	PDomain_Device pDevice
+	)
+{
+	MV_U8 i;
+	PDomain_Port pPort = pDevice->PPort;
+
+
+	switch ( pDevice->State )
+	{
+		case DEVICE_STATE_RESET_DONE:
+			MV_DPRINT(("Device %d DEVICE_STATE_RESET_DONE.\n", pDevice->Id));
+
+			/* To do identify */
+			Device_IssueIdentify(pDevice->PPort, pDevice);
+			break;
+
+		case DEVICE_STATE_IDENTIFY_DONE:
+			MV_DPRINT(("Device %d DEVICE_STATE_IDENTIFY_DONE.\n", pDevice->Id));
+
+			/* To do set UDMA mode */
+			Device_IssueSetPIOMode(pDevice->PPort, pDevice);
+			break;
+
+		case DEVICE_STATE_SET_UDMA_DONE:
+			MV_DPRINT(("Device %d DEVICE_STATE_SET_UDMA_DONE.\n", pDevice->Id));
+
+			/* To do set PIO mode */
+			Device_EnableWriteCache(pDevice->PPort, pDevice);
+			break;
+
+		case DEVICE_STATE_SET_PIO_DONE:
+			MV_DPRINT(("Device %d DEVICE_STATE_SET_PIO_DONE.\n", pDevice->Id));
+
+			/* To do enable write cache */
+			Device_IssueSetUDMAMode(pDevice->PPort, pDevice);
+			break;
+
+		case DEVICE_STATE_ENABLE_WRITE_CACHE_DONE:
+			MV_DPRINT(("Device %d DEVICE_STATE_ENABLE_WRITE_CACHE_DONE.\n", pDevice->Id));
+
+            /* To do enable read ahead */
+			Device_EnableReadAhead( pDevice->PPort, pDevice );
+			break;
+
+		case DEVICE_STATE_ENABLE_READ_AHEAD_DONE:
+			MV_DPRINT(("Device %d DEVICE_STATE_ENABLE_READ_AHEAD_DONE.\n", pDevice->Id));
+
+	pDevice->State = DEVICE_STATE_INIT_DONE;
+	pCore->Total_Device_Count++;
+
+#ifdef COMMAND_ISSUE_WORKROUND
+	if (mv_core_check_is_reseeting(pCore)) {
+		MV_DPRINT(("Find disk after reset HBA on port[%d].\n",pPort->Id));
+		mv_core_init_reset_para(pPort);
+	}
+#endif
+
+		/* No break here. */
+
+		case DEVICE_STATE_INIT_DONE:
+			MV_DPRINT(("Device %d DEVICE_STATE_INIT_DONE.\n", pDevice->Id));
+			/* Check whether all devices attached to this port are done. */
+			for ( i=0; i<MAX_DEVICE_PER_PORT; i++ )
+			{
+				if ( pPort->Device[i].State!=DEVICE_STATE_INIT_DONE )
+					return MV_TRUE;
+			}
+			pPort->Port_State = PORT_STATE_INIT_DONE;
+			mvChannelStateMachine(pCore, pDevice->PPort);
+			break;
+
+		default:
+			break;
+	}
+
+	return MV_TRUE;
+}
+
+/*
+ * Global controller reset
+ */
+MV_BOOLEAN
+ResetController(PCore_Driver_Extension pCore)
+{
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_U32 tmp;
+	MV_BOOLEAN ret = MV_TRUE;
+
+/* #if (VER_OEM==VER_OEM_ASUS) */
+	MV_U8 i=0;
+/* #endif */
+
+	/*
+	If enter hibernation, do not do PHY reset to save time
+	*/
+	if (pCore->Is_Dump)
+		return MV_TRUE;
+
+	/* Reset controller */
+	tmp = MV_REG_READ_DWORD(mmio, HOST_CTL);
+	if ((tmp & HOST_RESET) == 0) {
+
+/* #if (VER_OEM==VER_OEM_ASUS) */
+		if(pCore->VS_Reg_Saved!=VS_REG_SIG)
+		{
+			for ( i=0; i<pCore->SATA_Port_Num; i++ )
+			{
+				Domain_Port *port;
+				port = &pCore->Ports[i];
+				MV_REG_WRITE_DWORD(port->Mmio_Base, PORT_VSR_ADDR, 0xc);
+				port->VS_RegC= MV_REG_READ_DWORD(port->Mmio_Base, PORT_VSR_DATA);
+				pCore->VS_Reg_Saved=VS_REG_SIG;
+			}
+		}
+/* #endif */
+		MV_REG_WRITE_DWORD(mmio, HOST_CTL, tmp|HOST_RESET);
+		MV_REG_READ_DWORD(mmio, HOST_CTL); /* flush */
+	}
+
+	/* Reset must complete within 1 second, or the hardware should be considered fried. */
+	HBA_SleepMillisecond(pCore, 1000);
+	//hba_msleep(1000);
+
+	tmp = MV_REG_READ_DWORD(mmio, HOST_CTL);
+	if (tmp & HOST_RESET) {
+		MV_DASSERT(MV_FALSE);
+		ret = MV_FALSE;
+	}
+
+
+/* #if (VER_OEM==VER_OEM_ASUS) */
+	if(pCore->VS_Reg_Saved==VS_REG_SIG)
+	{
+		for ( i=0; i<pCore->SATA_Port_Num; i++ )
+		{
+			Domain_Port *port;
+			port = &pCore->Ports[i];
+			MV_REG_WRITE_DWORD(port->Mmio_Base, PORT_VSR_ADDR, 0xc);
+			MV_REG_WRITE_DWORD(port->Mmio_Base, PORT_VSR_DATA, port->VS_RegC);
+		}
+	}
+	/* link error work around */
+	for ( i=0; i<pCore->SATA_Port_Num; i++ )
+	{
+		MV_U32 tmp, old_stat;
+		Domain_Port *port;
+		port = &pCore->Ports[i];
+
+		mvDisableIntr( port->Mmio_Base, old_stat );
+		MV_REG_WRITE_DWORD( port->Mmio_Base, PORT_VSR_ADDR, 0x5 );
+		tmp = MV_REG_READ_DWORD( port->Mmio_Base, PORT_VSR_DATA );
+		MV_REG_WRITE_DWORD( port->Mmio_Base, PORT_VSR_DATA, tmp | MV_BIT(26));
+		HBA_SleepMillisecond( pCore, 1 );
+		mvEnableIntr( port->Mmio_Base, old_stat );
+	}
+
+/* #endif */
+	return ret;
+}
+
+void PATA_ResetPort(PCore_Driver_Extension pCore, MV_U8 portId)
+{
+	PDomain_Port pPort = &pCore->Ports[portId];
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 tmp;
+
+	/* Make sure port is not active. If yes, stop the port. */
+	tmp = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+	/* For ACHI, four bits are avaiable. For 614x, PORT_CMD_FIS_ON is reserved. */
+	if (tmp & (PORT_CMD_PATA_LIST_ON | PORT_CMD_PATA_START)) {
+		tmp &= ~(PORT_CMD_PATA_LIST_ON | PORT_CMD_PATA_START);
+		MV_REG_WRITE_DWORD(portMmio, PORT_CMD, tmp);
+		MV_REG_READ_DWORD(portMmio, PORT_CMD); /* flush */
+
+		/* spec says 500 msecs for each bit, so
+			* this is slightly incorrect.
+			*/
+		HBA_SleepMillisecond(pCore, 500);
+	}
+
+	/* Clear error register if any */
+
+	/* Ack any pending irq events for this port */
+	tmp = MV_REG_READ_DWORD(portMmio, PORT_IRQ_STAT)&0xF;
+	if (tmp)
+		MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_STAT, tmp);
+	/* Ack pending irq in the host interrupt status register */
+	MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, 1 << portId);
+
+	/* set irq mask (enables interrupts) */
+#ifdef ENABLE_PATA_ERROR_INTERRUPT
+	MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_MASK, DEF_PORT_PATA_IRQ);
+#else
+	/*
+	 * Workaround
+	 * If PATA device has a error, even the error bit in the interrupt register is cleared.
+	 * Internal hardware will trigger one more(OS has no idea).
+	 * So because there is interrupt bit not cleared, the next command won't be issued.
+	 */
+	MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_MASK, MV_BIT(2)|MV_BIT(0));
+#endif
+}
+#ifdef CONFIG_PM
+extern void InitChip(PCore_Driver_Extension pCore);
+
+int core_resume(void *ext)
+{
+	int ret;
+	PCore_Driver_Extension core_ext;
+	core_ext=(PCore_Driver_Extension)ext;
+	MV_REG_WRITE_DWORD(core_ext->Mmio_Base, HOST_CTL, 0);
+	if(ResetController(core_ext) == MV_FALSE) {
+		MV_DPRINT(("Reset controller failed."));
+		return MV_FALSE;
+
+	}
+	InitChip(core_ext);
+	mvEnableGlobalIntr_resume(core_ext->Mmio_Base);
+
+	return 0;
+}
+int core_suspend(void *ext)
+{
+	int ret,tmp;
+	PCore_Driver_Extension core_ext;
+	core_ext=(PCore_Driver_Extension)ext;
+
+	mvDisableGlobalIntr(core_ext->Mmio_Base,tmp);
+	return 0;
+}
+#endif
+
+void SATA_ResetPort(PCore_Driver_Extension pCore, MV_U8 portId)
+{
+	PDomain_Port pPort = &pCore->Ports[portId];
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_LPVOID portMmio = pPort->Mmio_Base;
+	MV_U32 tmp, j;
+
+	/* Make sure port is not active. If yes, stop the port. */
+	tmp = MV_REG_READ_DWORD(portMmio, PORT_CMD);
+	/* For ACHI, four bits are avaiable. For 614x, PORT_CMD_FIS_ON is reserved. */
+	if (tmp & (PORT_CMD_LIST_ON | PORT_CMD_FIS_ON |
+			PORT_CMD_FIS_RX | PORT_CMD_START)) {
+		tmp &= ~(PORT_CMD_LIST_ON | PORT_CMD_FIS_ON |
+				PORT_CMD_FIS_RX | PORT_CMD_START);
+		MV_REG_WRITE_DWORD(portMmio, PORT_CMD, tmp);
+		MV_REG_READ_DWORD(portMmio, PORT_CMD); /* flush */
+
+		/* spec says 500 msecs for each bit, so
+			* this is slightly incorrect.
+			*/
+		HBA_SleepMillisecond(pCore, 500);
+		//hba_msleep(500);
+	}
+
+	//PORT_CMD enable bit(5): PIO command will issue PIO setup interrupt bit.
+	// Only after clear the PIO setup interrupt bit, the hardware will issue the PIO done interrupt bit.
+	// Maybe in this case, we needn't enable PIO setup interrupt bit but for some others we should.
+
+	#ifdef AHCI
+	/* For 614x, it's reserved. */
+	MV_REG_WRITE_DWORD(portMmio, PORT_CMD, PORT_CMD_SPIN_UP);
+	#endif
+
+	/* Wait for SATA DET(Device Detection) */
+	j = 0;
+	while (j < 100) {
+		HBA_SleepMillisecond(pCore, 10);
+		//hba_msleep(10);
+		tmp = MV_REG_READ_DWORD(portMmio, PORT_SCR_STAT);
+		if ((tmp & 0xf) == 0x3)
+			break;
+		j++;
+	}
+
+
+	/* Clear SATA error */
+	tmp = MV_REG_READ_DWORD(portMmio, PORT_SCR_ERR);
+	MV_REG_WRITE_DWORD(portMmio, PORT_SCR_ERR, tmp);
+
+	/* Ack any pending irq events for this port */
+	tmp = MV_REG_READ_DWORD(portMmio, PORT_IRQ_STAT);
+	if (tmp)
+		MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_STAT, tmp);
+	/* Ack pending irq in the host interrupt status register */
+	MV_REG_WRITE_DWORD(mmio, HOST_IRQ_STAT, 1 << portId);
+
+	/* set irq mask (enables interrupts) */
+	MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_MASK, DEF_PORT_IRQ);
+
+
+	/* FIFO controller workaround for 6121-B0B1B2, 6111-B0B1, and 6145-A0 */
+	if (
+		( (pCore->Device_Id==DEVICE_ID_THORLITE_2S1P)&&(pCore->Revision_Id>=0xB0) )
+		||
+		( (pCore->Device_Id==DEVICE_ID_THORLITE_2S1P_WITH_FLASH)&&(pCore->Revision_Id>=0xB0) )
+		||
+		( (pCore->Device_Id==DEVICE_ID_THORLITE_1S1P)&&(pCore->Revision_Id>=0xB0) )
+		||
+		( (pCore->Device_Id==DEVICE_ID_THOR_4S1P_NEW)&&(pCore->Revision_Id>=0xA0) )
+	)
+	{
+		tmp = (MV_REG_READ_DWORD( portMmio, PORT_FIFO_CTL ) & 0xFFFFF0FF ) | 0x500;
+		MV_REG_WRITE_DWORD( portMmio, PORT_FIFO_CTL, tmp);
+		MV_REG_READ_DWORD( portMmio, PORT_FIFO_CTL);		/* flush */
+	}
+}
+/*
+ * It's equivalent to ahci_host_init and ahci_port_start
+ */
+void InitChip(PCore_Driver_Extension pCore)
+{
+	MV_LPVOID mmio = pCore->Mmio_Base;
+	MV_U8 i;
+	PDomain_Port pPort;
+	MV_U32 tmp;
+
+	pCore->Capacity = MV_REG_READ_DWORD(mmio, HOST_CAP);
+
+	/*
+	 * For 614x, enable enhanced mode for PATA and interrupt.
+	 * For AHCI, enable AHCI.
+	 */
+	tmp = MV_REG_READ_DWORD(mmio, HOST_CTL);
+	MV_REG_WRITE_DWORD(mmio, HOST_CTL, (MV_U32)(tmp | HOST_IRQ_EN | HOST_MVL_EN));
+	tmp = MV_REG_READ_DWORD(mmio, HOST_CTL);
+
+	/* Ports implemented: enable ports */
+	pCore->Port_Map = MV_REG_READ_DWORD(mmio, HOST_PORTS_IMPL);
+	tmp = MV_REG_READ_DWORD(mmio, HOST_CAP);
+	//MV_DASSERT( pCore->Port_Num == ((tmp & 0x1f) + 1) );
+
+	/* Initialize ports */
+	for ( i = 0; i<pCore->Port_Num; i++) {
+		pPort = &pCore->Ports[i];
+		/* make sure port is not active */
+		if ( pPort->Type==PORT_TYPE_PATA )
+			PATA_ResetPort(pCore, i);
+		else
+			SATA_ResetPort(pCore, i);
+	}
+
+
+	/* Initialize port, set uncached memory pointer. */
+	for ( i = 0; i<pCore->Port_Num; i++) {
+		pPort = &pCore->Ports[i];
+
+		/* Set the sata port register */
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_LST_ADDR_HI, pPort->Cmd_List_DMA.parts.high);
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_LST_ADDR, pPort->Cmd_List_DMA.parts.low);
+		MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_LST_ADDR);
+
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_FIS_ADDR_HI, pPort->RX_FIS_DMA.parts.high);
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_FIS_ADDR, pPort->RX_FIS_DMA.parts.low);
+		MV_REG_READ_DWORD(pPort->Mmio_Base, PORT_FIS_ADDR);
+
+		/* AHCI is different with Thor */
+		#ifdef AHCI
+		MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_CMD,
+			PORT_CMD_ICC_ACTIVE | PORT_CMD_FIS_RX |	PORT_CMD_POWER_ON | PORT_CMD_SPIN_UP | PORT_CMD_START );
+		#else
+		if ( pPort->Type==PORT_TYPE_PATA )
+		{	/* 12<<24: Bit 24-28: Indicates ATAPI command CDB length in bytes */
+			MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_CMD, (12L<<24) | PORT_CMD_PATA_INTERRUPT | PORT_CMD_PATA_START );
+		}
+		else
+		{
+			/*
+			 * Workaround: Don't enable PORT_CMD_FIS_RX otherwise system will hang.
+			 */
+			MV_REG_WRITE_DWORD(pPort->Mmio_Base, PORT_CMD, PORT_CMD_START );
+		}
+		#endif
+	}
+
+	//MV_DPRINT("HostCtrl=0x%x,HostIntStatus=0x%x\n",MV_REG_READ_DWORD(mmio, HOST_CTL),MV_REG_READ_DWORD(mmio, HOST_IRQ_STAT));
+
+}
+
+MV_BOOLEAN mvAdapterStateMachine(
+	IN OUT MV_PVOID This,
+	IN MV_PVOID temp
+	)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+
+	switch (pCore->Adapter_State)
+	{
+		case ADAPTER_INITIALIZING:
+			MV_DPRINT(("start mvAdapterStateMachine.\n"));
+			if(ResetController(pCore) == MV_FALSE)
+				return MV_FALSE;
+
+			InitChip(pCore);
+			pCore->Adapter_State = ADAPTER_READY;
+#ifdef SUPPORT_TIMER
+			Timer_AddRequest( pCore, 2, mvAdapterStateMachine, pCore, NULL );
+#else
+			HBA_RequestTimer(pCore, 2000, (MV_VOID(*)(MV_PVOID))mvAdapterStateMachine);
+#endif
+
+			break;
+
+		case ADAPTER_READY:
+			MV_DPRINT(("start mvChannelStateMachine in timer.\n"));
+			mvChannelStateMachine(pCore, NULL);
+			break;
+
+		default:
+			break;
+	}
+	return MV_TRUE;
+}
+
+void Device_ParseIdentifyData(
+	IN PDomain_Device pDevice,
+	IN PATA_Identify_Data pATAIdentify
+	);
+
+void Core_InternalReqCallback(
+	 IN PCore_Driver_Extension pCore,
+	 IN PMV_Request pReq
+	 )
+{
+	PDomain_Port pPort;
+	PDomain_Device pDevice;
+	PATA_Identify_Data pATAIdentify;
+	MV_U8 portId, deviceId;
+	MV_U32 tmp;
+
+	portId = PATA_MapPortId(pReq->Device_Id);
+	deviceId = PATA_MapDeviceId(pReq->Device_Id);
+
+	pPort = &pCore->Ports[portId];
+	pDevice = &pPort->Device[deviceId];
+
+	//It's possible that CDB_CORE_READ_LOG_EXT returns error and come here
+	//because we send CDB_CORE_READ_LOG_EXT no matter NCQ is running or not.
+	//if ( pReq->Cdb[2]!=CDB_CORE_READ_LOG_EXT )
+	if (( pReq->Cdb[2]!=CDB_CORE_READ_LOG_EXT ) && ( pReq->Cdb[2]!=CDB_CORE_ENABLE_WRITE_CACHE )&&( pReq->Cdb[2]!=CDB_CORE_ENABLE_READ_AHEAD ))
+	{
+		if( pReq->Scsi_Status != REQ_STATUS_SUCCESS )
+		{
+			/* request didn't finish correctly - we set device to existing
+			   and finish state machine */
+			pDevice->Status = DEVICE_STATUS_EXISTING;
+			pDevice->State = DEVICE_STATE_INIT_DONE;
+			mvDeviceStateMachine(pCore, pDevice);
+			return;
+		}
+	}
+
+	pATAIdentify = (PATA_Identify_Data)pPort->Device[deviceId].Scratch_Buffer;
+	/* Handle internal request like identify */
+	MV_DASSERT( pReq->Cdb[0]==SCSI_CMD_MARVELL_SPECIFIC );
+	MV_DASSERT( pReq->Cdb[1]==CDB_CORE_MODULE );
+	MV_ASSERT( portId < MAX_PORT_NUMBER );
+
+	if ( pReq->Cdb[2]==CDB_CORE_IDENTIFY )
+	{
+#ifdef _OS_LINUX
+		hba_swap_buf_le16((MV_PU16) pATAIdentify,
+				  sizeof(ATA_Identify_Data)/sizeof(MV_U16));
+#endif /* _OS_LINUX  */
+		Device_ParseIdentifyData(pDevice, pATAIdentify);
+		if(pDevice->State != DEVICE_STATE_RESET_DONE){
+			return;
+			}
+		//MV_ASSERT( pDevice->State == DEVICE_STATE_RESET_DONE );
+		pDevice->State = DEVICE_STATE_IDENTIFY_DONE;
+
+		/* Detect PATA Cable:
+		 *** if there are master and slave, only detect after slave
+			 has finished identify
+		 *** if there is only master, detect after master has finished
+			 identify
+		 */
+		if (pPort->Type == PORT_TYPE_PATA)
+		{
+			if ((pDevice->Is_Slave) ||
+				(!(pDevice->Is_Slave) && !(pPort->Device[1].Status & DEVICE_STATUS_FUNCTIONAL)))
+			{
+				tmp = MV_IO_READ_DWORD(pCore->Base_Address[4], 0);
+				if( tmp & MV_BIT(8) )	{//40 pin cable
+					pPort->PATA_cable_type = MV_40_PIN_CABLE;
+				}else {//80 pin cable
+					pPort->PATA_cable_type = MV_80_PIN_CABLE;
+				}
+
+				/* Cable is detected, now we start state machine for master,
+				   if needed */
+				if (pDevice->Is_Slave && (pPort->Device[0].Status & DEVICE_STATUS_FUNCTIONAL))
+				{
+					mvDeviceStateMachine(pCore, &pPort->Device[0]);
+				}
+			}
+		}
+
+		mvDeviceStateMachine(pCore, pDevice);
+		return;
+	}
+	else if ( pReq->Cdb[2]==CDB_CORE_SET_UDMA_MODE )
+	{
+		pDevice->State = DEVICE_STATE_SET_UDMA_DONE;
+		mvDeviceStateMachine(pCore, pDevice);
+	}
+	else if ( pReq->Cdb[2]==CDB_CORE_SET_PIO_MODE )
+	{
+		pDevice->State = DEVICE_STATE_SET_PIO_DONE;
+		mvDeviceStateMachine(pCore, pDevice);
+	}
+	else if ( pReq->Cdb[2]==CDB_CORE_ENABLE_WRITE_CACHE )
+	{
+		pDevice->State = DEVICE_STATE_ENABLE_WRITE_CACHE_DONE;
+		mvDeviceStateMachine(pCore, pDevice);
+	}
+	else if ( pReq->Cdb[2]==CDB_CORE_ENABLE_READ_AHEAD )
+	{
+		pDevice->State = DEVICE_STATE_ENABLE_READ_AHEAD_DONE;
+		mvDeviceStateMachine(pCore, pDevice);
+	}
+	else if ( pReq->Cdb[2]==CDB_CORE_READ_LOG_EXT )
+	{
+		/* Do nothing. Just use this command to clear outstanding IO during error handling. */
+		MV_PRINT("Read Log Ext is finished on device 0x%x.\n", pDevice->Id);
+	}
+}
+
+static void Device_IssueIdentify(
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice
+	)
+{
+	PMV_Request pReq = pDevice->Internal_Req;
+	PMV_SG_Table pSGTable = &pReq->SG_Table;
+
+	MV_ZeroMvRequest(pReq);
+
+	/* Prepare identify ATA task */
+	pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+	pReq->Cdb[1] = CDB_CORE_MODULE;
+	pReq->Cdb[2] = CDB_CORE_IDENTIFY;
+	pReq->Device_Id = pDevice->Id;
+
+	//pReq->Req_Flag;
+	pReq->Cmd_Initiator = pPort->Core_Extension;
+	pReq->Data_Transfer_Length = sizeof(ATA_Identify_Data);
+	pReq->Data_Buffer = pDevice->Scratch_Buffer;
+	pReq->Completion = (void(*)(MV_PVOID,PMV_Request))Core_InternalReqCallback;
+	MV_DASSERT( SATA_SCRATCH_BUFFER_SIZE>=sizeof(ATA_Identify_Data) );
+
+	/* Make SG table */
+	SGTable_Init(pSGTable, 0);
+	SGTable_Append(pSGTable,
+				pDevice->Scratch_Buffer_DMA.parts.low,
+				pDevice->Scratch_Buffer_DMA.parts.high,
+				pReq->Data_Transfer_Length
+				);
+	MV_DASSERT( pReq->Data_Transfer_Length%2==0 );
+
+
+	/* Send this internal request */
+	Core_ModuleSendRequest(pPort->Core_Extension, pReq);
+}
+
+void Device_IssueReadLogExt(
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice
+	)
+{
+	PMV_Request pReq = pDevice->Internal_Req;
+	PMV_SG_Table pSGTable = &pReq->SG_Table;
+
+	MV_ZeroMvRequest(pReq);
+	MV_PRINT("Device_IssueReadLogExt on device 0x%x.\n", pDevice->Id);
+
+	//Disable NCQ after we found NCQ error.
+	pDevice->Capacity &= ~(DEVICE_CAPACITY_NCQ_SUPPORTED);
+
+	/* We support READ LOG EXT command with log page of 10h. */
+	pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+	pReq->Cdb[1] = CDB_CORE_MODULE;
+	pReq->Cdb[2] = CDB_CORE_READ_LOG_EXT;
+	pReq->Device_Id = pDevice->Id;
+
+	//pReq->Req_Flag;
+	pReq->Cmd_Initiator = pPort->Core_Extension;
+	pReq->Data_Transfer_Length = SATA_SCRATCH_BUFFER_SIZE;
+	pReq->Data_Buffer = pDevice->Scratch_Buffer;
+	pReq->Completion = (void(*)(MV_PVOID,PMV_Request))Core_InternalReqCallback;
+	MV_DASSERT( SATA_SCRATCH_BUFFER_SIZE>=sizeof(ATA_Identify_Data) );
+
+	/* Make SG table */
+	SGTable_Init(pSGTable, 0);
+	SGTable_Append(pSGTable,
+				pDevice->Scratch_Buffer_DMA.parts.low,
+				pDevice->Scratch_Buffer_DMA.parts.high,
+				pReq->Data_Transfer_Length
+				);
+	MV_DASSERT( pReq->Data_Transfer_Length%2==0 );
+
+
+	/* Send this internal request */
+	Core_ModuleSendRequest(pPort->Core_Extension, pReq);
+}
+
+static MV_VOID mvAta2HostString(IN MV_U16 *source,
+                                OUT MV_U16 *target,
+                                IN MV_U32 wordsCount
+                               )
+{
+    MV_U32 i;
+    for (i=0 ; i < wordsCount; i++)
+    {
+        target[i] = (source[i] >> 8) | ((source[i] & 0xff) << 8);
+        target[i] = MV_LE16_TO_CPU(target[i]);
+    }
+}
+
+void Device_ParseIdentifyData(
+	IN PDomain_Device pDevice,
+	IN PATA_Identify_Data pATAIdentify
+	)
+{
+	PDomain_Port pPort = pDevice->PPort;
+	MV_U8 i;
+	MV_U32 temp;
+
+	/* Get serial number, firmware revision and model number. */
+
+	MV_CopyMemory(pDevice->Serial_Number, pATAIdentify->Serial_Number, 20);
+	MV_CopyMemory(pDevice->Firmware_Revision, pATAIdentify->Firmware_Revision, 8);
+
+	MV_CopyMemory(pDevice->Model_Number, pATAIdentify->Model_Number, 40);
+	mvAta2HostString((MV_U16 *)pDevice->Serial_Number, (MV_U16 *)pDevice->Serial_Number, 10);
+	mvAta2HostString((MV_U16 *)pDevice->Firmware_Revision, (MV_U16 *)pDevice->Firmware_Revision, 4);
+
+	mvAta2HostString((MV_U16 *)pDevice->Model_Number, (MV_U16 *)pDevice->Model_Number, 20);
+
+	/* Capacity: 48 bit LBA, smart, write cache and NCQ */
+	pDevice->Capacity = 0;
+	pDevice->Setting = 0;
+	if ( pATAIdentify->Command_Set_Supported[1] & MV_BIT(10) )
+	{
+		MV_DPRINT(("Device: %d 48 bit supported.\n", pDevice->Id));
+
+		pDevice->Capacity |= DEVICE_CAPACITY_48BIT_SUPPORTED;
+	}
+	else
+	{
+		MV_DPRINT(("Device: %d 48 bit not supported.\n", pDevice->Id));
+	}
+
+	if ( pATAIdentify->Command_Set_Supported[0] & MV_BIT(0) )
+	{
+		pDevice->Capacity |= DEVICE_CAPACITY_SMART_SUPPORTED;
+		if ( pATAIdentify->Command_Set_Enabled[0] & MV_BIT(0) )
+		{
+			pDevice->Setting |= DEVICE_SETTING_SMART_ENABLED;
+		}
+		if ( pATAIdentify->Command_Set_Supported_Extension & MV_BIT(0) )
+		{
+			pDevice->Capacity |= DEVICE_CAPACITY_SMART_SELF_TEST_SUPPORTED;
+		}
+	}
+	if ( pATAIdentify->Command_Set_Supported[0] & MV_BIT(5) )
+	{
+		pDevice->Capacity |= DEVICE_CAPACITY_WRITECACHE_SUPPORTED;
+		if ( pATAIdentify->Command_Set_Enabled[0] & MV_BIT(5) )
+		{
+			pDevice->Setting |= DEVICE_SETTING_WRITECACHE_ENABLED;
+		}
+	}
+	if ( pATAIdentify->Command_Set_Supported[0] & MV_BIT(6) )
+	{
+		pDevice->Capacity |= DEVICE_CAPACITY_READ_LOOK_AHEAD_SUPPORTED;
+		if ( pATAIdentify->Command_Set_Enabled[0] & MV_BIT(6) )
+		{
+			pDevice->Setting |= DEVICE_SETTING_READ_LOOK_AHEAD;
+		}
+	}
+#ifdef SUPPORT_ATA_SECURITY_CMD
+	if ( pATAIdentify->Security_Status & MV_BIT(0) )
+	{
+		pDevice->Capacity |= DEVICE_CAPACITY_SECURITY_SUPPORTED;
+		if (pATAIdentify->Security_Status & MV_BIT(2) )
+		{
+			MV_DPRINT(("securiy: locked\n"));
+			pDevice->Setting |= DEVICE_SETTING_SECURITY_LOCKED;
+		}
+	}
+#endif
+	if ( pATAIdentify->SATA_Capabilities & MV_BIT(8) )
+	{
+
+		if (pDevice->Capacity & DEVICE_CAPACITY_48BIT_SUPPORTED)
+			pDevice->Capacity |= DEVICE_CAPACITY_NCQ_SUPPORTED;
+
+	}
+	if ( pATAIdentify->Command_Set_Supported_Extension & MV_BIT(5) )
+	{
+		if ( pATAIdentify->Command_Set_Default & MV_BIT(5) )
+			pDevice->Capacity |= DEVICE_CAPACITY_READLOGEXT_SUPPORTED;
+	}
+
+	temp = MV_REG_READ_DWORD( pPort->Mmio_Base, PORT_SCR_STAT );
+	if ( ((temp >> 4) & 0xF) == 1 )
+		pDevice->Capacity |= DEVICE_CAPACITY_RATE_1_5G;
+	else if ( ((temp >> 4) & 0xF) == 2 )
+		pDevice->Capacity |= DEVICE_CAPACITY_RATE_3G;
+
+	/* Disk size */
+	if ( pDevice->Capacity&DEVICE_CAPACITY_48BIT_SUPPORTED )
+	{
+		pDevice->Max_LBA.parts.low = *((MV_PU32)&pATAIdentify->Max_LBA[0]);
+		pDevice->Max_LBA.parts.high = *((MV_PU32)&pATAIdentify->Max_LBA[2]);
+	}else
+	{
+		pDevice->Max_LBA.parts.low = *((MV_PU32)&pATAIdentify->User_Addressable_Sectors[0]);
+		pDevice->Max_LBA.parts.high = 0;
+	}
+	/* For SATA drive, MAX LBA in identify data is actually disk size */
+	pDevice->Max_LBA = U64_SUBTRACT_U32(pDevice->Max_LBA, 1);
+
+	/* PIO, MDMA and UDMA mode */
+	if ( ( pATAIdentify->Fields_Valid&MV_BIT(1) )
+		&& ( pATAIdentify->PIO_Modes&0x0F ) )
+	{
+	if ( (MV_U8)pATAIdentify->PIO_Modes>=0x2 )
+			pDevice->PIO_Mode = 0x04;
+		else
+			pDevice->PIO_Mode = 0x03;
+	}
+    else
+	{
+	pDevice->PIO_Mode = 0x02;
+	}
+
+	pDevice->MDMA_Mode = 0xFF;
+	if ( pATAIdentify->Multiword_DMA_Modes & MV_BIT(2) )
+		pDevice->MDMA_Mode = 2;
+	else if ( pATAIdentify->Multiword_DMA_Modes & MV_BIT(1) )
+		pDevice->MDMA_Mode = 1;
+	else if ( pATAIdentify->Multiword_DMA_Modes & MV_BIT(0) )
+		pDevice->MDMA_Mode = 0;
+
+	pDevice->UDMA_Mode = 0xFF;
+    if ( pATAIdentify->Fields_Valid&MV_BIT(2) )
+	{
+		for ( i=0; i<7; i++ )
+		{
+			if ( pATAIdentify->UDMA_Modes & MV_BIT(i) )
+				pDevice->UDMA_Mode = i;
+		}
+	}
+
+	/* CRC identify buffer to get the U32 GUID. */
+	pDevice->WWN = MV_CRC((MV_PU8)pATAIdentify, sizeof(ATA_Identify_Data));
+
+}
+
+static void Device_IssueSetMDMAMode(
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PMV_Request pReq = pDevice->Internal_Req;
+    MV_U32 temp;
+	MV_U32 offset;
+	MV_LPVOID base;
+	MV_BOOLEAN memoryIO=MV_FALSE;
+	MV_U8 mode = pDevice->MDMA_Mode;
+
+	/* Only if the Device doesn't support UDMA, we'll use MDMA mode. */
+	MV_DASSERT( pDevice->UDMA_Mode==0xFF );
+	/* Is that possible that one device doesn't support either UDMA and MDMA? */
+	MV_DASSERT( (pDevice->MDMA_Mode<=2) );
+	MV_ZeroMvRequest(pReq);
+
+	/* Set controller timing register for PATA port before set the device MDMA mode. */
+	if ( pPort->Type==PORT_TYPE_PATA )
+	{
+		if ( pCore->Device_Id==DEVICE_ID_THORLITE_2S1P
+			|| pCore->Device_Id==DEVICE_ID_THORLITE_2S1P_WITH_FLASH
+			|| pCore->Device_Id==DEVICE_ID_THORLITE_0S1P )
+		{
+			if ( pCore->Revision_Id==0xA0 )
+			{
+				/* Thorlite A0 */
+				temp = MV_IO_READ_DWORD(pCore->Base_Address[4], 0);
+				temp &= 0xFFFF00FF;
+				temp |= 0x0000A800;
+				MV_IO_WRITE_DWORD(pCore->Base_Address[4], 0, temp);
+
+				if ( !pDevice->Is_Slave )
+					offset = 0x10;
+				else
+					offset = 0x14;
+				base = pCore->Base_Address[4];
+				memoryIO = MV_FALSE;
+			}
+			else
+			{
+				/* Thorlite B0 */
+				MV_DASSERT( (pCore->Revision_Id==0xB0)||(pCore->Revision_Id==0xB1)||(pCore->Revision_Id==0xB2) );
+				if ( !pDevice->Is_Slave )
+					offset = 0xA0;
+				else
+					offset = 0xA4;
+				base = pCore->Base_Address[5];
+				memoryIO = MV_TRUE;
+			}
+		}
+		else
+		{
+			MV_DASSERT( (pCore->Device_Id==DEVICE_ID_THOR_4S1P)||(pCore->Device_Id==DEVICE_ID_THOR_4S1P_NEW) );
+			if ( pCore->Revision_Id==0x00	/* A0 */
+				|| pCore->Revision_Id==0x01	/* A1 */
+				|| pCore->Revision_Id==0x10	/* B0 and C0 */	)
+			{
+				/* Thor A0-C0 */
+				if ( !pDevice->Is_Slave )
+					offset = 0x08;
+				else
+					offset = 0x0c;
+				base = pCore->Base_Address[4];
+				memoryIO = MV_FALSE;
+			}
+			else
+			{
+				/* Thor D0 = Thor New A0 */
+				MV_DASSERT( (pCore->Revision_Id==0xA0) || (pCore->Revision_Id==0xA1) ||
+							(pCore->Revision_Id==0xA2) );
+				if ( !pDevice->Is_Slave )
+					offset = 0xA0;
+				else
+					offset = 0xA4;
+				base = pCore->Base_Address[5];
+				memoryIO = MV_TRUE;
+			}
+		}
+
+		if ( !memoryIO )
+		{
+			temp = MV_IO_READ_DWORD(base, offset);
+			temp &= 0xFFFFFF3F;
+			temp |= ((MV_U32)mode)<<6;
+			temp |= 0x100;		/* Enable MDAM */
+			MV_IO_WRITE_DWORD(base, offset, temp);
+		}
+		else
+		{
+			temp = MV_REG_READ_DWORD(base, offset);
+			temp &= 0xFFFFFF3F;
+			temp |= ((MV_U32)mode)<<6;
+			temp |= 0x100;		/* Enable MDAM */
+			MV_REG_WRITE_DWORD(base, offset, temp);
+		}
+	}
+
+	//pDevice->MDMA_Mode = mode;
+	pDevice->Current_MDMA = mode;
+
+	/* Prepare set UDMA mode task */
+	pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+	pReq->Cdb[1] = CDB_CORE_MODULE;
+	pReq->Cdb[2] = CDB_CORE_SET_UDMA_MODE;
+	pReq->Cdb[3] = mode;
+	/* Means we are setting MDMA mode. I still use CDB_CORE_SET_UDMA_MODE because I don't want to change the state machine. */
+	pReq->Cdb[4] = MV_TRUE;
+	pReq->Device_Id = pDevice->Id;
+	//pReq->Req_Flag;
+	pReq->Cmd_Initiator = pPort->Core_Extension;
+	pReq->Data_Transfer_Length = 0;
+	pReq->Data_Buffer = NULL;
+	pReq->Completion = (void(*)(MV_PVOID,PMV_Request))Core_InternalReqCallback;
+
+
+	/* Send this internal request */
+	Core_ModuleSendRequest(pPort->Core_Extension, pReq);
+}
+
+static void Device_IssueSetUDMAMode(
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PMV_Request pReq = pDevice->Internal_Req;
+    MV_U32 temp;
+	MV_U32 offset;
+	MV_LPVOID base;
+	MV_BOOLEAN memoryIO=MV_FALSE;
+	MV_U8 mode = pDevice->UDMA_Mode;
+
+	if ( pDevice->UDMA_Mode==0xFF )
+	{
+		Device_IssueSetMDMAMode(pPort, pDevice);
+		return;
+	}
+
+	//GT 10/19/2006 11:07AM
+	//Check PATA port cable if 40_pin or 80_pin
+	if ( pPort->Type==PORT_TYPE_PATA )
+	{
+#ifdef PATA_CABLE_DETECTION
+		if (pPort->PATA_cable_type==MV_40_PIN_CABLE){
+			if ( mode>2 )
+				mode = 2;
+			pDevice->UDMA_Mode = mode;
+		}
+#else
+		temp = MV_IO_READ_DWORD(pCore->Base_Address[4], 0);
+		if( temp & MV_BIT(8) )	//40_pin cable
+		{
+			if ( mode>2 )
+				mode = 2;
+			pDevice->UDMA_Mode = mode;
+		}
+#endif
+	}
+
+	/* Hardware team required us to downgrade UDMA mode to zero. */
+	//if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+	//	mode = 0;
+	//if ( mode>=5 ) mode = 4; //???
+
+	MV_ZeroMvRequest(pReq);
+
+	//if ( (pCore->Device_Id!=DEVICE_ID_THOR_4S1P_NEW) && (pCore->Revision_Id!=0xB0) && (pCore->Revision_Id!=0xB1) && (pCore->Revision_Id!=0xB2) )
+	if ( (pCore->Device_Id!=DEVICE_ID_THOR_4S1P_NEW) && (pCore->Revision_Id<0xB0) )
+	{
+		/* Degrade ATAPI device UDMA mode always to 2. */
+		if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+		{
+			if ( mode>2 ) mode = 2;
+		}
+		else
+		{
+			/*
+			* Workaround:
+			* Thor lite A0 has problem with Hitesh(IBM) HDD under UDMA 5
+			* And it has problem with any HDD under UDMA 6
+			* So we degrade the HDD mode to 5 and ignore Hitesh HDD for now.
+			*/
+			if ( mode>5 ) mode = 5;
+		}
+	}
+
+	/*
+	 * Set controller timing register for PATA port before set the device UDMA mode.
+	 * Thorlite A0:	To enable timing programming, BAR 4 offset x0, write a800
+	 *				To set values, BAR 4 offset x10, x14
+	 * Thorlite B0:	BAR 5 offset xa0, xa4
+	 * Thor A0~C0:	BAR 4 offset x8, xc
+	 * Thor D0(=Thor New A0):BAR 5 offset xa0, xa4 ( Same as Thorlite B0 )
+	 */
+	if ( pPort->Type==PORT_TYPE_PATA )
+	{
+		if ( pCore->Device_Id==DEVICE_ID_THORLITE_2S1P
+			|| pCore->Device_Id==DEVICE_ID_THORLITE_2S1P_WITH_FLASH
+			|| pCore->Device_Id==DEVICE_ID_THORLITE_0S1P )
+		{
+			if ( pCore->Revision_Id==0xA0 )
+			{
+				/* Thorlite A0 */
+				temp = MV_IO_READ_DWORD(pCore->Base_Address[4], 0);
+				temp &= 0xFFFF00FF;
+				temp |= 0x0000A800;
+				MV_IO_WRITE_DWORD(pCore->Base_Address[4], 0, temp);
+
+				if ( !pDevice->Is_Slave )
+					offset = 0x10;
+				else
+					offset = 0x14;
+				base = pCore->Base_Address[4];
+				memoryIO = MV_FALSE;
+			}
+			else
+			{
+				/* Thorlite B0 */
+				MV_DASSERT( (pCore->Revision_Id==0xB0)||(pCore->Revision_Id==0xB1)||(pCore->Revision_Id==0xB2) );
+				if ( !pDevice->Is_Slave )
+					offset = 0xA0;
+				else
+					offset = 0xA4;
+				base = pCore->Base_Address[5];
+				memoryIO = MV_TRUE;
+			}
+		}
+		else
+		{
+			MV_DASSERT( (pCore->Device_Id==DEVICE_ID_THOR_4S1P)||(pCore->Device_Id==DEVICE_ID_THOR_4S1P_NEW) );
+			if ( pCore->Revision_Id==0x00	/* A0 */
+				|| pCore->Revision_Id==0x01	/* A1 */
+				|| pCore->Revision_Id==0x10	/* B0 and C0 */	)
+			{
+				/* Thor A0-C0 */
+				if ( !pDevice->Is_Slave )
+					offset = 0x08;
+				else
+					offset = 0x0c;
+				base = pCore->Base_Address[4];
+				memoryIO = MV_FALSE;
+			}
+			else
+			{
+				/* Thor D0 = Thor New A0 */
+				MV_DASSERT( (pCore->Revision_Id==0xA0) || (pCore->Revision_Id==0xA1) ||
+							(pCore->Revision_Id==0xA2) );
+				if ( !pDevice->Is_Slave )
+					offset = 0xA0;
+				else
+					offset = 0xA4;
+				base = pCore->Base_Address[5];
+				memoryIO = MV_TRUE;
+			}
+		}
+
+		if ( !memoryIO )
+		{
+			temp = MV_IO_READ_DWORD(base, offset);
+			temp &= 0xFFFFFFF8;
+			temp |= (MV_U32)mode;
+			MV_IO_WRITE_DWORD(base, offset, temp);
+		}
+		else
+		{
+			temp = MV_REG_READ_DWORD(base, offset);
+			temp &= 0xFFFFFFF8;
+			temp |= (MV_U32)mode;
+			MV_REG_WRITE_DWORD(base, offset, temp);
+		}
+	}
+
+	//pDevice->UDMA_Mode = mode;
+	pDevice->Current_UDMA = mode;
+
+	/* Prepare set UDMA mode task */
+	pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+	pReq->Cdb[1] = CDB_CORE_MODULE;
+	pReq->Cdb[2] = CDB_CORE_SET_UDMA_MODE;
+	pReq->Cdb[3] = mode;
+	/* Not setting MDMA but UDMA mode. */
+	pReq->Cdb[4] = MV_FALSE;
+	pReq->Device_Id = pDevice->Id;
+	//pReq->Req_Flag;
+	pReq->Cmd_Initiator = pPort->Core_Extension;
+	pReq->Data_Transfer_Length = 0;
+	pReq->Data_Buffer = NULL;
+	pReq->Completion = (void(*)(MV_PVOID,PMV_Request))Core_InternalReqCallback;
+
+
+	/* Send this internal request */
+	Core_ModuleSendRequest(pPort->Core_Extension, pReq);
+}
+
+static void Device_IssueSetPIOMode(
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PMV_Request pReq = pDevice->Internal_Req;
+    MV_U32 temp;
+	MV_U32 offset;
+	MV_LPVOID base;
+	MV_BOOLEAN memoryIO=MV_FALSE;
+	MV_U8 mode = pDevice->PIO_Mode;
+
+	MV_ZeroMvRequest(pReq);
+
+	/* Hardware team required us to downgrade PIO mode to zero. */
+	if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+		mode = 0;
+
+
+	/*
+	 * Set controller timing register for PATA port before set the device UDMA mode.
+	 * Thorlite A0:	To enable timing programming, BAR 4 offset x0, write a800
+	 *				To set values, BAR 4 offset x10, x14
+	 * Thorlite B0:	BAR 5 offset xa0, xa4
+	 * Thor A0~C0:	BAR 4 offset x8, xc
+	 * Thor D0:		BAR 5 offset xa0, xa4 ( Same as Thorlite B0 )
+	 */
+	if ( pPort->Type==PORT_TYPE_PATA )
+	{
+
+		if ( pCore->Device_Id==DEVICE_ID_THORLITE_2S1P
+			|| pCore->Device_Id==DEVICE_ID_THORLITE_2S1P_WITH_FLASH
+			|| pCore->Device_Id==DEVICE_ID_THORLITE_0S1P )
+		{
+			if ( pCore->Revision_Id==0xA0 )
+			{
+				/* Thorlite A0 */
+				temp = MV_IO_READ_DWORD(pCore->Base_Address[4], 0);
+				temp &= 0xFFFF00FF;
+				temp |= 0x0000A800;
+				MV_IO_WRITE_DWORD(pCore->Base_Address[4], 0, temp);
+
+				if ( !pDevice->Is_Slave )
+					offset = 0x10;
+				else
+					offset = 0x14;
+				base = pCore->Base_Address[4];
+				memoryIO = MV_FALSE;
+			}
+			else
+			{
+				/* Thorlite B0 */
+				MV_DASSERT( (pCore->Revision_Id==0xB0)||(pCore->Revision_Id==0xB1)||(pCore->Revision_Id==0xB2) );
+				if ( !pDevice->Is_Slave )
+					offset = 0xA0;
+				else
+					offset = 0xA4;
+				base = pCore->Base_Address[5];
+				memoryIO = MV_TRUE;
+			}
+		}
+		else
+		{
+
+			MV_DASSERT( (pCore->Device_Id==DEVICE_ID_THOR_4S1P)||(pCore->Device_Id==DEVICE_ID_THOR_4S1P_NEW) );
+			if ( pCore->Revision_Id==0x00	/* A0 */
+				|| pCore->Revision_Id==0x01	/* A1 */
+				|| pCore->Revision_Id==0x10	/* B0 and C0 */	)
+			{
+				/* Thor A0-C0 */
+				if ( !pDevice->Is_Slave )
+					offset = 0x08;
+				else
+					offset = 0x0c;
+				base = pCore->Base_Address[4];
+				memoryIO = MV_FALSE;
+			}
+			else
+			{
+				/* Thor D0 = Thor New A0 */
+				MV_DASSERT( (pCore->Revision_Id==0xA0) || (pCore->Revision_Id==0xA1) ||
+							(pCore->Revision_Id==0xA2) );
+				if ( !pDevice->Is_Slave )
+					offset = 0xA0;
+				else
+					offset = 0xA4;
+				base = pCore->Base_Address[5];
+				memoryIO = MV_TRUE;
+			}
+		}
+
+		if ( !memoryIO )
+		{
+			temp = MV_IO_READ_DWORD(base, offset);
+			temp &= 0xFFFFFFC7;
+			temp |= ((MV_U32)mode<<3);
+			MV_IO_WRITE_DWORD(base, offset, temp);
+		}
+		else
+		{
+			temp = MV_REG_READ_DWORD(base, offset);
+			temp &= 0xFFFFFFC7;
+			temp |= ((MV_U32)mode<<3);
+			MV_REG_WRITE_DWORD(base, offset, temp);
+		}
+	}
+
+	//pDevice->PIO_Mode = mode;
+	pDevice->Current_PIO = mode;
+
+	/* Prepare set PIO mode task */
+	pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+	pReq->Cdb[1] = CDB_CORE_MODULE;
+	pReq->Cdb[2] = CDB_CORE_SET_PIO_MODE;
+	pReq->Cdb[3] = mode;
+	pReq->Device_Id = pDevice->Id;
+	//pReq->Req_Flag;
+	pReq->Cmd_Initiator = pPort->Core_Extension;
+	pReq->Data_Transfer_Length = 0;
+	pReq->Data_Buffer = NULL;
+	pReq->Completion = (void(*)(MV_PVOID,PMV_Request))Core_InternalReqCallback;
+
+	/* Send this internal request */
+	Core_ModuleSendRequest(pPort->Core_Extension, pReq);
+
+}
+
+static void Device_EnableWriteCache(
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PMV_Request pReq = pDevice->Internal_Req;
+
+	MV_ZeroMvRequest(pReq);
+
+	/* Prepare enable write cache command */
+	pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+	pReq->Cdb[1] = CDB_CORE_MODULE;
+	pReq->Cdb[2] = CDB_CORE_ENABLE_WRITE_CACHE;
+	pReq->Device_Id = pDevice->Id;
+	//pReq->Req_Flag;
+	pReq->Cmd_Initiator = pPort->Core_Extension;
+	pReq->Data_Transfer_Length = 0;
+	pReq->Data_Buffer = NULL;
+	pReq->Completion = (void(*)(MV_PVOID,PMV_Request))Core_InternalReqCallback;
+
+	/* skip if this is ATAPI device */
+	if( pDevice->Device_Type & DEVICE_TYPE_ATAPI )
+	{
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		pReq->Completion(pCore, pReq);
+	}
+	else
+	{
+
+		/* Send this internal request */
+		Core_ModuleSendRequest(pPort->Core_Extension, pReq);
+	}
+}
+
+static void Device_EnableReadAhead(
+	IN PDomain_Port pPort,
+	IN PDomain_Device pDevice
+	)
+{
+	PCore_Driver_Extension pCore = pPort->Core_Extension;
+	PMV_Request pReq = pDevice->Internal_Req;
+
+	MV_ZeroMvRequest(pReq);
+
+	/* Prepare enable read ahead command */
+	pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+	pReq->Cdb[1] = CDB_CORE_MODULE;
+	pReq->Cdb[2] = CDB_CORE_ENABLE_READ_AHEAD;
+	pReq->Device_Id = pDevice->Id;
+	//pReq->Req_Flag;
+	pReq->Cmd_Initiator = pPort->Core_Extension;
+	pReq->Data_Transfer_Length = 0;
+	pReq->Data_Buffer = NULL;
+	pReq->Completion = (void(*)(MV_PVOID,PMV_Request))Core_InternalReqCallback;
+
+	/* skip if this is ATAPI device */
+	if( pDevice->Device_Type & DEVICE_TYPE_ATAPI )
+	{
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		pReq->Completion(pCore, pReq);
+	}
+	else
+	{
+		/* Send this internal request */
+		Core_ModuleSendRequest(pPort->Core_Extension, pReq);
+	}
+}
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_init.h
@@ -0,0 +1,135 @@
+#if !defined(CORE_INIT_H)
+#define CORE_INIT_H
+
+#include "core_inter.h"
+
+typedef enum mvAdapterState
+{
+    ADAPTER_INITIALIZING,
+    ADAPTER_READY,
+    ADAPTER_FATAL_ERROR
+} MV_ADAPTER_STATE;
+
+typedef enum mvChannelState
+{
+    CHANNEL_NOT_CONNECTED,
+    CHANNEL_CONNECTED,
+    CHANNEL_IN_SRST,
+    CHANNEL_PM_STAGGERED_SPIN_UP,
+    CHANNEL_PM_SRST_DEVICE,
+    CHANNEL_READY,
+    CHANNEL_PM_HOT_PLUG,
+} MV_CHANNEL_STATE;
+
+MV_BOOLEAN mvAdapterStateMachine(
+	MV_PVOID This,
+	MV_PVOID temp
+	);
+
+void SATA_PortReset(
+	PDomain_Port pPort,
+	MV_BOOLEAN hardReset
+	);
+
+void PATA_PortReset(
+	PDomain_Port pPort,
+	MV_BOOLEAN hardReset
+	);
+void Core_InternalReqCallback(
+	 IN PCore_Driver_Extension pCore,
+	 IN PMV_Request pReq
+	 );
+
+MV_BOOLEAN SATA_DoSoftReset(PDomain_Port pPort, MV_U8 PMPort);
+
+#define SATA_PortDeviceDetected(port)	\
+	 ( MV_REG_READ_DWORD(port->Mmio_Base, PORT_SCR_STAT) & 0x01 )
+
+#define SATA_PortDeviceReady(port)		\
+	(								\
+		( ( (MV_REG_READ_DWORD(port->Mmio_Base, PORT_SCR_STAT) & 0x0F00 ) >> 8) != PORT_SSTATUS_IPM_NO_DEVICE )		\
+	)
+
+#define FIS_REG_H2D_SIZE_IN_DWORD	5
+
+/* PM related - move elsewhere? */
+#define MV_ATA_COMMAND_PM_READ_REG              0xe4
+#define MV_ATA_COMMAND_PM_WRITE_REG             0xe8
+
+#define MV_SATA_GSCR_ID_REG_NUM                 0
+#define MV_SATA_GSCR_REVISION_REG_NUM           1
+#define MV_SATA_GSCR_INFO_REG_NUM               2
+#define MV_SATA_GSCR_ERROR_REG_NUM              32
+#define MV_SATA_GSCR_ERROR_ENABLE_REG_NUM       33
+#define MV_SATA_GSCR_FEATURES_REG_NUM           64
+#define MV_SATA_GSCR_FEATURES_ENABLE_REG_NUM    96
+
+#define MV_SATA_PSCR_SSTATUS_REG_NUM            0
+#define MV_SATA_PSCR_SERROR_REG_NUM             1
+#define MV_SATA_PSCR_SCONTROL_REG_NUM           2
+#define MV_SATA_PSCR_SACTIVE_REG_NUM            3
+
+#define MV_Read_Reg  1
+#define MV_Write_Reg 0
+
+void mvPMDevReWrReg(
+	PDomain_Port pPort,
+	MV_U8 read,
+	MV_U8 PMreg,
+	MV_U32 regVal,
+	MV_U8 PMport,
+	MV_BOOLEAN control
+	);
+
+void SATA_InitPM (
+    PDomain_Port pPort
+	);
+
+void SATA_InitPMPort (
+	PDomain_Port pPort,
+	MV_U8 portNum
+	);
+
+MV_BOOLEAN SATA_SoftResetDevice(
+	PDomain_Port pPort,
+	MV_U8 portNum
+	);
+
+MV_BOOLEAN SATA_PortSoftReset(
+	PCore_Driver_Extension pCore,
+	PDomain_Port pPort
+	);
+
+void SATA_PortReportNoDevice (
+    PCore_Driver_Extension pCore,
+	PDomain_Port pPort
+	);
+
+PMV_Request GetInternalReqFromPool(
+	PCore_Driver_Extension pCore
+	);
+
+void ReleaseInternalReqToPool(
+	PCore_Driver_Extension pCore,
+	PMV_Request pReq
+	);
+
+#define mvDisableIntr(portMmio, old_stat) do{ \
+		old_stat = MV_REG_READ_DWORD(portMmio, PORT_IRQ_MASK); \
+		MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_MASK, 0);\
+		}while(0)
+
+#define mvEnableIntr(portMmio, old_stat)	MV_REG_WRITE_DWORD(portMmio, PORT_IRQ_MASK, old_stat)
+
+#define mvDisableGlobalIntr(mmio, old_stat) do{ \
+		old_stat = MV_REG_READ_DWORD(mmio, HOST_CTL); \
+		MV_REG_WRITE_DWORD(mmio, HOST_CTL, old_stat & (~HOST_IRQ_EN));\
+		}while(0)
+
+#define mvEnableGlobalIntr(mmio, old_stat)	MV_REG_WRITE_DWORD(mmio, HOST_CTL, old_stat)
+
+#define mvEnableGlobalIntr_resume(mmio)	MV_REG_WRITE_DWORD(mmio, HOST_CTL, MV_REG_READ_DWORD(mmio, HOST_CTL)|HOST_IRQ_EN)
+
+#define CORE_MAX_RESET_COUNT		0xffff
+#define CORE_MAX_PATA_RESET_COUNT	0xffff
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_inter.h
@@ -0,0 +1,77 @@
+#if !defined(CORE_MAIN_H)
+#define CORE_MAIN_H
+
+#include "core_thor.h"
+
+#ifdef SUPPORT_CONSOLIDATE
+#include "consolid.h"
+#endif
+
+#ifdef _OS_WINDOWS
+#define CPU_TO_LE_16(x) x
+#define CPU_TO_LE_32(x) x
+#endif /* _OS_WINDOWS  */
+
+struct _Domain_Port;
+typedef struct _Domain_Port Domain_Port, *PDomain_Port;
+
+struct _Domain_Device;
+typedef struct _Domain_Device Domain_Device, *PDomain_Device;
+
+#define CORE_STATE_IDLE			0
+#define CORE_STATE_STARTED		1
+
+/* Flag definition for Fast Boot Skip */
+#define FLAG_SKIP_PATA_PORT		MV_BIT(0)
+#define FLAG_SKIP_PATA_DEVICE	MV_BIT(1)
+#define FLAG_SKIP_PM			MV_BIT(2)
+
+typedef struct _Core_Driver_Extension
+{
+#ifdef __MM_SE__
+/* Must be first */
+	struct mv_mod_desc *desc;
+#endif /* __MM_SE__ */
+	MV_LPVOID	Mmio_Base;						/* Memory IO base address */
+	MV_U16		Vendor_Id;
+	MV_U16		Device_Id;
+	MV_U8		State;
+	MV_U8		Revision_Id;
+	MV_U8		VS_Reg_Saved;
+	MV_U8		Flag_Fastboot_Skip;
+
+	MV_U32		Capacity;
+	MV_U32		Port_Map;
+	MV_U8		Port_Num;						/* How much ports we have? */
+	MV_U8		SATA_Port_Num;
+	MV_U8		PATA_Port_Num;
+	MV_U8		Adapter_State;					/* Adatper state */
+	MV_U8		Is_Dump;						/* Is during dump */
+	MV_U8		Need_Reset;						/* Need_Reset == 1 means controller need reset. Lily 3/7/2006*/
+	MV_U8		Resetting;
+
+	MV_U8		Total_Device_Count;
+#ifndef  SUPPORT_TASKLET
+	MV_U8		Reserved1;
+	MV_U8		Reserved2[3];
+#else
+	MV_U32		Saved_ISR_Status;
+#endif
+
+	MV_LPVOID	Base_Address[MAX_BASE_ADDRESS];	/* Base Address */
+	Domain_Port Ports[MAX_PORT_NUMBER];			/* Domain Ports */
+
+	List_Head	Waiting_List; 					/* Waiting Request Queue */
+	List_Head	Internal_Req_List;				/* Internal Request Queue */
+
+#ifdef SUPPORT_CONSOLIDATE
+	PConsolidate_Extension	pConsolid_Extent;
+	PConsolidate_Device		pConsolid_Device;
+#endif
+#ifdef COMMAND_ISSUE_WORKROUND
+	MV_U32 	error_handle_state;
+	MV_U8 resetting_command;
+#endif
+}Core_Driver_Extension, *PCore_Driver_Extension;
+
+#endif /* CORE_MAIN_H */
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_sat.c
@@ -0,0 +1,836 @@
+#include "mv_include.h"
+#include "core_sat.h"
+#include "core_inter.h"
+#include "core_thor.h"
+#include "core_exp.h"
+#include "core_sata.h"
+#include "core_ata.h"
+#include "core_init.h"
+
+#ifdef SUPPORT_ATA_SMART
+
+/***************************************************************************
+* SCSI_ATA_CheckCondition
+* Purpose: Send Check Condition when request is illegal
+*
+***************************************************************************/
+void SCSI_ATA_CheckCondition(
+	IN PMV_Request pReq,
+	IN MV_U8 senseKey,
+	IN MV_U8 senseCode
+	)
+{
+	 pReq->Scsi_Status = SCSI_STATUS_CHECK_CONDITION;
+	 if ( pReq->Sense_Info_Buffer ) {
+		((MV_PU8)pReq->Sense_Info_Buffer)[2] = senseKey;
+		((MV_PU8)pReq->Sense_Info_Buffer)[7] = 0;		// additional sense length
+		((MV_PU8)pReq->Sense_Info_Buffer)[12] = senseCode; //additional sense code
+	 }
+}
+
+#define MAX_MODE_PAGE_LENGTH	28
+MV_U32 Core_get_mode_page_caching(MV_PU8 pBuf, PDomain_Device pDevice)
+{
+	pBuf[0] = 0x08;		/* Page Code, PS = 0; */
+	pBuf[1] = 0x12;		/* Page Length */
+	/* set the WCE and RCD bit based on device identification data */
+	if (pDevice->Setting & DEVICE_SETTING_WRITECACHE_ENABLED)
+		pBuf[2] |= MV_BIT(2);
+	pBuf[3] = 0;	/* Demand read/write retention priority */
+	pBuf[4] = 0xff;	/* Disable pre-fetch trnasfer length (4,5) */
+	pBuf[5] = 0xff;	/* all anticipatory pre-fetching is disabled */
+	pBuf[6] = 0;	/* Minimum pre-fetch (6,7) */
+	pBuf[7] = 0;
+	pBuf[8] = 0;	/* Maximum pre-fetch (8,9) */
+	pBuf[9] = 0x01;
+	pBuf[10] = 0;	/* Maximum pre-fetch ceiling (10,11) */
+	pBuf[11] = 0x01;
+//	pBuf[12] |= MV_BIT(5);	/* How do I know if Read Ahead is enabled or disabled???  */
+	pBuf[12] = 0x00;
+	pBuf[13] = 0x01;	/* Number of cache segments */
+	pBuf[14] = 0xff;	/* Cache segment size (14, 15) */
+	pBuf[15] = 0xff;
+	return 0x14;	/* Total page length in byte */
+}
+
+MV_U32 Core_Fill_ProSpecPortModePage(MV_PU8 pBuf)
+{
+	static const MV_U8 pro_spec_mode_page[] = {
+		0x19,/*Protocol Specific Port mode page*/
+		0x01,	/*Page Length*/
+		0x08, /*protocol identify*/
+		0,
+	};
+	MV_CopyMemory(pBuf, pro_spec_mode_page, sizeof(pro_spec_mode_page));
+	return sizeof(pro_spec_mode_page);	/* Total page length in byte */
+}
+
+static const MV_U8 info_except_ctrl_mode_page[] = {
+	0x1c,	/* Page Code, PS = 0; */
+	0x0a,  /* Page Length */
+	0xb7, /* Report Error  */
+	0x04, /*set MRIE*/
+	0, 0, 0, 0,
+	0, 0, 0, 0
+};
+
+MV_U32 Core_Fill_InfoExcepCtrlModePage(MV_PU8 pBuf, PDomain_Device pDevice,PMV_Request pReq)
+{
+	MV_CopyMemory(pBuf, info_except_ctrl_mode_page, sizeof(info_except_ctrl_mode_page));
+
+	// Setting Extended Self-test Completion time
+	if(!(pDevice->Capacity & DEVICE_CAPACITY_SMART_SUPPORTED))
+	{
+		SCSI_ATA_CheckCondition(pReq, SCSI_SK_ILLEGAL_REQUEST,
+				SCSI_ASC_INVALID_FEILD_IN_CDB);
+	}
+	return sizeof( info_except_ctrl_mode_page);	/* Total page length in byte */
+}
+
+
+
+MV_U32  Core_Fill_CtrlModePage(MV_PU8 pBuf, PDomain_Device pDevice)
+{
+	static const MV_U8 ctrl_mode_page[] = {
+		0x0a,
+		10,
+		2,	/* DSENSE=0, GLTSD=1 */
+		0,	/* [QAM+QERR may be 1, see 05-359r1] */
+		0, 0, 0, 0, 0, 0,
+		0x02, 0x4b	/* extended self test time, see 05-359r1 */
+	};
+	MV_CopyMemory(pBuf, ctrl_mode_page, sizeof(ctrl_mode_page));
+
+	// Setting Extended Self-test Completion time
+	if(pDevice->Capacity & DEVICE_CAPACITY_SMART_SELF_TEST_SUPPORTED)
+	{
+		pBuf[10] = 0;
+		pBuf[11] = 30;
+	} else {
+		pBuf[10] = 0;
+		pBuf[11] = 0;
+	}
+	return sizeof(ctrl_mode_page);
+}
+
+MV_U32  Core_Fill_RWErrorRecoveryModePage(MV_PU8 pBuf )
+{
+	static const MV_U8 rw_recovery_mdpage[] = {
+		0x01,
+		10,
+		(1 << 7),	/* AWRE */
+		0,		/* read retry count */
+		0, 0, 0, 0,
+		0,		/* write retry count */
+		0, 0, 0
+	};
+	MV_CopyMemory(pBuf, rw_recovery_mdpage, sizeof(rw_recovery_mdpage));
+	return sizeof(rw_recovery_mdpage);
+}
+
+MV_U32 Core_Fill_ErrorCounterLogPage(MV_PU8 pBuf)
+{
+	static const MV_U8 error_counter_log_pg[] = {
+		0x03,	/* DU =0, DS =0, TSD =0, ETC=0, TMC=00, FL=11 */
+		0x02, 	/* Parameter length */
+		0x00 ,0x00
+	};
+
+	pBuf[0] = 0x00;	/* Parameter Code*/
+	pBuf[1] = 0x06;/*00,01,02,03,04,05,06 Total uncorrected errors ?*/
+       MV_CopyMemory(pBuf+2, error_counter_log_pg, sizeof(error_counter_log_pg));
+       return sizeof(error_counter_log_pg)+2;
+}
+
+MV_U32 Core_Fill_SelfTestLogPage(MV_PU8 pBuf, MV_U8 pageNum)
+{
+	static const MV_U8 self_test_results_log_pg[] = {
+		0x03,	/* DU =0, DS =0, TSD =0, ETC=0, TMC=00, FL=11 */
+		0x10, 	/* Parameter length */
+		0x81,   /*SELF-TEST CODE=100,RES=0,SELF-TEST RESULTS=0001*/
+		0x08, /*self test number*/
+		0x0, 0x0,0x0,0x0,
+		0x0, 0x0,0x0,0x0
+	};
+
+	pBuf[0] = 0;	/* Parameter Code */
+	pBuf[1] = pageNum+1;
+       MV_CopyMemory(pBuf+2, self_test_results_log_pg, sizeof(self_test_results_log_pg));
+       return sizeof(self_test_results_log_pg)+2;
+}
+
+ MV_U32 Core_Fill_TempLogPage(MV_PU8 pBuf)
+{
+	static const MV_U8  temp_log_page[] = {
+		0x0, 0x0,
+		0x3, 0x2, 0x0,
+		0xFF,/*TEMPERATURE*/
+	       0x0, 0x1, 0x3, 0x2, 0x0,
+	       0xFF /*REFERENCE TEMPERATURE*/
+	};
+        MV_CopyMemory(pBuf, temp_log_page, sizeof(temp_log_page));
+        return sizeof(temp_log_page);
+}
+
+ MV_U32 Core_Fill_InfoExceptLogPage(MV_PU8 pBuf)
+{
+	static const MV_U8  info_except_log_page[] = {
+		0x0, 0x0, /*parameter code*/
+		0x3, /*DU=0 OBS=0 TSD=0 ETC=0 TMC=00 FL=11h*/
+		0x3,/*parampter length*/
+		0x0, 0x0,
+		38, /*MOST RECENT TEMPERATURE READING*/
+	};
+
+       MV_CopyMemory(pBuf, info_except_log_page, sizeof(info_except_log_page));
+	if (info_except_ctrl_mode_page[2] & 0x4) {	/* TEST bit set */
+		pBuf[4] = SCSI_ASC_FAILURE_PREDICTION_THRESHOLD_EXCEEDED;
+		pBuf[5] = 0xff;
+	}
+        return sizeof(info_except_log_page);
+}
+
+ MV_U32 Core_Fill_StartStopCycleCounterLogPage(MV_PU8 pBuf)
+{
+	static const MV_U8  start_stop_cycle_counter_log_page[] = {
+		0x00, 0x01,0x15,0x06,00,00,20,8,00,12,
+		0x00, 0x02,0x16,0x06,00,00,20,8,00,12,
+		0x00,0x03,0x17,0x04,00,00,00,10,
+	       0x00, 0x04,0x18,0x04,00,00,00,02
+	};
+        MV_CopyMemory(pBuf, start_stop_cycle_counter_log_page, sizeof(start_stop_cycle_counter_log_page));
+        return sizeof(start_stop_cycle_counter_log_page);
+}
+
+
+void  mvScsiLogSenseTranslation(PCore_Driver_Extension pCore,  PMV_Request pReq)
+{
+	MV_U8 pageCode,subpcode,ppc, sp, pcontrol;
+	MV_U32 pageLen = 0, tmpLen = 0,i,n;
+
+	unsigned char ptmpBuf[512];
+	MV_U8 *buf = pReq->Data_Buffer;
+
+	MV_ZeroMemory(buf, pReq->Data_Transfer_Length);
+	MV_ZeroMemory(ptmpBuf, sizeof(ptmpBuf));
+
+	ppc = pReq->Cdb[1] & 0x2;
+	sp = pReq->Cdb[1] & 0x1;
+	if (ppc || sp) {
+		pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+		return ;
+	}
+	pcontrol = (pReq->Cdb[2] & 0xc0) >> 6;
+	pageCode = pReq->Cdb[2] & 0x3f;
+	subpcode = pReq->Cdb[3] & 0xff;
+
+	ptmpBuf[0] = pageCode;
+	if (0 == subpcode) {
+		switch (pageCode) {
+		case SUPPORTED_LPAGES:/* Supported log pages log page */
+			n = 4;
+			ptmpBuf[0] = SUPPORTED_LPAGES; /* Page Code */
+			ptmpBuf[1] = 0x00; /*subpage Code */
+			ptmpBuf[2] = 0x00;/*length MSB*/
+			ptmpBuf[n++] = SUPPORTED_LPAGES;		/* this page */
+			ptmpBuf[n++] = WRITE_ERROR_COUNTER_LPAGE;
+			ptmpBuf[n++] = READ_ERROR_COUNTER_LPAGE;
+			ptmpBuf[n++] = READ_REVERSE_ERROR_COUNTER_LPAGE;
+			ptmpBuf[n++] = VERIFY_ERROR_COUNTER_LPAGE;
+			ptmpBuf[n++] = SELFTEST_RESULTS_LPAGE;
+			ptmpBuf[n++] = TEMPERATURE_LPAGE;
+
+			ptmpBuf[n++] = STARTSTOP_CYCLE_COUNTER_LPAGE;
+		//	ptmpBuf[n++] = SEAGATE_CACHE_LPAGE;
+		//	ptmpBuf[n++] = SEAGATE_FACTORY_LPAGE;
+
+			ptmpBuf[n++] = IE_LPAGE;	/* Informational exceptions */
+			ptmpBuf[3] = n - 4; /*length LSB*/
+			pageLen = ptmpBuf[3] ;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+4));
+			MV_CopyMemory(buf, ptmpBuf, tmpLen);
+			pReq->Data_Transfer_Length = tmpLen;
+			pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+			break;
+		case WRITE_ERROR_COUNTER_LPAGE:
+		case READ_ERROR_COUNTER_LPAGE:
+		case READ_REVERSE_ERROR_COUNTER_LPAGE:
+		case VERIFY_ERROR_COUNTER_LPAGE:
+			if (pageCode == WRITE_ERROR_COUNTER_LPAGE)
+				ptmpBuf[0] = WRITE_ERROR_COUNTER_LPAGE; // Page Code
+			if (pageCode == READ_ERROR_COUNTER_LPAGE)
+				ptmpBuf[0] = READ_ERROR_COUNTER_LPAGE; // Page Code
+			if (pageCode == READ_REVERSE_ERROR_COUNTER_LPAGE)
+				ptmpBuf[0] = READ_REVERSE_ERROR_COUNTER_LPAGE; // Page Code
+			if (pageCode == VERIFY_ERROR_COUNTER_LPAGE)
+			ptmpBuf[0] = VERIFY_ERROR_COUNTER_LPAGE; // Page Code
+
+			ptmpBuf[1] = 0x00; // subPage Code
+			ptmpBuf[2] = 0x00;
+
+			// Error Counter  log parameter
+			ptmpBuf[3] = Core_Fill_ErrorCounterLogPage(ptmpBuf + 4);
+			pageLen = ptmpBuf[3] ;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, pageLen);
+
+			MV_CopyMemory(buf, ptmpBuf, tmpLen);
+			pReq->Data_Transfer_Length = tmpLen;
+			pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+			break;
+		case SELFTEST_RESULTS_LPAGE:
+			ptmpBuf[0] = SELFTEST_RESULTS_LPAGE; // Page Code
+			ptmpBuf[1] = 0x00; // subPage Code
+			ptmpBuf[2] = 0x1;
+			ptmpBuf[3] = 0x90;
+
+			// Self-test Results log parameter
+			for(i=0; i<20; i++)
+			{
+				pageLen = Core_Fill_SelfTestLogPage(&ptmpBuf[4+(i*20)], i);
+			}
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen*20+4));
+
+			MV_CopyMemory(buf, ptmpBuf, tmpLen);
+			pReq->Data_Transfer_Length = tmpLen;
+			pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+			break;
+		case TEMPERATURE_LPAGE:	/* Temperature log page */
+			ptmpBuf[0] = TEMPERATURE_LPAGE; /* Page Code*/
+			ptmpBuf[1] = 0x00; /* subPage Code*/
+			ptmpBuf[2] = 0x00;
+			ptmpBuf[3] = Core_Fill_TempLogPage(ptmpBuf + 4);
+			pageLen = ptmpBuf[3] ;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, pageLen);
+
+			MV_CopyMemory(buf, ptmpBuf, tmpLen);
+			pReq->Data_Transfer_Length = tmpLen;
+			pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+			break;
+		case STARTSTOP_CYCLE_COUNTER_LPAGE:	/* Informational exceptions log page */
+			ptmpBuf[0] = STARTSTOP_CYCLE_COUNTER_LPAGE; /* Page Code*/
+			ptmpBuf[1] = 0x00; /* subPage Code*/
+			ptmpBuf[2] = 0x00;
+			ptmpBuf[3] = Core_Fill_StartStopCycleCounterLogPage(ptmpBuf + 4);
+
+			pageLen = ptmpBuf[3] ;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, pageLen);
+
+			MV_CopyMemory(buf, ptmpBuf, tmpLen);
+			pReq->Data_Transfer_Length = tmpLen;
+			pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+			break;
+		case IE_LPAGE:	/* Informational exceptions log page */
+			ptmpBuf[0] = IE_LPAGE; /* Page Code*/
+			ptmpBuf[1] = 0x00; /* subPage Code*/
+			ptmpBuf[2] = 0x00;
+			ptmpBuf[3] = Core_Fill_InfoExceptLogPage(ptmpBuf + 4);
+
+			pageLen = ptmpBuf[3] ;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, pageLen);
+
+			MV_CopyMemory(buf, ptmpBuf, tmpLen);
+			pReq->Data_Transfer_Length = tmpLen;
+			pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+			break;
+		default:
+		pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+		break;
+
+		}
+	} else {
+		pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+		return ;
+	}
+}
+static MV_U8 mvScsiModeSelectWceGet(PCore_Driver_Extension pCore, PMV_Request pReq, MV_U32 *WCEisEnabled)
+{
+    struct  scsi_cmnd *scmd=NULL;
+    struct  scatterlist *sg=NULL;
+    MV_U8  *req_buf=NULL;
+    MV_U32  length= pReq->Data_Transfer_Length;
+    MV_U8   offset= 0, cachePageOffset= 0;
+    MV_U8   rc= 1;
+
+    /* check for parameter list length error */
+    if (length < 4)
+        return 1;
+
+    if(NULL == WCEisEnabled)
+        return 1;
+
+    *WCEisEnabled= 0;
+
+    scmd=(struct scsi_cmnd *)pReq->Org_Req;
+    if(!scmd)
+        return 1;
+
+    if(!mv_use_sg(scmd))
+        return 1;
+
+    sg = (struct scatterlist *)mv_rq_bf(scmd);
+    if(NULL == sg)
+        return 1;
+
+    req_buf=(char *)(map_sg_page(sg)+sg->offset);
+    if(NULL == req_buf)
+        return 1;
+
+    /* check for invalid field in parameter list */
+    if (req_buf[0] || (req_buf[1] != 0) || req_buf[2])
+    {
+        MV_DPRINT(("Mode Select Error: invalid field in parameter \n"));
+        goto cleanup;
+    }
+
+    if (req_buf[3])
+    {
+        /* check for invalid field in parameter list */
+        if (req_buf[3] != 8)
+        {
+            MV_DPRINT(("Mode Select Error: wrong size for mode parameter"
+                     " block descriptor, BLOCK DESCRIPTOR LENGTH %d\n.",req_buf[3]));
+            goto cleanup;
+        }
+
+        /* check for parameter list length error */
+        if (length < 12)
+        {
+            goto cleanup;
+        }
+
+        /* check for invalid field in parameter list */
+        if (req_buf[4] || req_buf[5] || req_buf[6] || req_buf[7] || req_buf[8] || req_buf[9] ||(req_buf[10] != 0x2) || req_buf[11])
+        {
+            MV_DPRINT(("Mode Select Error: invalid field in parameter block descriptor list.\n"));
+            goto cleanup;
+        }
+    }
+    /* skip the mode parameter block descriptor */
+    offset = 4 + req_buf[3];
+
+    /* check for available mode pages */
+    if (length == offset)
+    {
+        MV_DPRINT(("Mode Select: no mode pages available\n"));
+        goto cleanup;
+    }
+
+    /* normalize to SELECT_10 offset */
+    if (pReq->Cdb[0]==SCSI_CMD_MODE_SELECT_10)
+        offset+= 4;
+
+    while ((offset + 2) < length)
+    {
+        switch (req_buf[offset] & 0x3f)
+        {
+            case 0x8:
+                if (req_buf[offset + 1] != 0x12)
+                {
+                    MV_DPRINT(("Mode Select Error: bad length in caching mode page %d\n.",
+                             req_buf[offset + 1]));
+                    goto cleanup;
+                }
+                cachePageOffset = offset;
+                offset += req_buf[offset + 1] + 2;
+            break;
+            case 0xa:
+                if ((req_buf[offset] != 0xa) || (req_buf[offset+1] != 0xa))
+                {
+                    MV_DPRINT(("Mode Select Error: invalid field in"
+                             " mode control page, list[%x] %x, list[%x] %x\n",
+                             offset, req_buf[offset], offset + 1, req_buf[offset+1]));
+                    goto cleanup;
+                }
+
+                if (req_buf[offset + 3] != MV_BIT(4))
+                {
+                    MV_DPRINT(("Mode Select Error: invalid field in mode control page, list[%x] %x\n",
+                             offset + 3, req_buf[offset + 3]));
+                    goto cleanup;
+                }
+
+                if (req_buf[offset + 2] || req_buf[offset + 4]  || req_buf[offset + 5]  ||
+                    req_buf[offset + 6] || req_buf[offset + 7]  ||  req_buf[offset + 8] ||
+                    req_buf[offset + 9] || req_buf[offset + 10] || req_buf[offset + 11])
+                {
+                    MV_DPRINT(("Mode Select Error: invalid field in"
+                             " mode control page, line %d\n", __LINE__));
+                    goto cleanup;
+                }
+                offset += req_buf[offset + 1] + 2;
+            break;
+            default:
+                MV_DPRINT(("Mode Select Error: invalid field in parameter "
+                         "list, mode page %d not supported, offset %d\n",
+                         req_buf[offset], offset));
+                goto cleanup;
+        }
+    }
+
+    if (length != offset)
+    {
+        MV_DPRINT(("Mode Select Error: bad length %d\n.", length));
+        goto cleanup;
+    }
+
+    if (cachePageOffset)
+    {
+        *WCEisEnabled= (req_buf[cachePageOffset+2] & MV_BIT(2)) ? 1 : 0;
+        rc= 0;
+    }
+
+cleanup:
+    kunmap_atomic(sg, KM_IRQ0);
+    return rc;
+}
+
+MV_BOOLEAN mvScsiModeSelect(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	MV_U8 ptmpBuf[MAX_MODE_PAGE_LENGTH];
+	MV_U32 pageLen = 0, tmpLen = 0;
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId;
+	MV_U8 result = 0, cachePageOffset  =0 ,offset = 0;
+	MV_U8  wcachebit = 0,smartbit = 0;
+	MV_U8 *buf = pReq->Data_Buffer;
+	MV_U32  length = pReq->Data_Transfer_Length;
+	MV_U8   PF = (pReq->Cdb[1] & MV_BIT(4)) >> 4;
+	MV_U8   SP = (pReq->Cdb[1] & MV_BIT(0));
+	MV_U32 WCEisEnabled=0;
+	if (length == 0) {
+		pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		return MV_TRUE;
+	}
+	 if (PF == 0 || SP == 1) {/* Invalid field in CDB,PARAMETER LIST LENGTH ERROR */
+		pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ADSENSE_INVALID_CDB);
+		return MV_TRUE;
+	}
+
+	portId = PATA_MapPortId(pReq->Device_Id);
+	deviceId = PATA_MapDeviceId(pReq->Device_Id);
+	pDevice = &pCore->Ports[portId].Device[deviceId];
+
+	if (mvScsiModeSelectWceGet(pCore, pReq, &WCEisEnabled))
+	{
+		 pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FIELD_IN_PARAMETER);
+		return MV_TRUE;
+	}
+	MV_ZeroMemory(buf, pReq->Data_Transfer_Length);
+	MV_ZeroMemory(ptmpBuf, MAX_MODE_PAGE_LENGTH);
+	/* Block Descriptor Length set to 0 - No Block Descriptor */
+
+	if (  pReq->Cdb[0]==SCSI_CMD_MODE_SELECT_6){
+		pageLen = Core_get_mode_page_caching((ptmpBuf+4), pDevice);
+		ptmpBuf[0] = (MV_U8)(4 + pageLen - 1);	/* Mode data length */
+		ptmpBuf[2] = 0x10;
+		tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+4));
+		offset = 4;
+	}
+	else if  (pReq->Cdb[0]==SCSI_CMD_MODE_SELECT_10) {	/* Mode Sense 10,select10 */
+		pageLen = Core_get_mode_page_caching((ptmpBuf+8), pDevice);
+		/* Mode Data Length, it does not include the number of bytes in */
+		/* Mode Data Length field */
+		tmpLen = 8 + pageLen - 2;
+		ptmpBuf[0] = (MV_U8)(((MV_U16)tmpLen) >> 8);
+		ptmpBuf[1] = (MV_U8)tmpLen;
+		ptmpBuf[2] = 0x00;
+		ptmpBuf[3] = 0x10;
+		tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+8));
+		offset = 8;
+	}
+	MV_CopyMemory(buf, ptmpBuf, tmpLen);
+	pReq->Data_Transfer_Length = tmpLen;
+
+        if ( (buf[offset] & 0x3f) == 0x08)
+        {
+            if (buf[offset + 1] != 0x12)
+            {
+              /* Invalid field in parameter list */
+                pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+                Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FIELD_IN_PARAMETER);
+                return MV_TRUE;
+            }
+            cachePageOffset = offset;
+            if(WCEisEnabled){
+			// enable write cache ,send to hardware
+			pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+			pReq->Cdb[1] = CDB_CORE_MODULE;
+			pReq->Cdb[2] = CDB_CORE_ENABLE_WRITE_CACHE;
+			return MV_FALSE;
+            } else{
+			// disable write cache,send to hardware
+			pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+			pReq->Cdb[1] = CDB_CORE_MODULE;
+			pReq->Cdb[2] = CDB_CORE_DISABLE_WRITE_CACHE;
+			return MV_FALSE;
+            }
+        }
+	pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+	return MV_TRUE;
+}
+
+void mvScsiReadDefectData(PCore_Driver_Extension pCore, PMV_Request req)
+{
+        MV_U8  temp_buf[6];
+        MV_U32 length;
+        MV_PU8 data_buf = req->Data_Buffer;
+
+        temp_buf[0] = 0;
+        temp_buf[1] = req->Cdb[2] & 0x18;
+        temp_buf[2] = 0;
+        temp_buf[3] = 0;
+        temp_buf[4] = 0;
+        temp_buf[5] = 0;
+
+        length = MV_MIN(6, req->Data_Transfer_Length);
+
+        MV_CopyMemory(req->Data_Buffer, temp_buf, length);
+        req->Data_Transfer_Length = length;
+        req->Scsi_Status = REQ_STATUS_SUCCESS;
+}
+
+MV_U8 check_page_control(PMV_Request pReq,MV_U8 page_control){
+	MV_U8 ret=0;
+	switch(page_control){
+		case 0:/* only support current */
+			ret=0;
+			break;
+		case 3:
+			ret=1;
+			pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+			Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_SAVING_PARAMETERS_NOT_SUPPORT);
+			break;
+		case 1:
+		case 2:
+		default:
+			ret=1;
+			pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+			Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+			break;
+		}
+	return ret;
+}
+void mvScsiModeSense(PCore_Driver_Extension pCore, PMV_Request pReq)
+{
+	MV_U8 pageCode = pReq->Cdb[2] & 0x3F;		/* Same for mode sense 6 and 10 */
+	MV_U8 page_control=pReq->Cdb[2]>>6;
+	MV_U8 ptmpBuf[MAX_MODE_PAGE_LENGTH];
+	MV_U32 pageLen = 0, tmpLen = 0;
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId;
+	MV_U8 *buf = pReq->Data_Buffer;
+
+	portId = PATA_MapPortId(pReq->Device_Id);
+	deviceId = PATA_MapDeviceId(pReq->Device_Id);
+	pDevice = &pCore->Ports[portId].Device[deviceId];
+
+	MV_ZeroMemory(buf, pReq->Data_Transfer_Length);
+	MV_ZeroMemory(ptmpBuf, MAX_MODE_PAGE_LENGTH);
+	/* Block Descriptor Length set to 0 - No Block Descriptor */
+
+	switch (pageCode) {
+	case 0x3F:		/* Return all pages */
+	case 0x08:		/* Caching mode page */
+		if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_6) {
+			pageLen = Core_get_mode_page_caching((ptmpBuf+4), pDevice);
+			ptmpBuf[0] = (MV_U8)(4 + pageLen - 1);	/* Mode data length */
+			ptmpBuf[2] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+4));
+		}
+		else {	/* Mode Sense 10 */
+			pageLen = Core_get_mode_page_caching((ptmpBuf+8), pDevice);
+			/* Mode Data Length, it does not include the number of bytes in */
+			/* Mode Data Length field */
+			tmpLen = 8 + pageLen - 2;
+			ptmpBuf[0] = (MV_U8)(((MV_U16)tmpLen) >> 8);
+			ptmpBuf[1] = (MV_U8)tmpLen;
+			ptmpBuf[2] = 0x00;
+			ptmpBuf[3] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+8));
+		}
+		if(check_page_control(pReq,page_control))
+			break;
+		MV_CopyMemory(buf, ptmpBuf, tmpLen);
+		pReq->Data_Transfer_Length = tmpLen;
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		break;
+	case 0x19:	/*Protocol Specific Port mode page*/
+		if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_6) {
+			pageLen = Core_Fill_ProSpecPortModePage(ptmpBuf+4);
+			ptmpBuf[0] = (MV_U8)(4 + pageLen - 1);	/* Mode data length */
+			ptmpBuf[2] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+4));
+		}else if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_10) {	/* Mode Sense 10 */
+			pageLen = Core_Fill_ProSpecPortModePage(ptmpBuf+8);
+			/* Mode Data Length, it does not include the number of bytes in */
+			/* Mode Data Length field */
+			tmpLen = 8 + pageLen - 2;
+			ptmpBuf[0] = (MV_U8)(((MV_U16)tmpLen) >> 8);
+			ptmpBuf[1] = (MV_U8)tmpLen;
+			ptmpBuf[2] = 0x00;
+			ptmpBuf[3] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+8));
+		}
+		if(check_page_control(pReq,page_control))
+			break;
+		MV_CopyMemory(buf, ptmpBuf, tmpLen);
+		pReq->Data_Transfer_Length = tmpLen;
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		break;
+	case 0x1c:	/*Informational Exceptions Control mode page*/
+		if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_6) {
+			pageLen = Core_Fill_InfoExcepCtrlModePage((ptmpBuf+4), pDevice,pReq);
+			ptmpBuf[0] = (MV_U8)(4 + pageLen - 1);	/* Mode data length */
+			ptmpBuf[2] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+4));
+		}else if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_10) {	/* Mode Sense 10 */
+			pageLen = Core_Fill_InfoExcepCtrlModePage((ptmpBuf+8), pDevice,pReq);
+			/* Mode Data Length, it does not include the number of bytes in */
+			/* Mode Data Length field */
+			tmpLen = 8 + pageLen - 2;
+			ptmpBuf[0] = (MV_U8)(((MV_U16)tmpLen) >> 8);
+			ptmpBuf[1] = (MV_U8)tmpLen;
+			ptmpBuf[2] = 0x00;
+			ptmpBuf[3] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+8));
+		}
+		if(check_page_control(pReq,page_control))
+			break;
+		MV_CopyMemory(buf, ptmpBuf, tmpLen);
+		pReq->Data_Transfer_Length = tmpLen;
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		break;
+	case 0x0a:  /*Control mode page*/
+		if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_6) {
+			pageLen = Core_Fill_CtrlModePage((ptmpBuf+4), pDevice);
+			ptmpBuf[0] = (MV_U8)(4 + pageLen - 1);	/* Mode data length */
+			ptmpBuf[2] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+4));
+		}else if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_10) {	/* Mode Sense 10 */
+			pageLen = Core_Fill_CtrlModePage((ptmpBuf+8), pDevice);
+			/* Mode Data Length, it does not include the number of bytes in */
+			/* Mode Data Length field */
+			tmpLen = 8 + pageLen - 2;
+			ptmpBuf[0] = (MV_U8)(((MV_U16)tmpLen) >> 8);
+			ptmpBuf[1] = (MV_U8)tmpLen;
+			ptmpBuf[2] = 0x00;
+			ptmpBuf[3] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+8));
+		}
+		if(check_page_control(pReq,page_control))
+			break;
+		MV_CopyMemory(buf, ptmpBuf, tmpLen);
+		pReq->Data_Transfer_Length = tmpLen;
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		break;
+
+	case 0x01: /*RW Error Recovery*/
+		if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_6) {
+			pageLen = Core_Fill_RWErrorRecoveryModePage(ptmpBuf+4);
+			ptmpBuf[0] = (MV_U8)(4 + pageLen - 1);	/* Mode data length */
+			ptmpBuf[2] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+4));
+		}else if (pReq->Cdb[0]==SCSI_CMD_MODE_SENSE_10) {	/* Mode Sense 10 */
+			pageLen = Core_Fill_RWErrorRecoveryModePage(ptmpBuf+8);
+			/* Mode Data Length, it does not include the number of bytes in */
+			/* Mode Data Length field */
+			tmpLen = 8 + pageLen - 2;
+			ptmpBuf[0] = (MV_U8)(((MV_U16)tmpLen) >> 8);
+			ptmpBuf[1] = (MV_U8)tmpLen;
+			ptmpBuf[2] = 0x00;
+			ptmpBuf[3] = 0x10;
+			tmpLen = MV_MIN(pReq->Data_Transfer_Length, (pageLen+8));
+		}
+		if(check_page_control(pReq,page_control))
+			break;
+		MV_CopyMemory(buf, ptmpBuf, tmpLen);
+		pReq->Data_Transfer_Length = tmpLen;
+		pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+		break;
+
+	default:
+		pReq->Scsi_Status = REQ_STATUS_HAS_SENSE;
+		Core_FillSenseData(pReq, SCSI_SK_ILLEGAL_REQUEST, SCSI_ASC_INVALID_FEILD_IN_CDB);
+		break;
+	}
+}
+
+void Core_Fill_SendDiagTaskfile( PDomain_Device device,PMV_Request req, PATA_TaskFile taskfile)
+{
+
+        if ((req->Cdb[1] & 0x04) &&
+                (device->Capacity &
+                        DEVICE_CAPACITY_SMART_SELF_TEST_SUPPORTED) &&
+                (device->Setting & DEVICE_SETTING_SMART_ENABLED)) {
+
+                taskfile->LBA_Low = 0x01;
+
+        } else if ((req->Cdb[1] & 0x04) &&
+                !((device->Capacity &
+                        DEVICE_CAPACITY_SMART_SELF_TEST_SUPPORTED) &&
+                (device->Setting & DEVICE_SETTING_SMART_ENABLED))) {
+          //      scsi_ata_send_diag_verify_0(root, req);
+
+                taskfile->Device = MV_BIT(6);
+                taskfile->LBA_Low = 0;
+                taskfile->LBA_Mid = 0;
+                taskfile->LBA_High = 0;
+                taskfile->Sector_Count = 1;
+	        taskfile->LBA_Low_Exp = 0;
+                taskfile->LBA_Mid_Exp = 0;
+                taskfile->LBA_High_Exp = 0;
+                taskfile->Sector_Count_Exp = 0;
+                taskfile->Features = 0;
+	        taskfile->Control = 0;
+	        taskfile->Feature_Exp = 0;
+
+                if (device->Capacity & DEVICE_CAPACITY_48BIT_SUPPORTED) {
+                        taskfile->Command = ATA_CMD_VERIFY_EXT;
+                } else {
+                        taskfile->Command = ATA_CMD_VERIFY;
+                }
+
+                return;
+        } else {
+	        switch ((req->Cdb[1] & 0xE0) >> 5) {
+	        case BACKGROUND_SHORT_SELF_TEST:
+                        taskfile->LBA_Low = 0x01;
+		        break;
+	        case BACKGROUND_EXTENDED_SELF_TEST:
+                        taskfile->LBA_Low = 0x02;
+		        break;
+	        case ABORT_BACKGROUND_SELF_TEST:
+                        taskfile->LBA_Low = 0x7F;
+		        break;
+	        case FOREGROUND_SHORT_SELF_TEST:
+                        taskfile->LBA_Low = 0x81;
+		        break;
+	        case FOREGROUND_EXTENDED_SELF_TEST:
+                        taskfile->LBA_Low = 0x82;
+		        break;
+	        default:
+		        break;
+	        }
+        }
+
+        taskfile->Device = MV_BIT(6);
+        taskfile->LBA_Mid = 0x4F;
+        taskfile->LBA_High = 0xC2;
+        taskfile->Features = ATA_CMD_SMART_EXECUTE_OFFLINE;
+	 taskfile->Command = ATA_CMD_SMART;
+
+	taskfile->Sector_Count = 0;
+	taskfile->Control = 0;
+	taskfile->Feature_Exp = 0;
+	taskfile->Sector_Count_Exp = 0;
+	taskfile->LBA_Low_Exp = 0;
+	taskfile->LBA_Mid_Exp = 0;
+	taskfile->LBA_High_Exp = 0;
+}
+
+#endif //#ifdef SUPPORT_ATA_SMART
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_sat.h
@@ -0,0 +1,77 @@
+#ifndef __CORE_SAT_H__
+#define __CORE_SAT_H__
+
+#include "core_inter.h"
+
+// ATA Specification Command Register Values (Commands)
+#define ATA_IDENTIFY_DEVICE             0xec
+#define ATA_IDENTIFY_PACKET_DEVICE      0xa1
+#define ATA_SMART_CMD                   0xb0
+#define ATA_CHECK_POWER_MODE            0xe5
+
+// ATA Specification Feature Register Values (SMART Subcommands).
+// Note that some are obsolete as of ATA-7.
+#define ATA_SMART_READ_VALUES           0xd0
+#define ATA_SMART_READ_THRESHOLDS       0xd1
+#define ATA_SMART_AUTOSAVE              0xd2
+#define ATA_SMART_SAVE                  0xd3
+#define ATA_SMART_IMMEDIATE_OFFLINE     0xd4
+#define ATA_SMART_READ_LOG_SECTOR       0xd5
+#define ATA_SMART_WRITE_LOG_SECTOR      0xd6
+#define ATA_SMART_WRITE_THRESHOLDS      0xd7
+#define ATA_SMART_ENABLE                0xd8
+#define ATA_SMART_DISABLE               0xd9
+#define ATA_SMART_STATUS                0xda
+// SFF 8035i Revision 2 Specification Feature Register Value (SMART
+// Subcommand)
+#define ATA_SMART_AUTO_OFFLINE          0xdb
+
+/* ANSI SCSI-3 Log Pages retrieved by LOG SENSE. */
+#define SUPPORTED_LPAGES                        0x00
+#define BUFFER_OVERRUN_LPAGE                    0x01
+#define WRITE_ERROR_COUNTER_LPAGE               0x02
+#define READ_ERROR_COUNTER_LPAGE                0x03
+#define READ_REVERSE_ERROR_COUNTER_LPAGE        0x04
+#define VERIFY_ERROR_COUNTER_LPAGE              0x05
+#define NON_MEDIUM_ERROR_LPAGE                  0x06
+#define LAST_N_ERROR_LPAGE                      0x07
+#define FORMAT_STATUS_LPAGE                     0x08
+#define TEMPERATURE_LPAGE                       0x0d
+#define STARTSTOP_CYCLE_COUNTER_LPAGE           0x0e
+#define APPLICATION_CLIENT_LPAGE                0x0f
+#define SELFTEST_RESULTS_LPAGE                  0x10
+#define BACKGROUND_RESULTS_LPAGE                0x15   /* SBC-3 */
+#define IE_LPAGE                                0x2f
+
+/* Seagate vendor specific log pages. */
+#define SEAGATE_CACHE_LPAGE                     0x37
+#define SEAGATE_FACTORY_LPAGE                   0x3e
+
+enum mode_sense_page_code {
+        DIRECT_ACCESS_BLOCK_DEVICE_MODE_PAGE    = 0x00,
+        RW_ERROR_RECOVERY_MODE_PAGE             = 0x01,
+        CACHE_MODE_PAGE                         = 0x08,
+        CONTROL_MODE_PAGE                       = 0x0A,
+        PORT_MODE_PAGE                          = 0x19,
+        INFORMATIONAL_EXCEPTIONS_CONTROL_MODE_PAGE = 0x1C,
+        ALL_MODE_PAGE                           = 0x3F,
+};
+
+/**************************************************************/
+/**
+* Self-Test Code translation for Send Diagnostic Command
+**/
+#define DEFAULT_SELF_TEST					0x0
+#define BACKGROUND_SHORT_SELF_TEST			0x1
+#define BACKGROUND_EXTENDED_SELF_TEST		0x2
+#define ABORT_BACKGROUND_SELF_TEST			0x4
+#define FOREGROUND_SHORT_SELF_TEST			0x5
+#define FOREGROUND_EXTENDED_SELF_TEST		0x6
+
+void mvScsiReadDefectData(PCore_Driver_Extension pCore, PMV_Request req);
+void  mvScsiLogSenseTranslation(PCore_Driver_Extension pCore,  PMV_Request pReq);
+MV_BOOLEAN mvScsiModeSelect(PCore_Driver_Extension pCore, PMV_Request pReq);
+void mvScsiModeSense(PCore_Driver_Extension pCore, PMV_Request pReq);
+void Core_Fill_SendDiagTaskfile( PDomain_Device device,PMV_Request req, PATA_TaskFile taskfile);
+
+#endif /* __CORE_SAT_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_sata.h
@@ -0,0 +1,50 @@
+#if !defined(FIS_H)
+#define FIS_H
+
+/* SATA FIS: Register-Host to Device*/
+typedef struct _SATA_FIS_REG_H2D
+{
+	MV_U8	FIS_Type;
+#ifdef __MV_BIG_ENDIAN_BITFIELD__
+	MV_U8	C : 1;
+	MV_U8	Reserved0 : 3;
+	MV_U8	PM_Port : 4;
+#else
+	MV_U8	PM_Port : 4;
+	MV_U8	Reserved0 : 3;
+	MV_U8	C : 1;
+#endif /* __MV_BIG_ENDIAN_BITFIELD__ */
+	MV_U8	Command;
+	MV_U8	Features;
+
+	MV_U8	LBA_Low;
+	MV_U8	LBA_Mid;
+	MV_U8	LBA_High;
+	MV_U8	Device;
+
+	MV_U8	LBA_Low_Exp;
+	MV_U8	LBA_Mid_Exp;
+	MV_U8	LBA_High_Exp;
+	MV_U8	Features_Exp;
+
+	MV_U8	Sector_Count;
+	MV_U8	Sector_Count_Exp;
+	MV_U8	Reserved1;
+	MV_U8	Control;
+
+	MV_U8	Reserved2[4];
+} SATA_FIS_REG_H2D, *PSATA_FIS_REG_H2D;
+
+/* FIS type definition */
+#define SATA_FIS_TYPE_REG_H2D			0x27	/* Register FIS - Host to Device */
+#define SATA_FIS_TYPE_REG_D2H			0x34	/* Register FIS - Device to Host */
+
+#define SATA_FIS_TYPE_DMA_ACTIVATE		0x39	/* DMA Activate FIS - Device to Host */
+#define SATA_FIS_TYPE_DMA_SETUP			0x41	/* DMA Setup FIS - Bi-directional */
+
+#define SATA_FIS_TYPE_DATA				0x46	/* Data FIS - Bi-directional */
+#define SATA_FIS_TYPE_BIST_ACTIVATE		0x58	/* BIST Activate FIS - Bi-directional */
+#define SATA_FIS_TYPE_PIO_SETUP			0x5F	/* PIO Setup FIS - Device to Host */
+#define SATA_FIS_TYPE_SET_DEVICE_BITS	0xA1	/* Set Device Bits FIS - Device to Host */
+
+#endif /* FIS_H */
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_swxor.c
@@ -0,0 +1,528 @@
+#include "mv_include.h"
+#include "core_inter.h"
+
+#if defined(RAID_DRIVER) && defined(USE_NEW_SGTABLE)
+typedef struct _xor_strm_t
+{
+	sgd_t		sgd[2];
+	MV_U32		off;
+} xor_strm_t;
+
+static MV_PVOID sgd_kmap(
+	PCore_Driver_Extension	pCore,
+	sgd_t*					sg
+	)
+{
+#ifdef _OS_WINDOWS
+	sgd_pctx_t* pctx = (sgd_pctx_t*) sg;
+	MV_PTR_INTEGER addr = (MV_PTR_INTEGER)(pctx->baseAddr.value);
+
+	MV_DASSERT( sg->flags & SGD_PCTX );
+
+	addr &= ~0x80000000L; // just for fun, refer to GenerateSGTable in win_helper.c
+
+	return (MV_PVOID) addr;
+#endif /* _OS_WINDOWS */
+#ifdef _OS_LINUX
+	sgd_pctx_t* pctx = (sgd_pctx_t*)sg;
+	struct scatterlist *ksg = (struct scatterlist *)pctx->u.xctx;
+	void *kvaddr = NULL;
+
+	MV_DASSERT( sg->flags & SGD_PCTX );
+	MV_DASSERT( ksg );
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 24)
+	kvaddr = page_address(ksg->page);
+	if (!kvaddr)
+#endif
+		kvaddr = map_sg_page(ksg);
+	kvaddr += ksg->offset;
+	return kvaddr;
+#endif /* _OS_LINUX */
+}
+
+static void sgd_kunmap(
+	PCore_Driver_Extension	pCore,
+	sgd_t*					sg,
+	MV_PVOID				mapped_addr
+	)
+{
+#ifdef _OS_WINDOWS
+#endif /* _OS_WINDOWS */
+#ifdef _OS_LINUX
+	sgd_pctx_t* pctx = (sgd_pctx_t*)sg;
+	struct scatterlist *ksg = (struct scatterlist *)pctx->u.xctx;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 24)
+	void *kvaddr = NULL;
+	kvaddr = page_address(ksg->page);
+	if (!kvaddr)
+#endif
+	kunmap_atomic(mapped_addr - ksg->offset, KM_IRQ0);
+#endif /* _OS_LINUX */
+}
+static MV_PVOID sgd_kmap_sec(
+	PCore_Driver_Extension	pCore,
+	sgd_t*					sg
+	)
+{
+#ifdef _OS_WINDOWS
+	sgd_pctx_t* pctx = (sgd_pctx_t*) sg;
+	MV_PTR_INTEGER addr = (MV_PTR_INTEGER)(pctx->baseAddr.value);
+
+	MV_DASSERT( sg->flags & SGD_PCTX );
+
+	addr &= ~0x80000000L; // refer to GenerateSGTable in win_helper.c
+
+	return (MV_PVOID) addr;
+#endif /* _OS_WINDOWS */
+#ifdef _OS_LINUX
+	sgd_pctx_t* pctx = (sgd_pctx_t*)sg;
+	struct scatterlist *ksg = (struct scatterlist *)pctx->u.xctx;
+	void *kvaddr = NULL;
+	MV_DASSERT( sg->flags & SGD_PCTX );
+	MV_DASSERT( ksg );
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 24)
+	kvaddr = page_address(ksg->page);
+	if (!kvaddr)
+#endif
+		kvaddr = map_sg_page_sec(ksg);
+	kvaddr += ksg->offset;
+	return kvaddr;
+#endif /* _OS_LINUX */
+}
+
+static void sgd_kunmap_sec(
+	PCore_Driver_Extension	pCore,
+	sgd_t*					sg,
+	MV_PVOID				mapped_addr
+	)
+{
+#ifdef _OS_WINDOWS
+#endif /* _OS_WINDOWS */
+#ifdef _OS_LINUX
+	sgd_pctx_t* pctx = (sgd_pctx_t*)sg;
+	struct scatterlist *ksg = (struct scatterlist *)pctx->u.xctx;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 24)
+	void *kvaddr = NULL;
+	kvaddr = page_address(ksg->page);
+	if (!kvaddr)
+#endif
+	kunmap_atomic(mapped_addr - ksg->offset, KM_IRQ1);
+#endif /* _OS_LINUX */
+}
+
+void CopySGs(
+	PCore_Driver_Extension	pCore,
+	PMV_SG_Table			srctbl,
+	PMV_SG_Table			dsttbl
+	);
+
+#endif /* RAID_DRIVER && USE_NEW_SGTABLE */
+
+#if defined(RAID_DRIVER) && defined(SOFTWARE_XOR) && defined(USE_NEW_SGTABLE)
+/*
+ * CompareSGs needs not to be changed for SGD_PCTX since RAID always
+ * use memory with known virtual address.
+ */
+void
+CompareSGs(
+	PCore_Driver_Extension	pCore,
+	PMV_XOR_Request			pXORReq,
+	PMV_SG_Table			srctbl
+	)
+{
+	MV_PU32					pSrc[2] = { NULL, };
+	sgd_t					sgd[2];
+	sgd_iter_t				sg_iter[2];
+	MV_U32					wCount[2];
+	MV_BOOLEAN				bFinished = MV_FALSE;
+	MV_U8					bIndex;
+	MV_U32					offset = 0;
+	MV_PVOID				p = NULL;
+
+	MV_ASSERT( srctbl[0].Byte_Count == srctbl[1].Byte_Count );
+
+	for( bIndex = 0; bIndex < 2; bIndex++ )
+	{
+		sgd_iter_init( &sg_iter[bIndex], srctbl[bIndex].Entry_Ptr, 0, srctbl[bIndex].Byte_Count );
+
+		sgd_iter_get_next( &sg_iter[bIndex], sgd );
+
+		sgd_get_vaddr( sgd, p );
+		pSrc[bIndex] = (MV_PU32) p;
+
+		sgd_getsz( sgd, wCount[bIndex] );
+
+	}
+
+	while( !bFinished )
+	{
+		if( *pSrc[0] != *pSrc[1] )
+		{
+			pXORReq->Request_Status = XOR_STATUS_ERROR;
+			pXORReq->Error_Offset = offset;
+			return;
+		}
+
+		offset += sizeof( MV_U32 );
+
+		for( bIndex = 0; bIndex < 2; bIndex++ )
+		{
+			pSrc[bIndex]++;
+			wCount[bIndex] -= sizeof( MV_U32 );
+			if( wCount[bIndex] == 0 )
+			{
+				if( !sgd_iter_get_next( &sg_iter[bIndex], sgd ) )
+				{
+					bFinished = MV_TRUE;
+
+					/*
+					for( bIndex = 0; bIndex < srcCount + dstCount; bIndex++ )
+					{
+						MV_ASSERT( !sgd_iter_get_next( &sg_iter[bIndex], sgd ) );
+					}
+					*/
+					break;
+				}
+
+				sgd_get_vaddr( sgd, p );
+				pSrc[bIndex] = (MV_PU32) p;
+
+				sgd_getsz( sgd, wCount[bIndex] );
+			}
+		}
+	}
+}
+
+#if defined(XOR_AS_GF)
+typedef MV_U8 XORUNIT;
+#else
+typedef MV_U32 XORUNIT;
+#endif
+
+void sg_xor(
+	PCore_Driver_Extension	pCore,
+	xor_strm_t*				strm,
+	MV_U8					src_cnt,
+	MV_U8					dst_cnt,
+	MV_U32					byte_cnt
+	)
+{
+	//XORUNIT	*pSrc, *pDst;
+	XORUNIT		*p;
+	MV_PVOID 	ptmp;
+	int		i;
+	XORUNIT	value = 0;
+	MV_BOOLEAN	mapped;
+	MV_U32	off = 0;
+#ifdef _OS_LINUX
+	unsigned long flags = 0;
+#endif		/* _OS_LINUX */
+
+	while( byte_cnt )
+	{
+		for( i = 0; i < src_cnt+dst_cnt; i++ )
+		{
+			mapped = MV_FALSE;
+			if( strm[i].sgd[0].flags & SGD_PCTX )
+			{
+			#ifdef _OS_LINUX
+				local_irq_save(flags);
+			#endif
+				p = (XORUNIT*) sgd_kmap(pCore,strm[i].sgd);
+				ptmp = p;
+				mapped = MV_TRUE;
+	#ifdef _OS_LINUX
+				p = (XORUNIT*)(((MV_PU8)p) +
+				              strm[i].sgd[1].size);
+	#endif
+			}
+			else
+			{
+				ptmp = NULL;
+				sgd_get_vaddr( strm[i].sgd, ptmp );
+				p = (XORUNIT*) ptmp;
+			}
+
+			p = (XORUNIT*) (((MV_PU8) p) + strm[i].off + off);
+
+			if( i == 0 )
+				value = *p;
+			else if( i >= src_cnt )
+				*p = value;
+			else
+				value ^= *p;
+
+			if( mapped ){
+				sgd_kunmap( pCore, strm[i].sgd, ptmp );
+			#ifdef _OS_LINUX
+				local_irq_restore(flags);
+			#endif
+			}
+		}
+		byte_cnt -= sizeof(XORUNIT);
+		off += sizeof(XORUNIT);
+	}
+}
+
+MV_U32 min_of(MV_U32* ele, MV_U32 cnt)
+{
+	MV_U32 v = *ele++;
+
+	while( --cnt )
+	{
+		if( *ele < v )
+			v = *ele;
+		ele++;
+	}
+
+	return v;
+}
+
+void
+XorSGs(
+	PCore_Driver_Extension	pCore,
+	MV_U8					srcCount,
+	MV_U8					dstCount,
+	PMV_SG_Table			srctbl,
+	PMV_SG_Table			dsttbl,
+	XOR_COEF				Coef[XOR_TARGET_SG_COUNT][XOR_SOURCE_SG_COUNT]
+	)
+{
+	xor_strm_t				strm[XOR_SOURCE_SG_COUNT+ XOR_TARGET_SG_COUNT];
+	sgd_iter_t				sg_iter[XOR_SOURCE_SG_COUNT + XOR_TARGET_SG_COUNT];
+	MV_U32					wCount[XOR_SOURCE_SG_COUNT + XOR_TARGET_SG_COUNT];
+	MV_U8					bIndex;
+	MV_BOOLEAN				bFinished = MV_FALSE;
+	MV_U32					tmp = srctbl[0].Byte_Count;
+	MV_U32					count = 0xffffffffL;
+
+	for( bIndex = 0; bIndex < srcCount; bIndex++ )
+	{
+		MV_ASSERT( srctbl[bIndex].Byte_Count == tmp );
+		sgd_iter_init( &sg_iter[bIndex], srctbl[bIndex].Entry_Ptr, 0, srctbl[bIndex].Byte_Count );
+	}
+
+	for( bIndex = 0; bIndex < dstCount; bIndex++ )
+	{
+		MV_ASSERT( srctbl[bIndex].Byte_Count == tmp );
+		sgd_iter_init( &sg_iter[srcCount+bIndex], dsttbl[bIndex].Entry_Ptr, 0, dsttbl[bIndex].Byte_Count );
+	}
+
+	for( bIndex = 0; bIndex < srcCount + dstCount; bIndex++ )
+	{
+		strm[bIndex].off = 0;
+		sgd_iter_get_next( &sg_iter[bIndex], strm[bIndex].sgd );
+		sgd_getsz( strm[bIndex].sgd, wCount[bIndex] );
+
+		if( wCount[bIndex] < count )
+			count = wCount[bIndex];
+	}
+
+	while( !bFinished )
+	{
+
+		sg_xor( pCore, strm, srcCount, dstCount, count );
+
+		for( bIndex = 0; bIndex < srcCount + dstCount; bIndex++ )
+		{
+			wCount[bIndex] -= count;
+			if( wCount[bIndex] == 0 )
+			{
+				if( !sgd_iter_get_next( &sg_iter[bIndex], strm[bIndex].sgd ) )
+				{
+					bFinished = MV_TRUE;
+					break;
+				}
+
+				strm[bIndex].off = 0;
+				sgd_getsz( strm[bIndex].sgd, wCount[bIndex] );
+			}
+			else
+				strm[bIndex].off += count;
+		}
+		count = min_of( wCount, srcCount + dstCount );
+	}
+}
+
+
+
+#ifdef SIMULATOR
+void _Core_ModuleSendXORRequest(MV_PVOID This, PMV_XOR_Request pXORReq)
+#else	/*SIMULATOR*/
+void Core_ModuleSendXORRequest(MV_PVOID This, PMV_XOR_Request pXORReq)
+#endif	/*!SIMULATOR*/
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+
+	switch (pXORReq->Request_Type)
+	{
+		case XOR_REQUEST_WRITE:
+			XorSGs(
+				pCore,
+				pXORReq->Source_SG_Table_Count,
+				pXORReq->Target_SG_Table_Count,
+				pXORReq->Source_SG_Table_List,
+				pXORReq->Target_SG_Table_List,
+				pXORReq->Coef );
+			break;
+		case XOR_REQUEST_COMPARE:
+			MV_ASSERT( pXORReq->Source_SG_Table_Count == 2 );
+			CompareSGs(
+				pCore,
+				pXORReq,
+				pXORReq->Source_SG_Table_List );
+			break;
+		case XOR_REQUEST_DMA:
+			CopySGs(
+				pCore,
+				pXORReq->Source_SG_Table_List,
+				pXORReq->Target_SG_Table_List );
+			break;
+		default:
+			pXORReq->Request_Status = XOR_STATUS_INVALID_REQUEST;
+			break;
+	}
+#ifndef SIMULATOR
+	pXORReq->Completion( pXORReq->Cmd_Initiator, pXORReq );
+#endif	/*!SIMULATOR*/
+}
+
+#endif /* RAID_DRIVER && SOFTWARE_XOR && USE_NEW_SGTABLE */
+
+#if defined(RAID_DRIVER) && defined(USE_NEW_SGTABLE)
+
+static void sg_memcpy(
+	PCore_Driver_Extension	pCore,
+	xor_strm_t				*strm, /*strm[0]:source, strm[1]:destination*/
+	MV_U32					byte_cnt
+	)
+{
+	MV_U32		sz;
+	MV_PU32		pSrc[2] = { NULL, NULL };
+	MV_BOOLEAN	mapped[2] = { MV_FALSE, MV_FALSE };
+	MV_PVOID	p = NULL;
+	int			i;
+	MV_PVOID 	ptmp = NULL;
+#ifdef _OS_LINUX
+	unsigned long flags = 0;
+	int             enabled = 0;
+#endif		/* _OS_LINUX */
+
+	for( i = 0; i < 2; i++ )
+	{
+		sgd_getsz(strm[i].sgd,sz);
+		MV_DASSERT( strm[i].off < sz );
+		MV_DASSERT( strm[i].off+byte_cnt <= sz );
+	}
+
+#ifdef _OS_LINUX
+	if( strm[i].sgd[0].flags & SGD_PCTX ){
+		enabled = 1;
+		local_irq_save(flags);
+	}
+#endif		/* _OS_LINUX */
+
+	for( i = 0; i < 2; i++ )
+	{
+		if( strm[i].sgd[0].flags & SGD_PCTX )
+		{
+			if(i==0)
+				ptmp = (MV_PU32) sgd_kmap(pCore,strm[i].sgd);
+			else
+				ptmp = (MV_PU32) sgd_kmap_sec(pCore,strm[i].sgd);
+
+			mapped[i] = MV_TRUE;
+	#ifdef _OS_LINUX
+			pSrc[i] = (MV_PU32)((MV_PU8)ptmp + strm[i].sgd[1].size);
+	#else
+			pSrc[i] = (MV_PU32)ptmp;
+	#endif
+		}
+		else
+		{
+			sgd_get_vaddr( strm[i].sgd, p );
+			pSrc[i] = (MV_PU32) p;
+		}
+	}
+
+	MV_CopyMemory(
+		pSrc[1]+strm[1].off/sizeof(MV_U32),
+		pSrc[0]+strm[0].off/sizeof(MV_U32),
+		byte_cnt );
+
+	for(i = 0; i < 2; i++ ) {
+		if( mapped[i] ) {
+			if(i==0)
+				sgd_kunmap(pCore,strm[i].sgd, ptmp);
+			else
+				sgd_kunmap_sec(pCore,strm[i].sgd, ptmp);
+		}
+	}
+
+#ifdef _OS_LINUX
+	if( enabled == 1 )
+		local_irq_restore(flags);
+#endif		/* _OS_LINUX */
+
+	return;
+}
+
+
+void CopySGs(
+	PCore_Driver_Extension	pCore,
+	PMV_SG_Table			srctbl,
+	PMV_SG_Table			dsttbl
+	)
+{
+	//MV_PU32					pSrc[2] = { NULL, };
+	sgd_iter_t				sg_iter[2];
+	MV_U32					wCount[2], count;
+	MV_BOOLEAN				bFinished = MV_FALSE;
+	MV_U8					bIndex;
+	//MV_PVOID				p;
+	xor_strm_t				strm[2];
+
+	MV_ASSERT( srctbl->Byte_Count == dsttbl->Byte_Count );
+
+	sgd_iter_init( &sg_iter[0], srctbl->Entry_Ptr, 0, srctbl->Byte_Count );
+	sgd_iter_init( &sg_iter[1], dsttbl->Entry_Ptr, 0, dsttbl->Byte_Count );
+
+	for( bIndex = 0; bIndex < 2; bIndex++ )
+	{
+		strm[bIndex].off = 0;
+		sgd_iter_get_next( &sg_iter[bIndex], strm[bIndex].sgd );
+		sgd_getsz( strm[bIndex].sgd, wCount[bIndex] );
+	}
+
+	while( !bFinished )
+	{
+		count = MV_MIN( wCount[0], wCount[1] );
+
+		sg_memcpy(
+			pCore,
+			strm,
+			count
+			);
+
+		for( bIndex = 0; bIndex < 2; bIndex++ )
+		{
+			wCount[bIndex] -= count;
+			if( wCount[bIndex] == 0 )
+			{
+				if( !sgd_iter_get_next( &sg_iter[bIndex], strm[bIndex].sgd ) )
+				{
+					bFinished = MV_TRUE;
+					break;
+				}
+				sgd_getsz( strm[bIndex].sgd, wCount[bIndex] );
+				strm[bIndex].off = 0;
+			}
+			else
+				strm[bIndex].off += count;
+		}
+	}
+}
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_thor.h
@@ -0,0 +1,500 @@
+#if !defined(CORE_SATA_H)
+#define CORE_SATA_H
+
+#include "core_ata.h"
+#include "com_tag.h"
+
+#define MAX_SATA_PORT_NUMBER		4
+#if /*(VER_OEM==VER_OEM_ASUS) || */(VER_OEM==VER_OEM_INTEL)
+#define MAX_PATA_PORT_NUMBER		0
+#else
+#define MAX_PATA_PORT_NUMBER		1
+#endif
+#define MAX_PORT_NUMBER				(MAX_SATA_PORT_NUMBER+MAX_PATA_PORT_NUMBER)
+
+#ifdef SUPPORT_PM
+#define MAX_DEVICE_PER_PORT			4	/*Hardware only support 4 device for PM*/
+#else
+#define MAX_DEVICE_PER_PORT			2
+#endif
+
+#define MAX_DEVICE_NUMBER			(MAX_PORT_NUMBER*MAX_DEVICE_PER_PORT)
+
+#define MAX_SLOT_NUMBER				32
+
+#define INTERNAL_REQ_COUNT			MAX_DEVICE_SUPPORTED
+
+#define SATA_CMD_LIST_SIZE			(32 * MAX_SLOT_NUMBER)
+#define SATA_RX_FIS_SIZE			256
+
+#define SATA_CMD_TABLE_HEADER_SIZE	0x80
+#define SATA_CMD_TABLE_SG_SIZE		(HW_SG_ENTRY_MAX * 16)
+#define SATA_CMD_TABLE_SIZE			(SATA_CMD_TABLE_HEADER_SIZE + SATA_CMD_TABLE_SG_SIZE)
+
+#define SATA_SCRATCH_BUFFER_SIZE	sizeof(ATA_Identify_Data)
+
+/* various functions for master/slave support */
+#define PATA_MapDeviceId(ID)		( (ID) % MAX_DEVICE_PER_PORT )
+#define PATA_MapPortId(ID)			( (ID) / MAX_DEVICE_PER_PORT )
+
+#define DEVICE_TYPE_ATAPI						MV_BIT(0)
+
+/* Device initialization state */
+#define DEVICE_STATE_IDLE						0x0
+#define DEVICE_STATE_RESET_DONE					0x1
+#define DEVICE_STATE_IDENTIFY_DONE				0x2
+#define DEVICE_STATE_SET_UDMA_DONE				0x3
+#define DEVICE_STATE_SET_PIO_DONE				0x4
+#define DEVICE_STATE_ENABLE_WRITE_CACHE_DONE	0x5
+#define DEVICE_STATE_ENABLE_READ_AHEAD_DONE		0x6
+#define DEVICE_STATE_INIT_DONE					0xFF
+
+/* Device status */
+#define DEVICE_STATUS_NO_DEVICE					MV_BIT(0)
+#define DEVICE_STATUS_EXISTING					MV_BIT(1)
+#define DEVICE_STATUS_FUNCTIONAL				MV_BIT(2)
+#ifdef HOTPLUG_ISSUE_WORKROUND
+#define DEVICE_STATUS_UNPLUG    				MV_BIT(3)
+#endif
+
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+#define DEVICE_STATUS_SLEEP            MV_BIT(5)
+#define DEVICE_STATUS_IDLE             MV_BIT(6)
+#define DEVICE_STATUS_STANDBY          MV_BIT(7)
+#endif
+
+/* 3G and TCQ */
+#define DEVICE_CAPACITY_48BIT_SUPPORTED			MV_BIT(0)
+#define	DEVICE_CAPACITY_SMART_SUPPORTED			MV_BIT(1)
+#define	DEVICE_CAPACITY_WRITECACHE_SUPPORTED	MV_BIT(2)
+#define DEVICE_CAPACITY_NCQ_SUPPORTED			MV_BIT(3)
+#define DEVICE_CAPACITY_RATE_1_5G				MV_BIT(4)
+#define DEVICE_CAPACITY_RATE_3G					MV_BIT(5)
+#define DEVICE_CAPACITY_READLOGEXT_SUPPORTED	MV_BIT(6)
+#define DEVICE_CAPACITY_READ_LOOK_AHEAD_SUPPORTED	MV_BIT(7)
+#define DEVICE_CAPACITY_SMART_SELF_TEST_SUPPORTED	MV_BIT(8)
+#define 	DEVICE_CAPABILITY_PROTECTION_INFORMATION_SUPPORTED  MV_BIT(9)
+#ifdef SUPPORT_ATA_SECURITY_CMD
+#define DEVICE_CAPACITY_SECURITY_SUPPORTED	MV_BIT(10)
+#endif
+
+#define DEVICE_SETTING_SMART_ENABLED                MV_BIT(0)
+#define DEVICE_SETTING_WRITECACHE_ENABLED           MV_BIT(1)
+#define DEVICE_SETTING_PI_ENABLED                   MV_BIT(2)
+#define DEVICE_SETTING_READ_LOOK_AHEAD				MV_BIT(3)
+#ifdef SUPPORT_ATA_SECURITY_CMD
+#define DEVICE_SETTING_SECURITY_LOCKED				MV_BIT(4)
+#endif
+
+#define PATA_CABLE_DETECTION		1
+
+#ifdef PATA_CABLE_DETECTION
+#define MV_80_PIN_CABLE				0
+#define MV_40_PIN_CABLE				1
+#endif
+
+struct _Domain_Device {
+	MV_U16 Id;
+	MV_U8 Device_Type;				/* ATA or ATAPI */
+	MV_U8 State;					/* DEVICE_STATE_XXX */
+
+	MV_U8 Status;					/* DEVICE_STATUS_XXX */
+	MV_BOOLEAN Is_Slave;
+	MV_BOOLEAN Need_Notify;			/* added for PM hot plug */
+	MV_U8 Reserved0;
+
+	struct _Domain_Port * PPort;	/* Shortcut to the port. */
+
+	/*
+	 * Different device should have a different struct here.
+	 * Now it's SATA device only.
+	 */
+	MV_U16 Capacity;				/* Be able to support NCQ, 48 bit LBA. */
+	MV_U16 Setting;					/* The supported features are enabled or not. */
+
+	MV_U8 PM_Number;
+	MV_U8 PIO_Mode;
+	MV_U8 MDMA_Mode;
+	MV_U8 UDMA_Mode;
+
+	MV_U8 Current_PIO;
+	MV_U8 Current_MDMA;
+	MV_U8 Current_UDMA;
+	MV_U8 Reserved1;
+
+	MV_U64 Max_LBA;
+	MV_U8 Queue_Depth;
+	MV_U8 Timer_ID;					/* for error handling */
+	MV_U8 Outstanding_Req;			/* for error handling */
+	MV_U32 Reset_Count;				/* for error handling. If reset too many times, set down this device. */
+	MV_U8 Reserved2[4];
+
+
+	MV_U8 Serial_Number[20];
+	MV_U8 Model_Number[40];
+	MV_U8 Firmware_Revision[8];
+	MV_U32 WWN;
+
+	/* The scratch buffer used for initialization like identify */
+	MV_PVOID Scratch_Buffer;
+	MV_PHYSICAL_ADDR Scratch_Buffer_DMA;
+
+	/* Internal request used in device initialization */
+	PMV_Request Internal_Req;
+
+};
+
+/* Port initialization state */
+
+#define PORT_STATE_IDLE					0x00
+#define PORT_STATE_INIT_DONE			0xFF
+
+#define PORT_TYPE_SATA					0
+#define PORT_TYPE_PATA					1
+#define PORT_TYPE_PM					4 /*PM Support, lily tested*/
+
+#define PORT_CAPACITY_NCQ_SUPPORTED		MV_BIT(0)
+
+#define PORT_SETTING_NCQ_RUNNING		MV_BIT(0)
+#define PORT_SETTING_PM_EXISTING		MV_BIT(1)
+#define PORT_SETTING_PM_FUNCTIONAL		MV_BIT(2)	// added by Harriet for PM hot plug
+#define PORT_SETTING_DURING_RETRY		MV_BIT(3)
+typedef void (*mv_reset_cmd_completion)(MV_PVOID);
+#define MAX_RESET_TIMES 1
+
+#define PORT_ERROR_AT_PLUGIN			1
+#define PORT_ERROR_AT_RUNTIME		2
+
+struct _Domain_Port {
+	MV_PVOID Core_Extension;
+
+	MV_U8 Id;
+	MV_U8 Port_State;
+	MV_U8 Type;					/* PORT_TYPE_XXX */
+	MV_U8 Capacity;				/* PORT_CAPACITY_XXX */
+	MV_U8 Setting;				/* PORT_SETTING_XXX */
+	MV_U8 Device_Number;		/* How many devices this port has now? */
+	MV_U16 PM_Vendor_Id;
+	MV_U16 PM_Device_Id;
+	MV_U8 PM_Product_Revision;
+	MV_U8 PM_Spec_Revision;
+	MV_U8 PM_Num_Ports;
+	MV_U8 PATA_cable_type;      /* Save the correct cable type from BIOS.*/
+	MV_U8 Reserved0[2];
+	MV_LPVOID Mmio_Base;		/* Base address for SATA Port Registers */
+	MV_LPVOID Mmio_SCR;			/* Base address for sata register(SCR) */
+	MV_PVOID Cmd_List;			/* Can be PMV_PATA_Command_Header or PMV_Command_Header */
+	MV_PHYSICAL_ADDR Cmd_List_DMA;
+
+	/* Received FIS */
+	MV_PVOID RX_FIS;
+	MV_PHYSICAL_ADDR RX_FIS_DMA;
+
+	/* The 32 command tables. */
+	MV_PVOID Cmd_Table;
+	MV_PHYSICAL_ADDR Cmd_Table_DMA;
+
+	/* Running MV_Requests are linked together. */
+	PMV_Request Running_Req[MAX_SLOT_NUMBER];
+
+	/* Which slot has requests running. */
+	MV_U32	Running_Slot;
+//	MV_U32	Reserved1;
+	MV_U32	VS_RegC;
+#ifdef COMMAND_ISSUE_WORKROUND
+	OSSW_DECLARE_TIMER(timer);
+	MV_U8 Hot_Plug_Timer;
+	MV_U8 reset_hba_times;
+	MV_PVOID	timer_para;
+	mv_reset_cmd_completion	command_callback; /* call back function */
+	MV_U8 find_disk;
+	MV_U8 error_state;
+#endif
+
+	struct _Domain_Device Device[MAX_DEVICE_PER_PORT];
+
+	//Timer: for time out checking.
+
+	Tag_Stack Tag_Pool;
+#ifdef HOTPLUG_ISSUE_WORKROUND
+	MV_U32 old_stat;
+#endif
+};
+
+/*
+ * Hardware related format. Never change their size. Must follow hardware specification.
+ */
+/* AHCI a little difference */
+typedef struct _MV_Command_Header
+{
+#ifdef __MV_BIG_ENDIAN_BITFIELD__
+	MV_U8	Reserved0 : 2;
+	MV_U8	Packet_Command : 1;
+	MV_U8	FIS_Length : 5;
+
+	MV_U8	PM_Port : 4;
+	MV_U8	NCQ : 1;
+	MV_U8	Reserved1: 2;
+	MV_U8	Reset : 1;
+#else /* default to __MV_LITTLE_ENDIAN_BITFIELD__ */
+	MV_U8	FIS_Length : 5;		/* Command FIS Length in DWORD */
+	MV_U8	Packet_Command : 1;	/* ATAPI packet command */
+	MV_U8	Reserved0 : 2;
+
+	MV_U8	Reset : 1;
+	MV_U8	Reserved1: 2;
+	MV_U8	NCQ : 1;
+	MV_U8	PM_Port : 4;
+#endif /* __MV_BIG_ENDIAN_BITFIELD__ */
+	MV_U16	PRD_Entry_Count : 16;
+
+	MV_U32	Reserved2;
+	MV_U32	Table_Address;
+	MV_U32	Table_Address_High;
+
+	MV_U32	Reserved3[4];
+} MV_Command_Header, *PMV_Command_Header;
+
+typedef struct _MV_PATA_Command_Header
+{
+#ifdef __MV_BIG_ENDIAN_BITFIELD__
+	MV_U8	Packet_Command : 1;
+	MV_U8	TCQ : 1;
+	MV_U8	Controller_Command : 1;
+	MV_U8	PIO_Sector_Count : 5;
+
+	MV_U8	Is_Slave : 1;
+	MV_U8	Reset : 1;
+	MV_U8	Diagnostic_Command : 1;
+	MV_U8	Is_48Bit : 1;
+	MV_U8	PIO_Sector_Command : 1;
+	MV_U8	Non_Data : 1;
+	MV_U8	Data_In : 1;
+	MV_U8	DMA : 1;
+#else /* __MV_BIG_ENDIAN_BITFIELD__ */
+	MV_U8	PIO_Sector_Count : 5;   /* PIO command data block size in sector */
+	MV_U8	Controller_Command : 1;	/* If 1, command is for the controller instead of the device */
+	MV_U8	TCQ : 1;		/* TCQ command */
+	MV_U8	Packet_Command : 1;	/* ATAPI packet command */
+
+	MV_U8	DMA : 1;		/* DMA command */
+	MV_U8	Data_In : 1;		/* Data is from device to host. */
+	MV_U8	Non_Data : 1;		/* Non data command */
+	MV_U8	PIO_Sector_Command : 1;	/* PIO multiple sectors commands including read/write sector, read/write multiple. */
+	MV_U8	Is_48Bit : 1;		/* 48 bit command */
+	MV_U8	Diagnostic_Command : 1;	/* Execute device diagnostic command */
+	MV_U8	Reset : 1;		/* Device reset command */
+	MV_U8	Is_Slave : 1;		/* 0 for master and 1 for slave */
+#endif /* __MV_BIG_ENDIAN_BITFIELD__ */
+	MV_U16	PRD_Entry_Count;
+
+
+	MV_U32	Reserved0;
+	MV_U32	Table_Address;
+	MV_U32	Table_Address_High;
+
+	MV_U32	Reserved3[4];
+} MV_PATA_Command_Header, *PMV_PATA_Command_Header;
+
+/* SATA Command Table: same with AHCI */
+typedef struct _MV_Command_Table
+{
+	MV_U8	FIS[64];								/* Command FIS */
+	MV_U8	ATAPI_CDB[32];							/* ATAPI CDB */
+	MV_U8	Reserve0[32];
+	MV_SG_Entry PRD_Entry[MAX_SG_ENTRY];		/* 32 */
+} MV_Command_Table, *PMV_Command_Table;
+
+#define DIMMSGTABLE_SIZE sizeof(AHCI_DIMM_SG_TABLE)
+
+#define	MV_PCI_BAR			 5
+#define	MV_CMD_ATAPI		 (1L << 5)
+#define	MV_CMD_WRITE		 (1L << 6)
+
+#define	RX_FIS_D2H_REG		 0x40	/* offset of D2H Register FIS data */
+
+	/* global controller registers */
+#define	HOST_CAP			 0x00 	/* host capabilities */
+#define	HOST_CTL			 0x04	/* global host control */
+#define	HOST_IRQ_STAT		 0x08 	/* interrupt status */
+#define	HOST_PORTS_IMPL		 0x0c 	/* bitmap of implemented ports */
+#define	HOST_VERSION		 0x10 	/* AHCI spec. version compliancy */
+
+	/* HOST_CTL bits */
+#define	HOST_RESET			(1L << 0)  /* reset controller; self-clear */
+#define	HOST_IRQ_EN		 	(1L << 1)  /* global IRQ enable */
+#define	HOST_MVL_EN		 	(1L << 31) /* AHCI enabled */
+
+	/* HOST_CAP bits */
+#define	HOST_CAP_64			(1L << 31) /* PCI DAC (64-bit DMA) support */
+
+	/* Vendor specific register */
+#define VENDOR_DETECT		0xA4	/* PATA device/PM detection */
+	/* VENDOR_DETECT bits */
+#define VENDOR_DETECT_PATA	(1L << 10)	/* PATA device detection (bit10) (0 - default) */
+#define VENDOR_DETECT_PM	(1L << 11)	/* PM device detection (bit11) (0 - default) */
+
+	/* registers for each SATA port */
+#define	PORT_LST_ADDR		0x00 /* command list DMA addr */
+#define	PORT_LST_ADDR_HI	0x04 /* command list DMA addr hi */
+#define	PORT_FIS_ADDR		0x08 /* FIS rx buf addr */
+#define	PORT_FIS_ADDR_HI	0x0c /* FIS rx buf addr hi */
+#define	PORT_IRQ_STAT		0x10 /* interrupt status */
+#define	PORT_IRQ_MASK		0x14 /* interrupt enable/disable mask */
+#define	PORT_CMD			0x18 /* port command */
+
+	/* For SATA port */
+#define	PORT_TFDATA			0x20	/* taskfile data */
+#define	PORT_SIG			0x24	/* device TF signature */
+#define	PORT_CMD_ISSUE		0x38 	/* command issue */
+#define	PORT_FIFO_CTL		0x44	/* vendor unique FIFO control */
+#define	PORT_SCR			0x28 	/* SATA phy register block */
+#define	PORT_SCR_STAT		0x28 	/* SATA phy register: SStatus */
+#define	PORT_SCR_CTL		0x2c 	/* SATA phy register: SControl */
+#define	PORT_SCR_ERR		0x30 	/* SATA phy register: SError */
+#define	PORT_SCR_ACT		0x34 	/* SATA phy register: SActive */
+#define	PORT_PM_FIS_0		0x3c	/* port multiplier FIS content 0 */
+#define	PORT_PM_FIS_1		0x40	/* port multiplier FIS content 1 */
+
+/* #if (VER_OEM==VER_OEM_ASUS) */
+#define	PORT_VSR_ADDR		0x78	/* port Vendor Specific Register Address */
+#define	PORT_VSR_DATA		0x7c	/* port Vendor Specific Register Data */
+/* #endif */
+#define VS_REG_SIG			0xab
+
+	/* For PATA port */
+#define	PORT_MASTER_TF0		0x20
+#define	PORT_MASTER_TF1		0x24
+#define	PORT_MASTER_TF2		0x28
+#define	PORT_SLAVE_TF0		0x30
+#define	PORT_SLAVE_TF1		0x3c
+#define	PORT_SLAVE_TF2		0x40
+#define	PORT_INTERNAL_STATE_MACHINE	0x48
+
+
+#ifdef AHCI
+	/* PORT_IRQ_{STAT,MASK} bits */
+#define	PORT_IRQ_COLD_PRES		(1L << 31)	/* cold presence detect */
+#define	PORT_IRQ_TF_ERR			(1L << 30)	/* task file error */
+#define	PORT_IRQ_HBUS_ERR		(1L << 29)	/* host bus fatal error */
+#define	PORT_IRQ_HBUS_DATA_ERR	(1L << 28)	/* host bus data error */
+#define	PORT_IRQ_IF_ERR			(1L << 27)	/* interface fatal error */
+#define	PORT_IRQ_IF_NONFATAL	(1L << 26)	/* interface non-fatal error */
+#define	PORT_IRQ_OVERFLOW		(1L << 24)	/* xfer exhausted available S/G */
+#define	PORT_IRQ_BAD_PMP		(1L << 23)	/* incorrect port multiplier */
+
+#define	PORT_IRQ_PHYRDY			(1L << 22)	 /* PhyRdy changed */
+#define PORT_IRQ_ASYNC_NOTIF	(1L << 20)	 /* Asynchronous Notification, SDB FIS */
+#define	PORT_IRQ_DEV_ILCK		(1L << 7)		/* device interlock */
+#define	PORT_IRQ_CONNECT		(1L << 6)		/* port connect change status */
+#define	PORT_IRQ_SG_DONE		(1L << 5)		/* descriptor processed */
+#define	PORT_IRQ_UNK_FIS		(1L << 4)		/* unknown FIS rx'd */
+#define	PORT_IRQ_SDB_FIS		(1L << 3)		/* Set Device Bits FIS rx'd */
+#define	PORT_IRQ_DMAS_FIS		(1L << 2)		/* DMA Setup FIS rx'd */
+#define	PORT_IRQ_PIOS_FIS		(1L << 1)		/* PIO Setup FIS rx'd */
+#define	PORT_IRQ_D2H_REG_FIS	(1L << 0)		/* D2H Register FIS rx'd */
+
+#define	PORT_IRQ_FATAL	(PORT_IRQ_TF_ERR |\
+						PORT_IRQ_HBUS_ERR |\
+						PORT_IRQ_HBUS_DATA_ERR |\
+						PORT_IRQ_IF_ERR)
+#define	DEF_PORT_IRQ	(PORT_IRQ_FATAL | PORT_IRQ_PHYRDY |\
+						PORT_IRQ_CONNECT | PORT_IRQ_SG_DONE |\
+						PORT_IRQ_UNK_FIS | PORT_IRQ_SDB_FIS |\
+						PORT_IRQ_DMAS_FIS | PORT_IRQ_PIOS_FIS |\
+						PORT_IRQ_D2H_REG_FIS)
+#else
+	/* PORT_IRQ_{STAT,MASK} bits for SATA port */
+#define	PORT_IRQ_SIGNATURE_FIS	(1L << 31)	/* Signature FIS received */
+#define	PORT_IRQ_TF_ERR			(1L << 30)	/* task file error */
+#define	PORT_IRQ_PHYRDY			(1L << 22)	/* PhyRdy changed */
+
+#define	PORT_IRQ_BIST			(1L << 21)	/* BIST activate FIS received */
+#define	PORT_IRQ_ASYNC_NOTIF	(1L << 20)	/* Asynchronous notification received */
+#define	PORT_IRQ_LINK_RECEIVE_ERROR	(1L << 7)
+#define	PORT_IRQ_LINK_TRANSMIT_ERROR (1L << 6)
+#define	PORT_IRQ_PIO_DONE		(1L << 5)		/* PIO Data-in Done */
+#define	PORT_IRQ_UNK_FIS		(1L << 4)		/* unknown FIS rx'd */
+#define	PORT_IRQ_SDB_FIS		(1L << 3)		/* Set Device Bits FIS rx'd */
+#define	PORT_IRQ_DMAS_FIS		(1L << 2)		/* DMA Setup FIS rx'd */
+#define	PORT_IRQ_PIOS_FIS		(1L << 1)		/* PIO Setup FIS rx'd */
+#define	PORT_IRQ_D2H_REG_FIS	(1L << 0)		/* D2H Register FIS rx'd */
+
+#if 0
+#define	DEF_PORT_IRQ		 (MV_U32)(\
+				PORT_IRQ_SIGNATURE_FIS | PORT_IRQ_TF_ERR |\
+				PORT_IRQ_PHYRDY | \
+				PORT_IRQ_BIST |	PORT_IRQ_ASYNC_NOTIF | \
+				PORT_IRQ_LINK_RECEIVE_ERROR | PORT_IRQ_LINK_TRANSMIT_ERROR |\
+				PORT_IRQ_PIO_DONE | PORT_IRQ_UNK_FIS | PORT_IRQ_SDB_FIS | PORT_IRQ_D2H_REG_FIS)
+#else
+#define	DEF_PORT_IRQ		 (MV_U32)(\
+				PORT_IRQ_SIGNATURE_FIS | PORT_IRQ_TF_ERR |\
+				PORT_IRQ_PHYRDY | \
+				PORT_IRQ_BIST |	PORT_IRQ_ASYNC_NOTIF | \
+				PORT_IRQ_PIO_DONE | PORT_IRQ_UNK_FIS | PORT_IRQ_SDB_FIS | PORT_IRQ_D2H_REG_FIS)
+#endif
+
+	/* PORT_IRQ_{STAT,MASK} bits for PATA port */
+#define	PORT_IRQ_PATA_DEVICE0_DONE	MV_BIT(0)
+#define	PORT_IRQ_PATA_DEVICE0_ERROR	MV_BIT(1)
+#define	PORT_IRQ_PATA_DEVICE1_DONE	MV_BIT(2)
+#define	PORT_IRQ_PATA_DEVICE1_ERROR	MV_BIT(3)
+#define	DEF_PORT_PATA_IRQ	(PORT_IRQ_PATA_DEVICE0_DONE | PORT_IRQ_PATA_DEVICE0_ERROR\
+								| PORT_IRQ_PATA_DEVICE1_DONE | PORT_IRQ_PATA_DEVICE1_ERROR)
+#endif
+
+#ifdef AHCI
+	/* PORT_CMD bits */
+#define	PORT_CMD_LIST_ON		(1L << 15)	/* cmd list DMA engine running */
+#define	PORT_CMD_FIS_ON			(1L << 14)	/* FIS DMA engine running */
+#define	PORT_CMD_FIS_RX			(1L << 4)		/* Enable FIS receive DMA engine */
+#define	PORT_CMD_POWER_ON		(1L << 2)		/* Power up device */
+#define	PORT_CMD_SPIN_UP		(1L << 1)		/* Spin up device */
+#define	PORT_CMD_START			(1L << 0)		/* Enable port DMA engine */
+
+#define	PORT_CMD_ICC_ACTIVE		(0x1L << 28)	/* Put i/f in active state */
+#define	PORT_CMD_ICC_PARTIAL	(0x2L << 28)	/* Put i/f in partial state */
+#define	PORT_CMD_ICC_SLUMBER	(0x6L << 28)	/* Put i/f in slumber state */
+#else
+	/* PORT_CMD bits for SATA port */
+#define	PORT_CMD_LIST_ON		(1L << 15)	/* cmd list DMA engine running */
+#define	PORT_CMD_FIS_ON			(1L << 14)	/* FIS DMA engine running */
+
+#define	PORT_CMD_FIS_RX			(1L << 4)		/* Enable FIS receive DMA engine */
+#define	PORT_CMD_START			(1L << 0)		/* Enable port DMA engine */
+
+	/* PORT_CMD bits for PATA port */
+#define	PORT_CMD_PATA_LIST_ON	MV_BIT(15)
+#define	PORT_CMD_PATA_HARD_RESET	MV_BIT(3)
+#define	PORT_CMD_PATA_INTERRUPT MV_BIT(1)
+#define	PORT_CMD_PATA_START		MV_BIT(0)
+#endif
+
+#define	PORT_SSTATUS_IPM_NO_DEVICE	0x0L	/* IPM: device not present or communication not established */
+#define	PORT_SSTATUS_IPM_ACTIVE		0x1L	/* IPM: Interface in active state */
+#define	PORT_SSTATUS_IPM_PARTIAL	0x2L	/* IPM: Interface in partical power management state */
+#define	PORT_SSTATUS_IPRM_SLUMBER	0x6L	/* IPM: Interface in slumber power management state */
+
+#define	PORT_TF_STATUS_BSY	(1L<<7)	/* Task file status: BSY */
+#define	PORT_TF_STATUS_DRQ 	(1L<<3)	/* Task file status: DRQ */
+#define	PORT_TF_STATUS_ERR 	(1L<<0)	/* Task file status: ERR */
+
+
+typedef enum _MV_QUEUE_COMMAND_RESULT
+{
+    MV_QUEUE_COMMAND_RESULT_FINISHED = 0,
+    MV_QUEUE_COMMAND_RESULT_FULL,
+    MV_QUEUE_COMMAND_RESULT_SENT,
+} MV_QUEUE_COMMAND_RESULT;
+
+#define SATA_GetCommandHeader(pPort, slot)	\
+	((PMV_Command_Header)pPort->Cmd_List + slot)
+
+#define PATA_GetCommandHeader(pPort, slot)	\
+	((PMV_PATA_Command_Header)pPort->Cmd_List + slot)
+
+#define Port_GetCommandTable(pPort, slot)	\
+	((PMV_Command_Table)((MV_PU8)pPort->Cmd_Table + slot * SATA_CMD_TABLE_SIZE))
+
+#endif /* CORE_SATA_H */
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_xor.c
@@ -0,0 +1,846 @@
+#include "mv_include.h"
+#include "core_inter.h"
+
+#ifndef USE_NEW_SGTABLE
+#ifdef RAID_DRIVER
+#ifdef SOFTWARE_XOR
+
+#ifdef _OS_LINUX
+
+/*
+ * Software XOR operations
+ */
+void mvXORWrite (MV_PVOID This, PMV_XOR_Request pXORReq);
+void mvXORCompare (MV_PVOID This, PMV_XOR_Request pXORReq);
+void mvXORDMA (MV_PVOID This, PMV_XOR_Request pXORReq);
+
+void Core_ModuleSendXORRequest(MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+
+	switch (pXORReq->Request_Type)
+	{
+		case XOR_REQUEST_WRITE:
+			mvXORWrite (pCore, pXORReq);
+			break;
+		case XOR_REQUEST_COMPARE:
+			mvXORCompare (pCore, pXORReq);
+			break;
+		case XOR_REQUEST_DMA:
+			mvXORDMA (pCore, pXORReq);
+			break;
+		default:
+			pXORReq->Request_Status = XOR_STATUS_INVALID_REQUEST;
+			break;
+	}
+	pXORReq->Completion( pXORReq->Cmd_Initiator, pXORReq );
+}
+
+void mvXORInit(
+	PMV_SG_Entry	*pSGPtr,
+	MV_PU32			SGSizePtr,
+	MV_PVOID		*pVirPtr,
+	MV_PU32			SGOffPtr,
+	PMV_SG_Table	SGListPtr,
+	MV_U8			tableCount,
+	MV_PU32			minSizePtr)
+{
+	MV_U8 id;
+	for ( id=0; id<tableCount; id++ ) {
+		pSGPtr[id] = SGListPtr[id].Entry_Ptr;
+		pVirPtr[id] = (MV_PVOID)
+			( (MV_PTR_INTEGER)pSGPtr[id]->Base_Address
+			| (MV_PTR_INTEGER)((_MV_U64)pSGPtr[id]->Base_Address_High<<32) );
+		SGSizePtr[id] = pSGPtr[id]->Size;
+		SGOffPtr[id] = 0;
+		if ( *minSizePtr > SGSizePtr[id] ) *minSizePtr=SGSizePtr[id];
+	}
+}
+
+void mvXORUpdateEntry(
+	PMV_SG_Entry	*pSGPtr,
+	MV_PU32			SGSizePtr,
+	MV_PVOID		*pVirPtr,
+	MV_PU32			SGOffPtr,
+	MV_U32			finishSize,
+	MV_U8			tableCount,
+	MV_PU32			minSizePtr)
+{
+	MV_U8 id;
+	for ( id=0; id<tableCount; id++ ) {
+		if ( SGSizePtr[id] > finishSize )
+			SGSizePtr[id] -= finishSize;
+		else {
+			pSGPtr[id]++;
+			pVirPtr[id] = (MV_PVOID)
+					( (MV_PTR_INTEGER)pSGPtr[id]->Base_Address
+					| (MV_PTR_INTEGER)((_MV_U64)pSGPtr[id]->Base_Address_High<<32) );
+			SGSizePtr[id] = pSGPtr[id]->Size;
+			SGOffPtr[id] = 0;
+		}
+		if ( *minSizePtr > SGSizePtr[id] ) *minSizePtr=SGSizePtr[id];
+	}
+}
+
+MV_PVOID mv_get_kvaddr(PMV_SG_Entry sg_entry, MV_PVOID addr)
+{
+#ifdef _OS_LINUX
+	MV_U8 map;
+	struct scatterlist *sg = NULL;
+	MV_PVOID kvaddr = NULL;
+	MV_U16 offset = 0;
+
+	sg = (MV_PVOID)((MV_PTR_INTEGER)sg_entry->Base_Address |
+		(MV_PTR_INTEGER)((_MV_U64)sg_entry->Base_Address_High << 32));
+	map = sg_entry->Reserved0;
+	if (map == 0) return sg;
+	MV_ASSERT((sg_entry->Reserved0&0x1) == 1);
+
+	offset = ((sg_entry->Reserved0) >> 16) + sg->offset;
+	kvaddr = kmap_atomic(sg->page, KM_IRQ0) + offset;
+	return kvaddr;
+#endif
+#ifdef _OS_WINDOWS
+	return addr;
+#endif
+}
+
+MV_VOID mv_put_kvaddr(PMV_SG_Entry sg_entry, MV_PVOID kvaddr)
+{
+#ifdef _OS_LINUX
+	MV_U8 map;
+	struct scatterlist *sg = NULL;
+	MV_U16 offset = 0;
+
+	map = sg_entry->Reserved0;
+	if (map == 0) return;
+	MV_ASSERT((sg_entry->Reserved0&0x1) == 1);
+
+	sg = (MV_PVOID)((MV_PTR_INTEGER)sg_entry->Base_Address |
+		(MV_PTR_INTEGER)((_MV_U64)sg_entry->Base_Address_High << 32));
+	offset = ((sg_entry->Reserved0) >> 16) + sg->offset;
+	flush_dcache_page(sg->page);
+	kunmap_atomic(kvaddr - offset, KM_IRQ0);
+#endif
+}
+
+MV_VOID mv_save_val_8(PMV_SG_Entry sg_entry, MV_U32 sge_off,
+	MV_PU8 kaddr, MV_U8 val)
+{
+#ifdef _OS_LINUX
+	MV_U8 map;
+	struct scatterlist *sg = NULL;
+	MV_PU8 kvaddr = NULL;
+	MV_U16 offset = 0;
+
+	sg = (MV_PVOID)((MV_PTR_INTEGER)sg_entry->Base_Address |
+		(MV_PTR_INTEGER)((_MV_U64)sg_entry->Base_Address_High << 32));
+
+	map = sg_entry->Reserved0;
+	if (map == 0) {
+		MV_ASSERT((sg == (struct scatterlist *)kaddr));
+		*(kaddr + sge_off) = val;
+		return;
+	}
+
+	offset = ((sg_entry->Reserved0) >> 16) + sg->offset + sge_off;
+	kvaddr = kmap_atomic(sg->page, KM_IRQ0) + offset;
+	*(kvaddr) = val;
+	kunmap_atomic(kvaddr - offset, KM_IRQ0);
+#endif
+#ifdef _OS_WINDOWS
+	*(kaddr + sge_off) = val;
+#endif
+}
+
+#ifdef SUPPORT_XOR_DWORD
+static MV_VOID mv_save_val_32(PMV_SG_Entry sg_entry, MV_U32 sge_off,
+	MV_PU32 kaddr, MV_U32 val)
+{
+#ifdef _OS_LINUX
+	MV_U8 map;
+	struct scatterlist *sg = NULL;
+	MV_PU32 kvaddr = NULL;
+	MV_U16 offset = 0;
+
+	sg = (MV_PVOID)((MV_PTR_INTEGER)sg_entry->Base_Address |
+		(MV_PTR_INTEGER)((_MV_U64)sg_entry->Base_Address_High << 32));
+
+	map = sg_entry->Reserved0;
+	if (map == 0) {
+		MV_ASSERT((sg == (struct scatterlist *)kaddr));
+		*(kaddr + sge_off) = val;
+		return;
+	}
+
+	offset = ((sg_entry->Reserved0) >> 16) + sg->offset;
+	kvaddr = kmap_atomic(sg->page, KM_IRQ0) + offset;
+	kvaddr += sge_off;
+	*(kvaddr) = val;
+	kvaddr -= sge_off;
+	kvaddr = (MV_PU32)((MV_PU8)kvaddr - offset);
+	kunmap_atomic(kvaddr, KM_IRQ0);
+#endif
+#ifdef _OS_WINDOWS
+	*(kaddr + sge_off) = val;
+#endif
+}
+#endif /* SUPPORT_XOR_DWORD */
+
+MV_U8 mvXORByte(
+	PMV_SG_Entry	*sg_entry,
+	MV_PU8			*pSourceVirPtr,
+	PMV_XOR_Request	pXORReq,
+	MV_U8			tId,
+	MV_PU32			sge_off
+)
+{
+	MV_U8 xorResult, sId;
+	MV_PU8 addr;
+
+	addr = mv_get_kvaddr(sg_entry[0], pSourceVirPtr[0]);
+	xorResult = GF_Multiply(*(addr + sge_off[0]), pXORReq->Coef[tId][0]);
+	mv_put_kvaddr(sg_entry[0], addr);
+
+	for ( sId=1; sId<pXORReq->Source_SG_Table_Count; sId++ ) {
+		addr = mv_get_kvaddr(sg_entry[sId], pSourceVirPtr[sId]);
+		xorResult = GF_Add(xorResult,
+			GF_Multiply(*(addr + sge_off[sId]), pXORReq->Coef[tId][sId]));
+		mv_put_kvaddr(sg_entry[sId], addr);
+	}
+	return xorResult;
+}
+
+#ifdef SUPPORT_XOR_DWORD
+MV_U32 mvXORDWord(
+	PMV_SG_Entry	*sg_entry,
+	MV_PU32			*pSourceVirPtr,
+	PMV_XOR_Request	pXORReq,
+	MV_U8			tId,
+	MV_PU32			sge_off
+)
+{
+	MV_U8	sId;
+	MV_U32 xorResult;
+	MV_PU32 addr = NULL;
+
+	addr = mv_get_kvaddr(sg_entry[0], pSourceVirPtr[0]);
+	xorResult = GF_Multiply(*(addr + sge_off[0]), pXORReq->Coef[tId][0]);
+	mv_put_kvaddr(sg_entry[0], addr);
+
+	for ( sId=1; sId<pXORReq->Source_SG_Table_Count; sId++ ) {
+		addr = mv_get_kvaddr(sg_entry[sId], pSourceVirPtr[sId]);
+		xorResult = GF_Add(xorResult,
+			GF_Multiply(*(addr + sge_off[sId]), pXORReq->Coef[tId][sId]));
+		mv_put_kvaddr(sg_entry[sId], addr);
+	}
+	return xorResult;
+}
+#endif /* SUPPORT_XOR_DWORD */
+
+/* The SG Table should have the virtual address instead of the physical address. */
+void mvXORWrite(MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	PMV_SG_Entry	pSourceSG[XOR_SOURCE_SG_COUNT];
+	PMV_SG_Entry	pTargetSG[XOR_TARGET_SG_COUNT];
+	MV_U32			sourceSize[XOR_SOURCE_SG_COUNT];
+	MV_U32			targetSize[XOR_TARGET_SG_COUNT];
+	MV_U32			sourceOff[XOR_SOURCE_SG_COUNT];
+	MV_U32			targetOff[XOR_TARGET_SG_COUNT];
+	MV_U32 i;
+	/* source index and target index. */
+	MV_U8 sId,tId;
+	MV_U32 size, remainSize, minSize;
+#ifdef SUPPORT_XOR_DWORD
+	MV_PU32			pSourceVir[XOR_SOURCE_SG_COUNT];
+	MV_PU32			pTargetVir[XOR_TARGET_SG_COUNT];
+	MV_U32			xorResult, Dword_size;
+#else
+	MV_PU8			pSourceVir[XOR_SOURCE_SG_COUNT];
+	MV_PU8			pTargetVir[XOR_TARGET_SG_COUNT];
+	MV_U8			xorResult;
+#endif
+
+	/* Initialize these two variables. */
+	/* All the SG table should have same Byte_Count */
+	remainSize = pXORReq->Source_SG_Table_List[0].Byte_Count;
+	minSize = remainSize;
+	/* Initialize XOR source */
+	mvXORInit(pSourceSG, sourceSize, (MV_PVOID)pSourceVir, sourceOff,
+			  pXORReq->Source_SG_Table_List,
+			  pXORReq->Source_SG_Table_Count,
+			  &minSize);
+	/* Initialize XOR target */
+	mvXORInit(pTargetSG, targetSize, (MV_PVOID)pTargetVir, targetOff,
+			  pXORReq->Target_SG_Table_List,
+			  pXORReq->Target_SG_Table_Count,
+			  &minSize);
+
+	/* Navigate all the SG table, calculate the target xor value. */
+	while ( remainSize>0 )
+	{
+		size = minSize;
+#ifdef SUPPORT_XOR_DWORD
+		MV_DASSERT( !(size%4) );
+		Dword_size = size/4;
+		for ( i=0; i<Dword_size; i++ )
+#else
+		for ( i=0; i<size; i++ )
+#endif
+		{
+			for ( tId=0; tId<pXORReq->Target_SG_Table_Count; tId++ )
+			{
+#ifdef SUPPORT_XOR_DWORD
+				xorResult = mvXORDWord(pSourceSG, pSourceVir, pXORReq, tId, sourceOff);
+				mv_save_val_32(pTargetSG[tId], targetOff[tId], pTargetVir[tId], xorResult);
+				targetOff[tId]++;
+#else
+				xorResult = mvXORByte(pSourceSG, pSourceVir, pXORReq, tId, sourceOff);
+				mv_save_val_8(pTargetSG[tId], targetOff[tId], pTargetVir[tId], xorResult);
+				targetOff[tId]++;
+#endif
+			}
+
+			for ( sId=0; sId<pXORReq->Source_SG_Table_Count; sId++ )
+				sourceOff[sId]++;
+		}
+
+		/* Update entry pointer, size */
+		MV_DASSERT( remainSize>=size );
+		remainSize -= size;
+		minSize = remainSize;
+		/* Update XOR source */
+		mvXORUpdateEntry(pSourceSG, sourceSize, (MV_PVOID)pSourceVir, sourceOff,
+						 size, pXORReq->Source_SG_Table_Count, &minSize);
+		/* Update XOR target */
+		mvXORUpdateEntry(pTargetSG, targetSize, (MV_PVOID)pTargetVir, targetOff,
+						 size, pXORReq->Target_SG_Table_Count, &minSize);
+	}
+
+	pXORReq->Request_Status = XOR_STATUS_SUCCESS;
+}
+
+/* consolidate compare and write */
+void mvXORCompare(MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	PMV_SG_Entry	pSourceSG[XOR_SOURCE_SG_COUNT];
+	MV_U32			sourceSize[XOR_SOURCE_SG_COUNT];
+	MV_U32			sourceOff[XOR_SOURCE_SG_COUNT];
+	MV_U32			totalSize, remainSize, minSize, size, i;
+	MV_U8			sId;
+#ifdef SUPPORT_XOR_DWORD
+	MV_PU32			pSourceVir[XOR_SOURCE_SG_COUNT];
+	MV_U32			xorResult, Dword_size;
+#else
+	MV_PU8			pSourceVir[XOR_SOURCE_SG_COUNT];
+	MV_U8			xorResult;
+#endif
+
+	/* All the SG table should have same Byte_Count */
+	totalSize = remainSize = minSize = pXORReq->Source_SG_Table_List[0].Byte_Count;
+	mvXORInit(pSourceSG, sourceSize, (MV_PVOID)pSourceVir, sourceOff,
+			  pXORReq->Source_SG_Table_List,
+			  pXORReq->Source_SG_Table_Count,
+			  &minSize);
+	while ( remainSize>0 ) {
+		size = minSize;
+#ifdef SUPPORT_XOR_DWORD
+		MV_DASSERT( !(size%4) );
+		Dword_size = size/4;
+		for ( i=0; i<Dword_size; i++ ) {
+			xorResult = mvXORDWord(pSourceSG, pSourceVir, pXORReq, 0, sourceOff);
+#else
+		for ( i=0; i<size; i++ ) {
+			xorResult = mvXORByte(pSourceSG, pSourceVir, pXORReq, 0, sourceOff);
+#endif
+			if (xorResult != 0) {
+				pXORReq->Request_Status = XOR_STATUS_ERROR;
+#ifdef SUPPORT_XOR_DWORD
+				pXORReq->Error_Offset = totalSize - remainSize + i*4;
+#else
+				pXORReq->Error_Offset = totalSize - remainSize + i;
+#endif
+				return;
+			}
+			for ( sId=0; sId<pXORReq->Source_SG_Table_Count; sId++ )
+				sourceOff[sId]++;
+		}
+
+		/* Update entry pointer, size */
+		MV_DASSERT( remainSize>=size );
+		remainSize -= size;
+		minSize = remainSize;
+		mvXORUpdateEntry(pSourceSG, sourceSize, (MV_PVOID)pSourceVir, sourceOff,
+						 size, pXORReq->Source_SG_Table_Count, &minSize);
+	}
+}
+
+void mvXORDMA(MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	MV_ASSERT( MV_FALSE );
+}
+
+#endif /* _OS_LINUX */
+
+#ifdef _OS_WINDOWS
+
+/*
+ * Software XOR operations
+ */
+
+void mvXORWrite (MV_PVOID This, PMV_XOR_Request pXORReq);
+void mvXORCompare (MV_PVOID This, PMV_XOR_Request pXORReq);
+void mvXORDMA (MV_PVOID This, PMV_XOR_Request pXORReq);
+
+#ifndef SUPPORT_RAID6
+MV_U8 mvXORInitArray (
+	MV_PVOID This,
+	PMV_XOR_Request pXORReq,
+	PMV_SG_Entry *SG_entry,
+	MV_PU32 SG_size,
+	MV_PU8 *pSource,
+	MV_PU32 table_size);
+#endif
+
+#ifdef SIMULATOR
+void _Core_ModuleSendXORRequest(MV_PVOID This, PMV_XOR_Request pXORReq)
+#else	/*SIMULATOR*/
+void Core_ModuleSendXORRequest(MV_PVOID This, PMV_XOR_Request pXORReq)
+#endif	/*!SIMULATOR*/
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+
+	switch (pXORReq->Request_Type)
+	{
+		case XOR_REQUEST_WRITE:
+			mvXORWrite (pCore, pXORReq);
+			break;
+		case XOR_REQUEST_COMPARE:
+			mvXORCompare (pCore, pXORReq);
+			break;
+		case XOR_REQUEST_DMA:
+			mvXORDMA (pCore, pXORReq);
+			break;
+		default:
+			pXORReq->Request_Status = XOR_STATUS_INVALID_REQUEST;
+			break;
+	}
+#ifndef SIMULATOR
+	pXORReq->Completion( pXORReq->Cmd_Initiator, pXORReq );
+#endif	/*!SIMULATOR*/
+}
+
+#ifndef SUPPORT_RAID6
+void mvXORWrite (MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	MV_ASSERT(MV_FALSE);
+}
+
+void mvXORCompare (MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+
+//	PMV_SG_Entry SG_entry[MAX_SG_ENTRY + 1];		// last element is target
+	PMV_SG_Entry SG_entry[2];
+//	MV_U32 SG_size[MAX_SG_ENTRY + 1];
+	MV_U32 SG_size[2];	// number of source, currently support 2 sources
+
+	MV_U32 min_size;
+	MV_U32 table_size, total_byte = 0;
+	MV_U8 num_sources, sourceCount;
+	MV_U32 sizeCount;
+	//PMV_SG_Table current_table;
+//	MV_PU8 pSource[MAX_SG_ENTRY];
+	MV_PU8 pSource[2];
+	MV_U8 xorResult;
+
+	num_sources = pXORReq->Source_SG_Table_Count;
+	if (num_sources==0) {
+		pXORReq->Request_Status = XOR_STATUS_INVALID_PARAMETER;
+		return;
+	}
+
+	// init array & error checking
+	if ( mvXORInitArray(pCore, pXORReq, SG_entry, SG_size, pSource, &table_size) != num_sources )
+	{
+		pXORReq->Request_Status = XOR_STATUS_INVALID_PARAMETER;
+		return;
+	}
+
+	while (total_byte < table_size)
+	{
+		min_size = SG_size[0];
+		for (sourceCount=1; sourceCount<num_sources; sourceCount++)
+		{
+			if (min_size > SG_size[sourceCount])
+				min_size = SG_size[sourceCount];
+		}
+
+		for (sizeCount=0; sizeCount<min_size; sizeCount++)
+		{
+			xorResult = *pSource[0];
+			for (sourceCount=1; sourceCount<num_sources; sourceCount++)
+			{
+//				*pSource[0] = *pSource[0] ^ *pSource[sourceCount];
+				xorResult = xorResult ^ *pSource[sourceCount];
+				pSource[sourceCount]++;
+			}
+
+//			if (*pSource[0] != 0)
+			if (xorResult != 0)
+			{
+				pXORReq->Request_Status = XOR_STATUS_ERROR;
+				pXORReq->Error_Offset = total_byte + sizeCount;
+				return;
+			}
+			pSource[0]++;
+		}
+		total_byte += min_size;
+
+		if (total_byte<table_size) {
+			// make new size array & increment entry pointer if needed
+			for (sourceCount=0; sourceCount<num_sources; sourceCount++)
+			{
+				SG_size[sourceCount] -= min_size;
+				if (SG_size[sourceCount] == 0)
+				{
+					SG_entry[sourceCount]++;
+					SG_size[sourceCount] = SG_entry[sourceCount]->Size;
+					pSource[sourceCount] = raid_physical_to_virtual(pXORReq->Cmd_Initiator,
+											  SG_entry[sourceCount]->Base_Address);
+				}
+			}
+		}
+	}
+
+	pXORReq->Request_Status = XOR_STATUS_SUCCESS;
+}
+
+void mvXORDMA (MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	MV_ASSERT(MV_FALSE);
+}
+
+MV_U8 mvXORInitArray (
+	MV_PVOID This,
+	PMV_XOR_Request pXORReq,
+	PMV_SG_Entry *SG_entry,
+	MV_PU32 SG_size,
+	MV_PU8 *pSource,
+	MV_PU32 table_size)
+{
+	//PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	PMV_SG_Table current_table = NULL;
+	//MV_U8 j;
+	MV_U8 i = 0;
+
+	*table_size = 0;
+	for (i=0; i<pXORReq->Source_SG_Table_Count; i++) {
+//	while ( !List_Empty(&pXORReq->Source_SG_Table_List) )
+//	{
+//		current_table = (PMV_SG_Table)List_GetFirstEntry(&pXORReq->Source_SG_Table_List, MV_XOR_Request, Queue_Pointer);
+		current_table = &pXORReq->Source_SG_Table_List[i];
+		MV_ASSERT( current_table!=NULL );
+
+		SG_entry[i] = &current_table->Entry_Ptr[0];
+		SG_size[i] = current_table->Entry_Ptr[0].Size;
+//		pSource[i] = (MV_PU8)SG_entry[i]->Base_Address;
+		pSource[i] = raid_physical_to_virtual(pXORReq->Cmd_Initiator, current_table->Entry_Ptr[0].Base_Address);
+
+//		i++;
+	}
+	MV_DASSERT( current_table!=NULL );
+	if ( current_table ) *table_size = current_table->Byte_Count;
+	return i;
+}
+
+#else /* SUPPORT_RAID6 */
+
+void mvXORInit(
+	PMV_SG_Entry	*pSGPtr,
+	MV_PU32			SGSizePtr,
+	MV_PVOID		*pVirPtr,
+	PMV_SG_Table	SGListPtr,
+	MV_U8			tableCount,
+	MV_PU32			minSizePtr)
+{
+	MV_U8 id;
+	for ( id=0; id<tableCount; id++ ) {
+		pSGPtr[id] = SGListPtr[id].Entry_Ptr;
+		pVirPtr[id] = (MV_PVOID)
+			( (MV_PTR_INTEGER)pSGPtr[id]->Base_Address
+			| (MV_PTR_INTEGER)pSGPtr[id]->Base_Address_High<<32 );
+		SGSizePtr[id] = pSGPtr[id]->Size;
+		if ( *minSizePtr > SGSizePtr[id] ) *minSizePtr=SGSizePtr[id];
+	}
+}
+
+void mvXORUpdateEntry(
+	PMV_SG_Entry	*pSGPtr,
+	MV_PU32			SGSizePtr,
+	MV_PVOID		*pVirPtr,
+	MV_U32			finishSize,
+	MV_U8			tableCount,
+	MV_PU32			minSizePtr)
+{
+	MV_U8 id;
+	for ( id=0; id<tableCount; id++ ) {
+		if ( SGSizePtr[id] > finishSize )
+			SGSizePtr[id] -= finishSize;
+		else {
+			pSGPtr[id]++;
+			pVirPtr[id] = (MV_PVOID)
+					( (MV_PTR_INTEGER)pSGPtr[id]->Base_Address
+					| (MV_PTR_INTEGER)pSGPtr[id]->Base_Address_High<<32 );
+			SGSizePtr[id] = pSGPtr[id]->Size;
+		}
+		if ( *minSizePtr > SGSizePtr[id] ) *minSizePtr=SGSizePtr[id];
+	}
+}
+
+MV_U8 mvXORByte(
+	MV_PU8			*pSourceVirPtr,
+	PMV_XOR_Request	pXORReq,
+	MV_U8			tId
+)
+{
+	MV_U8 xorResult, sId;
+
+	xorResult = GF_Multiply(*pSourceVirPtr[0], pXORReq->Coef[tId][0]);
+	for ( sId=1; sId<pXORReq->Source_SG_Table_Count; sId++ ) {
+		xorResult = GF_Add(xorResult,
+						   GF_Multiply(*pSourceVirPtr[sId], pXORReq->Coef[tId][sId]));
+	}
+	return xorResult;
+}
+
+#ifdef SUPPORT_XOR_DWORD
+MV_U32 mvXORDWord(
+	MV_PU32			*pSourceVirPtr,
+	PMV_XOR_Request	pXORReq,
+	MV_U8			tId
+)
+{
+	MV_U8	sId;
+	MV_U32 xorResult;
+
+	xorResult = GF_Multiply(*pSourceVirPtr[0], pXORReq->Coef[tId][0]);
+	for ( sId=1; sId<pXORReq->Source_SG_Table_Count; sId++ ) {
+		xorResult = GF_Add(xorResult,
+						   GF_Multiply(*pSourceVirPtr[sId], pXORReq->Coef[tId][sId]));
+	}
+	return xorResult;
+}
+#endif /* SUPPORT_XOR_DWORD */
+
+/* The SG Table should have the virtual address instead of the physical address. */
+void mvXORWrite(MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	PMV_SG_Entry	pSourceSG[XOR_SOURCE_SG_COUNT];
+	PMV_SG_Entry	pTargetSG[XOR_TARGET_SG_COUNT];
+	MV_U32			sourceSize[XOR_SOURCE_SG_COUNT];
+	MV_U32			targetSize[XOR_TARGET_SG_COUNT];
+	MV_U32 i;
+	MV_U8 sId,tId;									/* source index and target index. */
+	MV_U32 size, remainSize, minSize;
+#ifdef SUPPORT_XOR_DWORD
+	MV_PU32			pSourceVir[XOR_SOURCE_SG_COUNT];
+	MV_PU32			pTargetVir[XOR_TARGET_SG_COUNT];
+	MV_U32			xorResult, Dword_size;
+#else
+	MV_PU8			pSourceVir[XOR_SOURCE_SG_COUNT];
+	MV_PU8			pTargetVir[XOR_TARGET_SG_COUNT];
+	MV_U8			xorResult;
+#endif
+
+	/* Initialize these two variables. */
+	remainSize = pXORReq->Source_SG_Table_List[0].Byte_Count;	/* All the SG table should have same Byte_Count */
+	minSize = remainSize;
+	/* Initialize XOR source */
+	mvXORInit(pSourceSG, sourceSize, (MV_PVOID*)pSourceVir,
+			  pXORReq->Source_SG_Table_List,
+			  pXORReq->Source_SG_Table_Count,
+			  &minSize);
+	/* Initialize XOR target */
+	mvXORInit(pTargetSG, targetSize, (MV_PVOID*)pTargetVir,
+			  pXORReq->Target_SG_Table_List,
+			  pXORReq->Target_SG_Table_Count,
+			  &minSize);
+
+/*
+	for ( sId=0; sId<pXORReq->Source_SG_Table_Count; sId++ )
+	{
+		pSourceSG[sId] = pXORReq->Source_SG_Table_List[sId].Entry;
+		sourceSize[sId] = pSourceSG[sId]->Size;
+		pSourceVir[sId] = (MV_PVOID)
+			( (MV_PTR_INTEGER)pSourceSG[sId]->Base_Address
+			| (MV_PTR_INTEGER)pSourceSG[sId]->Base_Address_High<<32 );
+		MV_DASSERT( remainSize==pXORReq->Source_SG_Table_List[sId].Byte_Count );
+		if ( minSize>sourceSize[sId] ) minSize=sourceSize[sId];
+	}
+
+	for ( tId=0; tId<pXORReq->Target_SG_Table_Count; tId++ )
+	{
+		pTargetSG[tId] = pXORReq->Target_SG_Table_List[tId].Entry;
+		targetSize[tId] = pTargetSG[tId]->Size;
+		pTargetVir[tId] = (MV_PVOID)
+			( (MV_PTR_INTEGER)pTargetSG[tId]->Base_Address
+			| (MV_PTR_INTEGER)pTargetSG[tId]->Base_Address_High<<32 );
+		MV_DASSERT( remainSize==pXORReq->Target_SG_Table_List[tId].Byte_Count );
+		if ( minSize>targetSize[tId] ) minSize=targetSize[tId];
+	}
+*/
+
+	/* Navigate all the SG table, calculate the target xor value. */
+	while ( remainSize>0 )
+	{
+		size = minSize;
+#ifdef SUPPORT_XOR_DWORD
+		MV_DASSERT( !(size%4) );
+		Dword_size = size/4;
+		for ( i=0; i<Dword_size; i++ )
+#else
+		for ( i=0; i<size; i++ )
+#endif
+		{
+			for ( tId=0; tId<pXORReq->Target_SG_Table_Count; tId++ )
+			{
+#ifdef SUPPORT_XOR_DWORD
+				xorResult = mvXORDWord(pSourceVir, pXORReq, tId);
+#else
+				xorResult = mvXORByte(pSourceVir, pXORReq, tId);
+#endif
+
+/*
+				tmp = GF_Multiply(*pSourceVir[0], pXORReq->Coef[tId][0]);
+
+				for ( sId=1; sId<pXORReq->Source_SG_Table_Count; sId++ )
+				{
+					tmp = GF_Add(tmp,
+								GF_Multiply(*pSourceVir[sId], pXORReq->Coef[tId][sId]));
+				}
+				*pTargetVir[tId] = tmp;
+*/
+				*pTargetVir[tId] = xorResult;
+				pTargetVir[tId]++;
+			}
+
+			for ( sId=0; sId<pXORReq->Source_SG_Table_Count; sId++ )
+				pSourceVir[sId]++;
+		}
+
+		/* Update entry pointer, size */
+		MV_DASSERT( remainSize>=size );
+		remainSize -= size;
+		minSize = remainSize;
+		/* Update XOR source */
+		mvXORUpdateEntry(pSourceSG, sourceSize, (MV_PVOID*)pSourceVir,
+						 size, pXORReq->Source_SG_Table_Count, &minSize);
+		/* Update XOR target */
+		mvXORUpdateEntry(pTargetSG, targetSize, (MV_PVOID*)pTargetVir,
+						 size, pXORReq->Target_SG_Table_Count, &minSize);
+/*
+
+		for ( sId=0; sId<pXORReq->Source_SG_Table_Count; sId++ )
+		{
+			if ( sourceSize[sId]>size )
+			{
+				sourceSize[sId]-=size;
+			}
+			else
+			{
+				pSourceSG[sId]++;
+				pSourceVir[sId] = (MV_PVOID)
+					( (MV_PTR_INTEGER)pSourceSG[sId]->Base_Address | (MV_PTR_INTEGER)pSourceSG[sId]->Base_Address_High<<32 );
+				sourceSize[sId] = pSourceSG[sId]->Size;
+			}
+			if ( minSize>sourceSize[sId] ) minSize=sourceSize[sId];
+		}
+
+		for ( tId=0; tId<pXORReq->Target_SG_Table_Count; tId++ )
+		{
+			if ( targetSize[tId]>size )
+			{
+				targetSize[tId]-=size;
+			}
+			else
+			{
+				pTargetSG[tId]++;
+				pTargetVir[tId] = (MV_PVOID)
+					( (MV_PTR_INTEGER)pTargetSG[tId]->Base_Address | (MV_PTR_INTEGER)pTargetSG[tId]->Base_Address_High<<32 );
+				targetSize[tId] = pTargetSG[tId]->Size;
+			}
+			if ( minSize>targetSize[tId] ) minSize=targetSize[tId];
+		}
+*/
+	}
+
+	pXORReq->Request_Status = XOR_STATUS_SUCCESS;
+}
+
+//consolidate compare and write
+void mvXORCompare (MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	PMV_SG_Entry	pSourceSG[XOR_SOURCE_SG_COUNT];
+	MV_U32			sourceSize[XOR_SOURCE_SG_COUNT];
+	MV_U32			totalSize, remainSize, minSize, size, i;
+	MV_U8			sId;
+#ifdef SUPPORT_XOR_DWORD
+	MV_PU32			pSourceVir[XOR_SOURCE_SG_COUNT];
+	MV_U32			xorResult, Dword_size;
+#else
+	MV_PU8			pSourceVir[XOR_SOURCE_SG_COUNT];
+	MV_U8			xorResult;
+#endif
+
+	/* All the SG table should have same Byte_Count */
+	totalSize = remainSize = minSize = pXORReq->Source_SG_Table_List[0].Byte_Count;
+	mvXORInit(pSourceSG, sourceSize, (MV_PVOID*)pSourceVir,
+			  pXORReq->Source_SG_Table_List,
+			  pXORReq->Source_SG_Table_Count,
+			  &minSize);
+	while ( remainSize>0 ) {
+		size = minSize;
+#ifdef SUPPORT_XOR_DWORD
+		MV_DASSERT( !(size%4) );
+		Dword_size = size/4;
+		for ( i=0; i<Dword_size; i++ ) {
+			xorResult = mvXORDWord(pSourceVir, pXORReq, 0);
+#else
+		for ( i=0; i<size; i++ ) {
+			xorResult = mvXORByte(pSourceVir, pXORReq, 0);
+#endif
+			if (xorResult != 0)	{
+				pXORReq->Request_Status = XOR_STATUS_ERROR;
+#ifdef SUPPORT_XOR_DWORD
+				pXORReq->Error_Offset = totalSize - remainSize + i*4;
+#else
+				pXORReq->Error_Offset = totalSize - remainSize + i;
+#endif
+				return;
+			}
+			for ( sId=0; sId<pXORReq->Source_SG_Table_Count; sId++ )
+				pSourceVir[sId]++;
+		}
+
+		/* Update entry pointer, size */
+		MV_DASSERT( remainSize>=size );
+		remainSize -= size;
+		minSize = remainSize;
+		mvXORUpdateEntry(pSourceSG, sourceSize, (MV_PVOID*)pSourceVir,
+						 size, pXORReq->Source_SG_Table_Count, &minSize);
+	}
+}
+
+void mvXORDMA (MV_PVOID This, PMV_XOR_Request pXORReq)
+{
+	MV_ASSERT( MV_FALSE );
+}
+
+#endif /* SUPPORT_RAID6 */
+
+#endif /* _OS_WINDOWS */
+
+#endif /* SOFTWARE_XOR */
+#endif /* RAID_DRIVER */
+#endif /* USE_NEW_SGTABLE */
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/core_xor.h
@@ -0,0 +1,10 @@
+#ifndef CORE_XOR_H
+#define CORE_XOR_H
+
+#ifdef RAID_DRIVER
+void Core_ModuleSendXORRequest(MV_PVOID This, PMV_XOR_Request pXORReq);
+void MV_DumpXORRegister(MV_PVOID This);
+MV_BOOLEAN HandleXORQueue(MV_PVOID This);
+#endif /* RAID_DRIVER */
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/core/thor/scsi2sata.c
@@ -0,0 +1,787 @@
+#include "mv_include.h"
+
+#include "core_inter.h"
+
+#include "core_sata.h"
+#include "core_ata.h"
+#include "core_sat.h"
+
+/*
+ * Translate SCSI command to SATA FIS
+ * The following SCSI command set is the minimum required.
+ *		Standard Inquiry
+ *		Read Capacity
+ *		Test Unit Ready
+ *		Start/Stop Unit
+ *		Read 10
+ *		Write 10
+ *		Request Sense
+ *		Mode Sense/Select
+ */
+MV_VOID SCSI_To_FIS(MV_PVOID This, PMV_Request pReq, MV_U8 tag, PATA_TaskFile pTaskFile)
+{
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)This;
+	PDomain_Port pPort = &pCore->Ports[PATA_MapPortId(pReq->Device_Id)];
+	PMV_Command_Table pCmdTable = Port_GetCommandTable(pPort, tag);
+
+	PSATA_FIS_REG_H2D pFIS = (PSATA_FIS_REG_H2D)pCmdTable->FIS;
+#ifdef SUPPORT_PM
+	PDomain_Device pDevice = &pPort->Device[PATA_MapDeviceId(pReq->Device_Id)];
+#endif
+	/*
+	 * 1. SoftReset is not supported yet.
+	 * 2. PM_Port
+	 */
+	pFIS->FIS_Type = SATA_FIS_TYPE_REG_H2D;
+
+#ifdef SUPPORT_PM
+	pFIS->PM_Port = pDevice->PM_Number;
+#else
+	pFIS->PM_Port = 0;
+#endif
+
+	pFIS->C = 1;	/* Update command register rather than devcie control register */
+	pFIS->Command = pTaskFile->Command;
+	pFIS->Features = pTaskFile->Features;
+	pFIS->Device = pTaskFile->Device;
+	pFIS->Control = pTaskFile->Control;
+
+	pFIS->LBA_Low = pTaskFile->LBA_Low;
+	pFIS->LBA_Mid = pTaskFile->LBA_Mid;
+	pFIS->LBA_High = pTaskFile->LBA_High;
+	pFIS->Sector_Count = pTaskFile->Sector_Count;
+
+	/* No matter it's 48bit or not, I've set the values. */
+	pFIS->LBA_Low_Exp = pTaskFile->LBA_Low_Exp;
+	pFIS->LBA_Mid_Exp = pTaskFile->LBA_Mid_Exp;
+	pFIS->LBA_High_Exp = pTaskFile->LBA_High_Exp;
+	pFIS->Features_Exp = pTaskFile->Feature_Exp;
+	pFIS->Sector_Count_Exp = pTaskFile->Sector_Count_Exp;
+}
+
+/* Set MV_Request.Cmd_Flag */
+MV_BOOLEAN Category_CDB_Type(
+	IN PDomain_Device pDevice,
+	IN PMV_Request pReq
+	)
+{
+	PDomain_Port pPort = pDevice->PPort;
+
+	switch ( pReq->Cdb[0] )
+	{
+		case SCSI_CMD_READ_10:
+		case SCSI_CMD_READ_16:
+			pReq->Cmd_Flag |= CMD_FLAG_DATA_IN;
+
+		case SCSI_CMD_WRITE_10:
+		case SCSI_CMD_WRITE_16:
+		case SCSI_CMD_VERIFY_10:
+			/*
+			 *
+			 * CMD_FLAG_DATA_IN
+			 * CMD_FLAG_NON_DATA
+			 * CMD_FLAG_DMA
+			 */
+			if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+				pReq->Cmd_Flag |= CMD_FLAG_PACKET;
+
+			if ( pDevice->Capacity&DEVICE_CAPACITY_48BIT_SUPPORTED )
+				pReq->Cmd_Flag |= CMD_FLAG_48BIT;
+
+			if ( pDevice->Capacity&DEVICE_CAPACITY_NCQ_SUPPORTED )
+			{
+				// might be a PM - assert is no longer true
+				//MV_DASSERT( pPort->Type==PORT_TYPE_SATA );
+#if 0
+				if ( (pReq->Cdb[0]==SCSI_CMD_READ_10)
+					|| (pReq->Cdb[0]==SCSI_CMD_WRITE_10) )
+#else
+				if (SCSI_IS_READ(pReq->Cdb[0]) || SCSI_IS_WRITE(pReq->Cdb[0]))
+#endif
+				{
+					if ( (pPort->Running_Slot==0)
+						|| (pPort->Setting&PORT_SETTING_NCQ_RUNNING) )
+					{
+						/* hardware workaround:
+						 * don't do NCQ on silicon image PM */
+						if( !((pPort->Setting & PORT_SETTING_PM_EXISTING) &&
+							(pPort->PM_Vendor_Id == 0x1095 )) )
+						{
+							if ( pReq->Scsi_Status!=REQ_STATUS_RETRY )
+								pReq->Cmd_Flag |= CMD_FLAG_NCQ;
+						}
+					}
+				}
+			}
+
+			break;
+
+		case SCSI_CMD_MARVELL_SPECIFIC:
+			{
+				/* This request should be for core module */
+				if ( pReq->Cdb[1]!=CDB_CORE_MODULE )
+					return MV_FALSE;
+				switch ( pReq->Cdb[2] )
+				{
+					case CDB_CORE_IDENTIFY:
+					case CDB_CORE_READ_LOG_EXT:
+						pReq->Cmd_Flag |= CMD_FLAG_DATA_IN;
+						break;
+					case CDB_CORE_SET_UDMA_MODE:
+					case CDB_CORE_SET_PIO_MODE:
+					case CDB_CORE_ENABLE_WRITE_CACHE:
+					case CDB_CORE_DISABLE_WRITE_CACHE:
+					case CDB_CORE_ENABLE_READ_AHEAD:
+					case CDB_CORE_DISABLE_READ_AHEAD:
+						pReq->Cmd_Flag |= CMD_FLAG_NON_DATA;
+						break;
+				#ifdef SUPPORT_ATA_SMART
+					case CDB_CORE_ATA_IDENTIFY_PACKET_DEVICE:
+						pReq->Cmd_Flag |= (CMD_FLAG_DATA_IN | CMD_FLAG_SMART);
+						break;
+					case CDB_CORE_ENABLE_SMART:
+					case CDB_CORE_DISABLE_SMART:
+					case CDB_CORE_SMART_RETURN_STATUS:
+					case  CDB_CORE_ATA_SMART_AUTO_OFFLINE:
+					case CDB_CORE_ATA_SMART_AUTOSAVE:
+					case CDB_CORE_ATA_SMART_IMMEDIATE_OFFLINE:
+						pReq->Cmd_Flag |= (CMD_FLAG_NON_DATA | CMD_FLAG_SMART);
+						break;
+					case CDB_CORE_ATA_SMART_READ_VALUES:
+					case CDB_CORE_ATA_SMART_READ_THRESHOLDS:
+					case CDB_CORE_ATA_SMART_READ_LOG_SECTOR:
+						pReq->Cmd_Flag |= (CMD_FLAG_DATA_IN | CMD_FLAG_SMART);
+						break;
+					case CDB_CORE_ATA_SMART_WRITE_LOG_SECTOR:
+						pReq->Cmd_Flag |= (CMD_FLAG_DATA_OUT | CMD_FLAG_SMART);
+						break;
+				#endif
+					case CDB_CORE_ATA_IDENTIFY:
+						pReq->Cmd_Flag |= (CMD_FLAG_DATA_IN | CMD_FLAG_SMART);
+						break;
+					case CDB_CORE_ATA_DOWNLOAD_MICROCODE:
+						break;
+					case CDB_CORE_SHUTDOWN:
+						if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+							return MV_FALSE;
+						MV_DPRINT(("Shutdown on device %d.\n", pReq->Device_Id));
+						pReq->Cmd_Flag |= CMD_FLAG_NON_DATA;
+						break;
+               #ifdef SUPPORT_ATA_POWER_MANAGEMENT
+					case CDB_CORE_ATA_SLEEP:
+					case CDB_CORE_ATA_IDLE:
+					case CDB_CORE_ATA_STANDBY:
+					case CDB_CORE_ATA_IDLE_IMMEDIATE:
+					case CDB_CORE_ATA_CHECK_POWER_MODE:
+					case CDB_CORE_ATA_STANDBY_IMMEDIATE:
+                                               pReq->Cmd_Flag |= CMD_FLAG_NON_DATA;
+                                               break;
+               #endif
+					case CDB_CORE_OS_SMART_CMD:
+						if  ((pReq->Cdb[3] != ATA_CMD_IDENTIFY_ATA) ||
+							(pReq->Cdb[3] != ATA_CMD_IDENTIY_ATAPI))
+							pReq->Cmd_Flag |= CMD_FLAG_SMART;
+						if (pDevice->Device_Type & DEVICE_TYPE_ATAPI)
+							pReq->Cmd_Flag |= CMD_FLAG_PACKET;
+						break;
+					default:
+						return MV_FALSE;
+				}
+				break;
+			}
+#ifdef SUPPORT_ATA_SECURITY_CMD
+			case ATA_16:
+			return MV_TRUE;
+			break;
+#endif
+		case SCSI_CMD_START_STOP_UNIT:
+		case SCSI_CMD_SYNCHRONIZE_CACHE_10:
+			if ( !(pDevice->Device_Type & DEVICE_TYPE_ATAPI )){
+				if ( pDevice->Capacity&DEVICE_CAPACITY_48BIT_SUPPORTED )
+					pReq->Cmd_Flag |= CMD_FLAG_48BIT;
+				pReq->Cmd_Flag |= CMD_FLAG_NON_DATA;
+				break;
+			}	//We will send this command to CD drive directly if it is ATAPI device.
+		case SCSI_CMD_SND_DIAG:
+			pReq->Cmd_Flag |= CMD_FLAG_DATA_OUT;
+		case SCSI_CMD_INQUIRY:
+		case SCSI_CMD_READ_CAPACITY_10:
+		case SCSI_CMD_TEST_UNIT_READY:
+		case SCSI_CMD_MODE_SENSE_10:
+		case SCSI_CMD_MODE_SELECT_10:
+		case SCSI_CMD_PREVENT_MEDIUM_REMOVAL:
+		case SCSI_CMD_READ_TOC:
+		case SCSI_CMD_REQUEST_SENSE:
+		default:
+			if ( pDevice->Device_Type&DEVICE_TYPE_ATAPI )
+			{
+				pReq->Cmd_Flag |= CMD_FLAG_PACKET;
+				break;
+			}
+			else
+			{
+				MV_DPRINT(("Error: Unknown request: 0x%x.\n", pReq->Cdb[0]));
+				return MV_FALSE;
+			}
+	}
+
+	return MV_TRUE;
+}
+
+static void rw16_taskfile(PMV_Request req, PATA_TaskFile tf, int tag)
+{
+	if (req->Cmd_Flag & CMD_FLAG_NCQ) {
+
+		tf->Features = req->Cdb[13];
+		tf->Feature_Exp = req->Cdb[12];
+
+		tf->Sector_Count = tag<<3;
+
+		tf->LBA_Low = req->Cdb[9];
+		tf->LBA_Mid = req->Cdb[8];
+		tf->LBA_High = req->Cdb[7];
+		tf->LBA_Low_Exp = req->Cdb[6];
+		tf->LBA_Mid_Exp = req->Cdb[5];
+		tf->LBA_High_Exp = req->Cdb[4];
+
+		tf->Device = MV_BIT(6);
+
+		if (req->Cdb[0] == SCSI_CMD_READ_16)
+			tf->Command = ATA_CMD_READ_FPDMA_QUEUED;
+		else if (req->Cdb[0] == SCSI_CMD_WRITE_16)
+			tf->Command = ATA_CMD_WRITE_FPDMA_QUEUED;
+	} else if (req->Cmd_Flag & CMD_FLAG_48BIT) {
+		MV_DASSERT(!(req->Cmd_Flag&CMD_FLAG_NCQ));
+
+		tf->Sector_Count = req->Cdb[13];
+		tf->Sector_Count_Exp = req->Cdb[12];
+
+		tf->LBA_Low = req->Cdb[9];
+		tf->LBA_Mid = req->Cdb[8];
+		tf->LBA_High = req->Cdb[7];
+		tf->LBA_Low_Exp = req->Cdb[6];
+		tf->LBA_Mid_Exp = req->Cdb[5];
+		tf->LBA_High_Exp = req->Cdb[4];
+
+		tf->Device = MV_BIT(6);
+
+		if (req->Cdb[0] == SCSI_CMD_READ_16)
+			tf->Command = ATA_CMD_READ_DMA_EXT;
+		else if (req->Cdb[0] == SCSI_CMD_WRITE_16)
+			tf->Command = ATA_CMD_WRITE_DMA_EXT;
+	}
+}
+void scsi_ata_passthru_12_fill_taskfile(MV_Request *req, OUT PATA_TaskFile  taskfile)
+{
+	taskfile->Command = req->Cdb[9];
+	taskfile->Features = req->Cdb[3];
+	taskfile->Device = req->Cdb[8];
+
+	taskfile->LBA_Low = req->Cdb[5];
+	taskfile->LBA_Mid = req->Cdb[6];
+	taskfile->LBA_High = req->Cdb[7];
+	taskfile->Sector_Count = req->Cdb[4];
+	taskfile->Control = req->Cdb[11];
+
+	taskfile->LBA_Low_Exp = 0;
+	taskfile->LBA_Mid_Exp = 0;
+	taskfile->LBA_High_Exp = 0;
+	taskfile->Sector_Count_Exp = 0;
+	taskfile->Feature_Exp = 0;
+}
+
+void scsi_ata_passthru_16_fill_taskfile(MV_Request *req, OUT PATA_TaskFile  taskfile)
+{
+	taskfile->Command = req->Cdb[14];
+	taskfile->Features = req->Cdb[4];
+	taskfile->Device = req->Cdb[13];
+
+	taskfile->LBA_Low = req->Cdb[8];
+	taskfile->LBA_Mid = req->Cdb[10];
+	taskfile->LBA_High = req->Cdb[12];
+	taskfile->Sector_Count = req->Cdb[6];
+	taskfile->Control = req->Cdb[15];
+	if ((req->Cdb[1] & 0x01) ||(req->Cmd_Flag & CMD_FLAG_48BIT)){
+		taskfile->LBA_Low_Exp = req->Cdb[7];
+		taskfile->LBA_Mid_Exp = req->Cdb[9];
+		taskfile->LBA_High_Exp = req->Cdb[11];
+		taskfile->Sector_Count_Exp = req->Cdb[5];
+		taskfile->Feature_Exp = req->Cdb[3];
+	} else {
+		taskfile->LBA_Low_Exp = 0;
+		taskfile->LBA_Mid_Exp = 0;
+		taskfile->LBA_High_Exp = 0;
+		taskfile->Sector_Count_Exp = 0;
+		taskfile->Feature_Exp = 0;
+	}
+}
+
+MV_BOOLEAN ATA_CDB2TaskFile(
+	IN PDomain_Device pDevice,
+	IN PMV_Request pReq,
+	IN MV_U8 tag,
+	OUT PATA_TaskFile pTaskFile
+	)
+{
+	MV_ZeroMemory(pTaskFile, sizeof(ATA_TaskFile));
+
+	switch ( pReq->Cdb[0] )
+	{
+		case SCSI_CMD_READ_10:
+		case SCSI_CMD_WRITE_10:
+			{
+
+				/*
+				 * The OS maximum tranfer length is set to 128K.
+				 * For ATA_CMD_READ_DMA and ATA_CMD_WRITE_DMA,
+				 * the max size they can handle is 256 sectors.
+				 * And Sector_Count==0 means 256 sectors.
+				 * If OS request max lenght>128K, for 28 bit device, we have to split requests.
+				 */
+#ifndef _OS_LINUX  //Avoid keneral panic
+				MV_DASSERT( ( (((MV_U16)pReq->Cdb[7])<<8) | (pReq->Cdb[8]) ) <= 256 );
+#endif
+				/*
+				 * 24 bit LBA can express 128GB.
+				 * 4 bytes LBA like SCSI_CMD_READ_10 can express 2TB.
+				 */
+
+				/* Make sure Cmd_Flag has set already. */
+				#ifdef SUPPORT_ATA_SECURITY_CMD
+				if ( pReq->Cmd_Flag&CMD_FLAG_NCQ && !((pDevice->Setting&DEVICE_SETTING_SECURITY_LOCKED)==0x10))
+				#else
+				if ( pReq->Cmd_Flag&CMD_FLAG_NCQ)
+				#endif
+				{
+
+					pTaskFile->Features = pReq->Cdb[8];
+					pTaskFile->Feature_Exp = pReq->Cdb[7];
+
+					pTaskFile->Sector_Count = tag<<3;
+
+					pTaskFile->LBA_Low = pReq->Cdb[5];
+					pTaskFile->LBA_Mid = pReq->Cdb[4];
+					pTaskFile->LBA_High = pReq->Cdb[3];
+					pTaskFile->LBA_Low_Exp = pReq->Cdb[2];
+
+					pTaskFile->Device = MV_BIT(6);
+
+					if ( pReq->Cdb[0]==SCSI_CMD_READ_10 )
+						pTaskFile->Command = ATA_CMD_READ_FPDMA_QUEUED;
+					else if ( pReq->Cdb[0]==SCSI_CMD_WRITE_10 )
+						pTaskFile->Command = ATA_CMD_WRITE_FPDMA_QUEUED;
+				}
+				else if ( pReq->Cmd_Flag&CMD_FLAG_48BIT )
+				{
+					MV_DASSERT( !(pReq->Cmd_Flag&CMD_FLAG_NCQ) );
+
+					pTaskFile->Sector_Count = pReq->Cdb[8];
+					pTaskFile->Sector_Count_Exp = pReq->Cdb[7];
+
+					pTaskFile->LBA_Low = pReq->Cdb[5];
+					pTaskFile->LBA_Mid = pReq->Cdb[4];
+					pTaskFile->LBA_High = pReq->Cdb[3];
+					pTaskFile->LBA_Low_Exp = pReq->Cdb[2];
+
+					pTaskFile->Device = MV_BIT(6);
+
+					if ( pReq->Cdb[0]==SCSI_CMD_READ_10 )
+						pTaskFile->Command = ATA_CMD_READ_DMA_EXT;
+					else if ( pReq->Cdb[0]==SCSI_CMD_WRITE_10 )
+						pTaskFile->Command = ATA_CMD_WRITE_DMA_EXT;
+				}
+				else
+				{
+					/* 28 bit DMA */
+					pTaskFile->Sector_Count = pReq->Cdb[8];		/* Could be zero */
+
+					pTaskFile->LBA_Low = pReq->Cdb[5];
+					pTaskFile->LBA_Mid = pReq->Cdb[4];
+					pTaskFile->LBA_High = pReq->Cdb[3];
+
+					pTaskFile->Device = MV_BIT(6) | (pReq->Cdb[2]&0xF);
+
+					MV_DASSERT( (pReq->Cdb[2]&0xF0)==0 );
+
+					if ( pReq->Cdb[0]==SCSI_CMD_READ_10 )
+						pTaskFile->Command = ATA_CMD_READ_DMA;
+					else if ( pReq->Cdb[0]==SCSI_CMD_WRITE_10 )
+						pTaskFile->Command = ATA_CMD_WRITE_DMA;
+				}
+
+				break;
+			}
+#ifdef SUPPORT_ATA_SECURITY_CMD
+		case ATA_16:
+			scsi_ata_passthru_16_fill_taskfile(pReq,pTaskFile);
+			break;
+#endif
+		case SCSI_CMD_READ_16:
+		case SCSI_CMD_WRITE_16:
+			rw16_taskfile(pReq, pTaskFile, tag);
+			break;
+
+		case SCSI_CMD_VERIFY_10:
+			/*
+			 * For verify command, the size may need use two MV_U8, especially Windows.
+			 * For 28 bit device, we have to split the request.
+			 * For 48 bit device, we use ATA_CMD_VERIFY_EXT.
+			 */
+			if ( pDevice->Capacity&DEVICE_CAPACITY_48BIT_SUPPORTED )
+			{
+				pTaskFile->Sector_Count = pReq->Cdb[8];
+				pTaskFile->Sector_Count_Exp = pReq->Cdb[7];
+
+				pTaskFile->LBA_Low = pReq->Cdb[5];
+				pTaskFile->LBA_Mid = pReq->Cdb[4];
+				pTaskFile->LBA_High = pReq->Cdb[3];
+				pTaskFile->LBA_Low_Exp = pReq->Cdb[2];
+
+				pTaskFile->Device = MV_BIT(6);
+
+				pTaskFile->Command = ATA_CMD_VERIFY_EXT;
+			}
+			else
+			{
+				pTaskFile->Sector_Count = pReq->Cdb[8];
+
+				pTaskFile->LBA_Low = pReq->Cdb[5];
+				pTaskFile->LBA_Mid = pReq->Cdb[4];
+				pTaskFile->LBA_High = pReq->Cdb[3];
+
+				pTaskFile->Device = MV_BIT(6) | (pReq->Cdb[2]&0xF);
+
+				MV_DASSERT( (pReq->Cdb[2]&0xF0)==0 );
+
+				pTaskFile->Command = ATA_CMD_VERIFY;
+			}
+
+			break;
+
+		case SCSI_CMD_MARVELL_SPECIFIC:
+			{
+				/* This request should be for core module */
+				if ( pReq->Cdb[1]!=CDB_CORE_MODULE )
+					return MV_FALSE;
+
+				switch ( pReq->Cdb[2] )
+				{
+					case CDB_CORE_IDENTIFY:
+						pTaskFile->Command = ATA_CMD_IDENTIFY_ATA;
+						break;
+					case CDB_CORE_SET_UDMA_MODE:
+						pTaskFile->Command = ATA_CMD_SET_FEATURES;
+						pTaskFile->Features = ATA_CMD_SET_TRANSFER_MODE;
+						pTaskFile->Sector_Count = 0x40 | pReq->Cdb[3];
+						MV_DASSERT( pReq->Cdb[4]==MV_FALSE );	/* Use UDMA mode */
+						break;
+
+					case CDB_CORE_SET_PIO_MODE:
+						pTaskFile->Command = ATA_CMD_SET_FEATURES;
+						pTaskFile->Features = ATA_CMD_SET_TRANSFER_MODE;
+						pTaskFile->Sector_Count = 0x08 | pReq->Cdb[3];
+						break;
+
+					case CDB_CORE_ENABLE_WRITE_CACHE:
+						pTaskFile->Command = ATA_CMD_SET_FEATURES;
+						pTaskFile->Features = ATA_CMD_ENABLE_WRITE_CACHE;
+						 pDevice->Setting |= DEVICE_SETTING_WRITECACHE_ENABLED;
+						break;
+
+					case CDB_CORE_DISABLE_WRITE_CACHE:
+						pTaskFile->Command = ATA_CMD_SET_FEATURES;
+						pTaskFile->Features = ATA_CMD_DISABLE_WRITE_CACHE;
+						 pDevice->Setting &= ~DEVICE_SETTING_WRITECACHE_ENABLED;
+						break;
+
+					case CDB_CORE_SHUTDOWN:
+						if ( pDevice->Capacity&DEVICE_CAPACITY_48BIT_SUPPORTED )
+							pTaskFile->Command = ATA_CMD_FLUSH_EXT;
+						else
+							pTaskFile->Command = ATA_CMD_FLUSH;
+						break;
+
+			 #ifdef SUPPORT_ATA_POWER_MANAGEMENT
+					case CDB_CORE_ATA_IDLE:
+						pTaskFile->Command = 0xe3;
+						pTaskFile->Sector_Count = pReq->Cdb[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?4:6];
+							break;
+					case CDB_CORE_ATA_STANDBY:
+						pTaskFile->Command = 0xe2;
+							break;
+					case CDB_CORE_ATA_IDLE_IMMEDIATE:
+						pTaskFile->Command = 0xe1;
+							break;
+					case CDB_CORE_ATA_STANDBY_IMMEDIATE:
+						pTaskFile->Command = 0xe0;
+							break;
+					case CDB_CORE_ATA_CHECK_POWER_MODE:
+						pTaskFile->Command = 0xe5;
+							break;
+				        case CDB_CORE_ATA_SLEEP:
+						pTaskFile->Command = 0xe6;
+						break;
+                         #endif
+					case CDB_CORE_ATA_IDENTIFY:
+						pTaskFile->Command = 0xec;
+						break;
+
+					case CDB_CORE_ENABLE_READ_AHEAD:
+						pTaskFile->Command = ATA_CMD_SET_FEATURES;
+						pTaskFile->Features = ATA_CMD_ENABLE_READ_LOOK_AHEAD;
+						break;
+
+					case CDB_CORE_DISABLE_READ_AHEAD:
+						pTaskFile->Command = ATA_CMD_SET_FEATURES;
+						pTaskFile->Features = ATA_CMD_DISABLE_READ_LOOK_AHEAD;
+						break;
+
+					case CDB_CORE_READ_LOG_EXT:
+						pTaskFile->Command = ATA_CMD_READ_LOG_EXT;
+						pTaskFile->Sector_Count = 1;	/* Read one sector */
+						pTaskFile->LBA_Low = 0x10;		/* Page 10h */
+						break;
+
+					case CDB_CORE_OS_SMART_CMD:
+						pTaskFile->Command = pReq->Cdb[3];
+						pTaskFile->Features = pReq->Cdb[4];
+						pTaskFile->LBA_Low = pReq->Cdb[5];
+						pTaskFile->LBA_Mid  = pReq->Cdb[6];
+						pTaskFile->LBA_High = pReq->Cdb[7];
+						pTaskFile->LBA_Low_Exp = pReq->Cdb[9];
+						pTaskFile->Sector_Count = pReq->Cdb[8];
+						break;
+				#ifdef SUPPORT_ATA_SMART
+					case CDB_CORE_ATA_IDENTIFY_PACKET_DEVICE:
+						pTaskFile->Command = ATA_IDENTIFY_PACKET_DEVICE;
+						pTaskFile->Sector_Count = 0x1;
+						break;
+					case CDB_CORE_ENABLE_SMART:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_CMD_ENABLE_SMART;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->LBA_Low = 0x1;
+						break;
+					case CDB_CORE_DISABLE_SMART:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_CMD_DISABLE_SMART;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->LBA_Low = 0x1;
+						break;
+					case CDB_CORE_SMART_RETURN_STATUS:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_CMD_SMART_RETURN_STATUS;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						break;
+					case CDB_CORE_ATA_SMART_READ_VALUES:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_SMART_READ_VALUES;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->LBA_Low = 0x1;
+						break;
+					case CDB_CORE_ATA_SMART_READ_THRESHOLDS:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_SMART_READ_THRESHOLDS;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->LBA_Low = 0x1;
+						break;
+					case CDB_CORE_ATA_SMART_READ_LOG_SECTOR:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_SMART_READ_LOG_SECTOR;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->Sector_Count = 0x1;
+						pTaskFile->LBA_Low = pReq->Cdb[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?5:8];
+						break;
+					case CDB_CORE_ATA_SMART_WRITE_LOG_SECTOR :
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_SMART_WRITE_LOG_SECTOR;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->Sector_Count = 0x1;
+						pTaskFile->LBA_Low = pReq->Cdb[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?5:8];
+						break;
+					case  CDB_CORE_ATA_SMART_AUTO_OFFLINE:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_SMART_AUTO_OFFLINE;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->LBA_Low= pReq->Cdb[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?5:8];
+						pTaskFile->Sector_Count = pReq->Cdb[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?4:6];
+						break;
+					case CDB_CORE_ATA_SMART_AUTOSAVE:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_SMART_AUTOSAVE;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->Sector_Count =  pReq->Cdb[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?4 : 6];
+						break;
+					case CDB_CORE_ATA_SMART_IMMEDIATE_OFFLINE:
+						pTaskFile->Command = ATA_CMD_SMART;
+						pTaskFile->Features = ATA_SMART_IMMEDIATE_OFFLINE;
+						pTaskFile->LBA_Mid = 0x4F;
+						pTaskFile->LBA_High = 0xC2;
+						pTaskFile->LBA_Low= pReq->Cdb[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?5:8];
+						pTaskFile->Sector_Count =  pReq->Cdb[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?4 : 6];
+						break;
+				#endif /*#ifdef SUPPORT_ATA_SMART*/
+					case CDB_CORE_ATA_DOWNLOAD_MICROCODE:
+						if(pReq->Cmd_Flag & CMD_FLAG_ATA_12)
+							scsi_ata_passthru_12_fill_taskfile(pReq,pTaskFile);
+						else
+							scsi_ata_passthru_16_fill_taskfile(pReq,pTaskFile);
+						break;
+					default:
+						return MV_FALSE;
+				}
+				break;
+			}
+
+		case SCSI_CMD_SYNCHRONIZE_CACHE_10:
+			if ( pDevice->Capacity&DEVICE_CAPACITY_48BIT_SUPPORTED )
+				pTaskFile->Command = ATA_CMD_FLUSH_EXT;
+			else
+				pTaskFile->Command = ATA_CMD_FLUSH;
+			pTaskFile->Device = MV_BIT(6);
+			break;
+		case SCSI_CMD_START_STOP_UNIT:
+			if (pReq->Cdb[4] & MV_BIT(0))
+			{
+				pTaskFile->Command = ATA_CMD_SEEK;
+				pTaskFile->Device = MV_BIT(6);
+			}
+			else
+			{
+				pTaskFile->Command = ATA_CMD_STANDBY_IMMEDIATE;
+			}
+			break;
+		case SCSI_CMD_SND_DIAG:
+			Core_Fill_SendDiagTaskfile( pDevice,pReq, pTaskFile);
+			break;
+		case SCSI_CMD_REQUEST_SENSE:
+		case SCSI_CMD_MODE_SELECT_10:
+		case SCSI_CMD_MODE_SENSE_10:
+			MV_DPRINT(("Error: Unknown request: 0x%x.\n", pReq->Cdb[0]));
+
+		default:
+			return MV_FALSE;
+	}
+
+	/*
+	 * Attention: Never return before this line if your return is MV_TRUE.
+	 * We need set the slave DEV bit here.
+	 */
+	if ( pDevice->Is_Slave )
+		pTaskFile->Device |= MV_BIT(4);
+
+	return MV_TRUE;
+}
+
+MV_BOOLEAN ATAPI_CDB2TaskFile(
+	IN PDomain_Device pDevice,
+	IN PMV_Request pReq,
+	OUT PATA_TaskFile pTaskFile
+	)
+{
+	MV_ZeroMemory(pTaskFile, sizeof(ATA_TaskFile));
+
+	/* At the same time, set the command category as well. */
+	switch ( pReq->Cdb[0] )
+	{
+	case SCSI_CMD_MARVELL_SPECIFIC:
+		/* This request should be for core module */
+		if ( pReq->Cdb[1]!=CDB_CORE_MODULE )
+			return MV_FALSE;
+
+		switch ( pReq->Cdb[2] )
+		{
+		case CDB_CORE_IDENTIFY:
+			pTaskFile->Command = ATA_CMD_IDENTIY_ATAPI;
+			break;
+
+		case CDB_CORE_SET_UDMA_MODE:
+			pTaskFile->Command = ATA_CMD_SET_FEATURES;
+			pTaskFile->Features = ATA_CMD_SET_TRANSFER_MODE;
+			if ( pReq->Cdb[4]==MV_TRUE )
+				pTaskFile->Sector_Count = 0x20 | pReq->Cdb[3];	/* MDMA mode */
+			else
+				pTaskFile->Sector_Count = 0x40 | pReq->Cdb[3];	/* UDMA mode*/
+
+			break;
+
+		case CDB_CORE_SET_PIO_MODE:
+			pTaskFile->Command = ATA_CMD_SET_FEATURES;
+			pTaskFile->Features = ATA_CMD_SET_TRANSFER_MODE;
+			pTaskFile->Sector_Count = 0x08 | pReq->Cdb[3];
+			break;
+
+		case CDB_CORE_OS_SMART_CMD:
+		//Now we don't support SMART on ATAPI device.
+		default:
+			return MV_FALSE;
+		}
+		break;
+
+#ifdef _OS_LINUX
+	case SCSI_CMD_READ_DISC_INFO:
+#endif /* _OS_LINUX */
+	case SCSI_CMD_READ_10:
+	case SCSI_CMD_WRITE_10:
+	case SCSI_CMD_VERIFY_10:
+	case SCSI_CMD_INQUIRY:
+	case SCSI_CMD_READ_CAPACITY_10:
+	case SCSI_CMD_TEST_UNIT_READY:
+	case SCSI_CMD_MODE_SENSE_10:
+	case SCSI_CMD_MODE_SELECT_10:
+	case SCSI_CMD_PREVENT_MEDIUM_REMOVAL:
+	case SCSI_CMD_READ_TOC:
+	case SCSI_CMD_START_STOP_UNIT:
+	case SCSI_CMD_SYNCHRONIZE_CACHE_10:
+	case SCSI_CMD_REQUEST_SENSE:
+	default:
+		/*
+		 * Use packet command
+		 */
+		/* Features: DMA, OVL, DMADIR */
+#if defined(USE_DMA_FOR_ALL_PACKET_COMMAND)
+		if ( !(pReq->Cmd_Flag&CMD_FLAG_NON_DATA) )
+		{
+			pTaskFile->Features |= MV_BIT(0);
+		}
+#elif defined(USE_PIO_FOR_ALL_PACKET_COMMAND)
+		/* do nothing */
+#else
+		if ( pReq->Cmd_Flag&CMD_FLAG_DMA )
+			pTaskFile->Features |= MV_BIT(0);
+#endif
+		/* Byte count low and byte count high */
+		if ( pReq->Data_Transfer_Length>0xFFFF )
+		{
+			pTaskFile->LBA_Mid = 0xFF;
+			pTaskFile->LBA_High = 0xFF;
+		}
+		else
+		{
+			pTaskFile->LBA_Mid = (MV_U8)pReq->Data_Transfer_Length;
+			pTaskFile->LBA_High = (MV_U8)(pReq->Data_Transfer_Length>>8);
+		}
+
+		pTaskFile->Command = ATA_CMD_PACKET;
+
+		break;
+	}
+
+	/*
+	 * Attention: Never return before this line if your return is MV_TRUE.
+	 * We need set the slave DEV bit here.
+	 */
+	if ( pDevice->Is_Slave )
+		pTaskFile->Device |= MV_BIT(4);
+
+	return MV_TRUE;
+}
--- /dev/null
+++ b/drivers/scsi/thor/include/com_dbg.h
@@ -0,0 +1,164 @@
+#if !defined(COMMON_DEBUG_H)
+#define COMMON_DEBUG_H
+
+/*
+ *	Marvell Debug Interface
+ *
+ *	MACRO
+ *		MV_DEBUG is defined in debug version not in release version.
+ *
+ *	Debug funtions:
+ *		MV_PRINT:	print string in release and debug build.
+ *		MV_DPRINT:	print string in debug build.
+ *		MV_TRACE:	print string including file name, line number in release and debug build.
+ *		MV_DTRACE:	print string including file name, line number in debug build.
+ *		MV_ASSERT:	assert in release and debug build.
+ *		MV_DASSERT: assert in debug build.
+ */
+
+#include "com_define.h"
+/*
+ *
+ * Debug funtions
+ *
+ */
+
+/* For both debug and release version */
+#if defined(SIMULATOR)
+#   include <assert.h>
+#   define MV_PRINT printf
+#   define MV_ASSERT assert
+#   define MV_TRACE(_x_)                                   \
+              do {	                                   \
+                 MV_PRINT("%s(%d) ", __FILE__, __LINE__);  \
+                 MV_PRINT _x_;                             \
+	      } while (0)
+#elif defined(_OS_WINDOWS)
+/* for VISTA */
+#   if ((_WIN32_WINNT >= 0x0600) && defined(MV_DEBUG))
+       ULONG _cdecl MV_PRINT(char* format, ...);
+#   else
+#      define MV_PRINT                      DbgPrint
+#   endif /* ((_WIN32_WINNT >= 0x0600) && defined(MV_DEBUG)) */
+
+#   if (defined(_CPU_IA_64B) || defined(_CPU_AMD_64B))
+#      if (_MSC_VER >= 800) || defined(_STDCALL_SUPPORTED)
+#         define NTAPI __stdcall
+#      else
+#         define NTAPI
+#      endif /* (_MSC_VER >= 800) || defined(_STDCALL_SUPPORTED) */
+
+       void NTAPI DbgBreakPoint(void);
+#      define MV_ASSERT(_condition_)    \
+                 do { if (!(_condition_)) DbgBreakPoint(); } while(0)
+#   else
+#      define MV_ASSERT(_condition_)    \
+                 do { if (!(_condition_)) {__asm int 3}; } while(0)
+#   endif /*  (defined(_CPU_IA_64B) || defined(_CPU_AMD_64B)) */
+
+#   define MV_TRACE(_x_)                                        \
+              do {                                              \
+                 MV_PRINT("%s(%d) ", __FILE__, __LINE__);       \
+                 MV_PRINT _x_;                                  \
+              } while (0)
+
+#elif defined(_OS_LINUX)
+#   define MV_PRINT(format, arg...)	printk("%s %d:" format, __FILE__, __LINE__, ## arg)
+#   define MV_ASSERT(_x_)  BUG_ON(!(_x_))
+
+#   define MV_TRACE(_x_)                                        \
+              do {                                              \
+                 MV_PRINT("%s(%d) ", __FILE__, __LINE__);       \
+                 MV_PRINT _x_;                                  \
+           } while(0)
+#elif defined(__QNXNTO__)
+#   define MV_PRINT      printk("%s: ", mv_full_name), printk
+#   define MV_ASSERT(_x_)                                       \
+              do {                                              \
+		 if (!(_x_))                                    \
+                    MV_PRINT("Assert at File %s: Line %d.\n",   \
+                             __FILE__, __LINE__);               \
+              } while (0)
+#   define MV_TRACE(_x_)                                        \
+              do {                                              \
+                 MV_PRINT("%s(%d) ", __FILE__, __LINE__);       \
+                 printk   _x_;                                  \
+           } while(0)
+#else /* OTHER OSes */
+#   define MV_PRINT(_x_)
+#   define MV_ASSERT(_condition_)
+#   define MV_TRACE(_x_)
+#endif /* _OS_WINDOWS */
+
+
+/*
+ * Used with MV_DBG macro, see below .
+ * Should be useful for Win driver too, so it is placed here.
+ *
+ */
+#define DMSG_POKE        0x0001  /* flag controlling the dump_stack()  */
+#define DMSG_KERN        0x0002  /* kernel driver dbg msg */
+#define DMSG_SCSI        0x0004  /* SCSI Subsystem dbg msg */
+#define DMSG_CORE        0x0008  /* CORE dbg msg */
+#define DMSG_HBA         0x0010  /* HBA dbg msg */
+#define DMSG_RESER03     0x0020
+#define DMSG_FREQ        0x0040  /* msg that'll pop up 100+ times per sec */
+#define DMSG_IOCTL       0x0080  /* ioctl msg */
+#define DMSG_MSG         0x0100  /* plain msg, should be enabled all time */
+#define DMSG_SCSI_FREQ   0x0200  /* freq scsi dbg msg */
+#define DMSG_RAID        0x0400  /* raid dbg msg */
+#define DMSG_PROF        0x0800  /* profiling msg */
+#define DMSG_PROF_FREQ   0x1000  /* freq profiling msg */
+#define DMSG_RES     	 0x2000
+#define DMSG_SAS         0x4000  /* sas msg */
+#define DMSG_CORE_EH     0x8000  /* core error handling */
+/* For debug version only */
+#if defined(MV_DEBUG)
+#   ifdef _OS_LINUX
+       extern unsigned int mv_dbg_opts;
+#      define MV_DBG(x,...)                          \
+          do {                                       \
+	     if (unlikely(x&mv_dbg_opts)) {          \
+                MV_PRINT(__VA_ARGS__);               \
+             }                                       \
+          } while (0)
+
+#      define MV_POKE()                              \
+          do {                                       \
+		dump_stack();                        \
+          } while (0)
+
+#      define MV_DPRINT(x)                           \
+          do {                                       \
+	     if (unlikely(DMSG_CORE&mv_dbg_opts))    \
+	        MV_PRINT x;                          \
+          } while (0)
+#   else
+#      define MV_PRINT        printk
+#      define MV_DPRINT(x)	MV_PRINT x
+/* in case drivers for non-linux os go crazy */
+#      define MV_DBG(x)         do{}while(0)
+#      define MV_POKE()
+#   endif /* _OS_LINUX */
+
+#   define MV_DASSERT	        MV_ASSERT
+#   define MV_DTRACE	        MV_DTRACE
+#else
+#   if defined(__QNXNTO__) || defined(_OS_LINUX)
+#      define MV_DBG(x,...)        do{}while(0)
+#   else
+#      define MV_DBG(x)            do{}while(0)
+#   endif /* __QNXNTO__ */
+//#   define MV_PRINT(x,...)
+#   define MV_POKE()
+#   define MV_DPRINT(x)
+#   define MV_DASSERT(x)
+#   define MV_DTRACE(x)
+#endif /* MV_DEBUG */
+
+MV_BOOLEAN mvLogRegisterModule(MV_U8 moduleId, MV_U32 filterMask, char* name);
+MV_BOOLEAN mvLogSetModuleFilter(MV_U8 moduleId, MV_U32 filterMask);
+MV_U32 mvLogGetModuleFilter(MV_U8 moduleId);
+void mvLogMsg(MV_U8 moduleId, MV_U32 type, char* format, ...);
+
+#endif /* COMMON_DEBUG_H */
--- /dev/null
+++ b/drivers/scsi/thor/include/com_list.c
@@ -0,0 +1,183 @@
+#include "com_define.h"
+#include "com_list.h"
+
+/*
+ * Insert a new entry between two known consecutive entries.
+ *
+ * This is only for internal list manipulation where we know
+ * the prev/next entries already!
+ */
+static MV_INLINE void __List_Add(List_Head *new_one,
+			      List_Head *prev,
+			      List_Head *next)
+{
+	next->prev = new_one;
+	new_one->next = next;
+	new_one->prev = prev;
+	prev->next = new_one;
+}
+
+/**
+ * List_Add - add a new entry
+ * @new_one: new entry to be added
+ * @head: list head to add it after
+ *
+ * Insert a new entry after the specified head.
+ * This is good for implementing stacks.
+ */
+static MV_INLINE void List_Add(List_Head *new_one, List_Head *head)
+{
+	__List_Add(new_one, head, head->next);
+}
+
+/**
+ * List_AddTail - add a new entry
+ * @new_one: new entry to be added
+ * @head: list head to add it before
+ *
+ * Insert a new entry before the specified head.
+ * This is useful for implementing queues.
+ */
+static MV_INLINE void List_AddTail(List_Head *new_one, List_Head *head)
+{
+	__List_Add(new_one, head->prev, head);
+}
+
+/*
+ * Delete a list entry by making the prev/next entries
+ * point to each other.
+ *
+ * This is only for internal list manipulation where we know
+ * the prev/next entries already!
+ */
+static MV_INLINE void __List_Del(List_Head * prev, List_Head * next)
+{
+	next->prev = prev;
+	prev->next = next;
+}
+
+/**
+ * List_Del - deletes entry from list.
+ * @entry: the element to delete from the list.
+ * Note: List_Empty on entry does not return true after this, the entry is
+ * in an undefined state.
+ */
+static MV_INLINE void List_Del(List_Head *entry)
+{
+	__List_Del(entry->prev, entry->next);
+	entry->next = NULL;
+	entry->prev = NULL;
+}
+
+/**
+ * List_DelInit - deletes entry from list and reinitialize it.
+ * @entry: the element to delete from the list.
+ */
+static MV_INLINE void List_DelInit(List_Head *entry)
+{
+	__List_Del(entry->prev, entry->next);
+	MV_LIST_HEAD_INIT(entry);
+}
+
+/**
+ * List_Move - delete from one list and add as another's head
+ * @list: the entry to move
+ * @head: the head that will precede our entry
+ */
+static MV_INLINE void List_Move(List_Head *list, List_Head *head)
+{
+        __List_Del(list->prev, list->next);
+        List_Add(list, head);
+}
+
+/**
+ * List_MoveTail - delete from one list and add as another's tail
+ * @list: the entry to move
+ * @head: the head that will follow our entry
+ */
+static MV_INLINE void List_MoveTail(List_Head *list,
+				  List_Head *head)
+{
+        __List_Del(list->prev, list->next);
+        List_AddTail(list, head);
+}
+
+/**
+ * List_Empty - tests whether a list is empty
+ * @head: the list to test.
+ */
+static MV_INLINE int List_Empty(const List_Head *head)
+{
+	return head->next == head;
+}
+
+static MV_INLINE int List_GetCount(const List_Head *head)
+{
+	int i=0;
+	List_Head *pos;
+	LIST_FOR_EACH(pos, head) {
+		i++;
+	}
+	return i;
+}
+
+static MV_INLINE List_Head* List_GetFirst(List_Head *head)
+{
+	List_Head * one = NULL;
+	if ( List_Empty(head) ) return NULL;
+
+	one = head->next;
+	List_Del(one);
+	return one;
+}
+
+static MV_INLINE List_Head* List_GetLast(List_Head *head)
+{
+	List_Head * one = NULL;
+	if ( List_Empty(head) ) return NULL;
+
+	one = head->prev;
+	List_Del(one);
+	return one;
+}
+
+static MV_INLINE void __List_Splice(List_Head *list,
+				 List_Head *head)
+{
+	List_Head *first = list->next;
+	List_Head *last = list->prev;
+	List_Head *at = head->next;
+
+	first->prev = head;
+	head->next = first;
+
+	last->next = at;
+	at->prev = last;
+}
+
+/**
+ * List_Splice - join two lists
+ * @list: the new list to add.
+ * @head: the place to add it in the first list.
+ */
+static MV_INLINE void List_Splice(List_Head *list, List_Head *head)
+{
+	if (!List_Empty(list))
+		__List_Splice(list, head);
+}
+
+/**
+ * List_Splice_Init - join two lists and reinitialise the emptied list.
+ * @list: the new list to add.
+ * @head: the place to add it in the first list.
+ *
+ * The list at @list is reinitialised
+ */
+static MV_INLINE void List_Splice_Init(List_Head *list,
+				    List_Head *head)
+{
+	if (!List_Empty(list)) {
+		__List_Splice(list, head);
+		MV_LIST_HEAD_INIT(list);
+	}
+}
--- /dev/null
+++ b/drivers/scsi/thor/include/com_list.h
@@ -0,0 +1,193 @@
+#if !defined(COMMON_LIST_H)
+#define COMMON_LIST_H
+
+#include "com_define.h"
+
+/*
+ * Simple doubly linked list implementation.
+ *
+ * Some of the internal functions ("__xxx") are useful when
+ * manipulating whole lists rather than single entries, as
+ * sometimes we already know the next/prev entries and we can
+ * generate better code by using them directly rather than
+ * using the generic single-entry routines.
+ */
+
+#ifdef NEW_LINUX_DRIVER
+/* use native code */
+#define List_Head           struct list_head
+#define PList_Head          struct list_head *
+
+#define MV_LIST_HEAD        LIST_HEAD
+#define MV_INIT_LIST_HEAD   LIST_HEAD_INIT
+#define MV_LIST_HEAD_INIT   INIT_LIST_HEAD
+
+//#define List_Add            list_add
+//#define List_AddTail        list_add_tail
+//#define List_Del            list_del
+#define List_DelInit        list_del_init
+#define List_Move           list_move
+#define List_MoveTail       list_move_tail
+#define List_Empty          list_empty
+
+#define CONTAINER_OF        container_of
+#define LIST_ENTRY          list_entry
+#define LIST_FOR_EACH       list_for_each
+#define LIST_FOR_EACH_PREV  list_for_each_prev
+#define LIST_FOR_EACH_ENTRY list_for_each_entry
+#define LIST_FOR_EACH_ENTRY_PREV list_for_each_entry_prev
+#define LIST_FOR_EACH_ENTRY_TYPE(ptr, list_ptr, type, entry) \
+           list_for_each_entry(ptr, list_ptr, entry)
+
+#define List_Splice         list_splice
+#define List_Splice_Init    list_splice_init
+#define __List_Del          __list_del  /* internal function, use with care */
+#if 0
+
+static MV_INLINE void List_Del(List_Head *entry)
+{
+#ifdef SUPPORT_REQUEST_TIMER
+	if((entry->prev) || (entry->next))
+#endif
+	__List_Del(entry->prev, entry->next);
+	entry->next = NULL;
+	entry->prev = NULL;
+}
+
+
+static inline struct list_head *List_GetFirst(struct list_head *head)
+{
+        struct list_head * one = NULL;
+        if (list_empty(head))
+		return NULL;
+
+        one = head->next;
+        list_del(one);
+        return one;
+}
+
+static inline struct list_head *List_GetLast(struct list_head *head)
+{
+        struct list_head * one = NULL;
+        if (list_empty(head))
+		return NULL;
+
+        one = head->prev;
+        list_del(one);
+        return one;
+}
+#endif
+
+#else /* _OS_LINUX */
+/*
+ *
+ *
+ * Data Structure
+ *
+ *
+ */
+typedef struct _List_Head {
+	struct _List_Head *prev, *next;
+} List_Head, * PList_Head;
+
+
+/*
+ *
+ *
+ * Exposed Functions
+ *
+ *
+ */
+
+#define MV_LIST_HEAD(name) \
+	List_Head name = { &(name), &(name) }
+
+#define MV_LIST_HEAD_INIT(ptr) do { \
+	(ptr)->next = (ptr); (ptr)->prev = (ptr); \
+} while (0)
+
+static MV_INLINE void List_Add(List_Head *new_one, List_Head *head);
+
+static MV_INLINE void List_AddTail(List_Head *new_one, List_Head *head);
+
+static MV_INLINE void List_Del(List_Head *entry);
+
+static MV_INLINE void List_DelInit(List_Head *entry);
+
+static MV_INLINE void List_Move(List_Head *list, List_Head *head);
+
+static MV_INLINE void List_MoveTail(List_Head *list,
+				  List_Head *head);
+
+static MV_INLINE int List_Empty(const List_Head *head);
+
+
+#define CONTAINER_OF(ptr, type, member) 			\
+        ( (type *)( (char *)(ptr) - OFFSET_OF(type,member) ) )
+
+#define LIST_ENTRY(ptr, type, member) \
+	CONTAINER_OF(ptr, type, member)
+
+/**
+ * LIST_FOR_EACH	-	iterate over a list
+ * @pos:	the &List_Head to use as a loop counter.
+ * @head:	the head for your list.
+ */
+#define LIST_FOR_EACH(pos, head) \
+	for (pos = (head)->next; pos != (head); pos = pos->next)
+
+/**
+ * LIST_FOR_EACH_PREV	-	iterate over a list backwards
+ * @pos:	the &List_Head to use as a loop counter.
+ * @head:	the head for your list.
+ */
+#define LIST_FOR_EACH_PREV(pos, head) \
+	for (pos = (head)->prev; pos != (head); pos = pos->prev)
+
+/**
+ * LIST_FOR_EACH_ENTRY	-	iterate over list of given type
+ * @pos:	the type * to use as a loop counter.
+ * @head:	the head for your list.
+ * @member:	the name of the list_struct within the struct.
+ */
+#define LIST_FOR_EACH_ENTRY(pos, head, member)				\
+	for (pos = LIST_ENTRY((head)->next, typeof(*pos), member);	\
+	     &pos->member != (head); 	\
+	     pos = LIST_ENTRY(pos->member.next, typeof(*pos), member))
+
+/**
+ * LIST_FOR_EACH_ENTRY_TYPE	-	iterate over list of given type
+ * @pos:	the type * to use as a loop counter.
+ * @head:	the head for your list.
+ * @member:	the name of the list_struct within the struct.
+ * @type:	the type of the struct this is embedded in.
+*/
+#define LIST_FOR_EACH_ENTRY_TYPE(pos, head, type, member)       \
+	for (pos = LIST_ENTRY((head)->next, type, member);	\
+	     &pos->member != (head); 	                        \
+	     pos = LIST_ENTRY(pos->member.next, type, member))
+
+/**
+ * LIST_FOR_EACH_ENTRY_PREV - iterate backwards over list of given type.
+ * @pos:	the type * to use as a loop counter.
+ * @head:	the head for your list.
+ * @member:	the name of the list_struct within the struct.
+ */
+#define LIST_FOR_EACH_ENTRY_PREV(pos, head, member)			\
+	for (pos = LIST_ENTRY((head)->prev, typeof(*pos), member);	\
+	     &pos->member != (head); 	\
+	     pos = LIST_ENTRY(pos->member.prev, typeof(*pos), member))
+
+#ifndef _OS_BIOS
+#include "com_list.c"
+#endif
+
+#endif /* _OS_LINUX */
+
+#define List_GetFirstEntry(head, type, member)	\
+	LIST_ENTRY(List_GetFirst(head), type, member)
+
+#define List_GetLastEntry(head, type, member)	\
+	LIST_ENTRY(List_GetLast(head), type, member)
+
+#endif /* COMMON_LIST_H */
--- /dev/null
+++ b/drivers/scsi/thor/include/com_mod_mgmt.h
@@ -0,0 +1,137 @@
+#ifndef __MV_MODULE_MGMT__
+#define __MV_MODULE_MGMT__
+
+#include "com_define.h"
+#include "com_type.h"
+#include "com_util.h"
+
+enum {
+	/* module status (module_descriptor) */
+	MV_MOD_VOID   = 0,
+	MV_MOD_UNINIT,
+	MV_MOD_REGISTERED,  /* module ops pointer registered */
+	MV_MOD_INITED,      /* resource assigned */
+	MV_MOD_FUNCTIONAL,
+	MV_MOD_STARTED,
+	MV_MOD_DEINIT,      /* extension released, be gone soon */
+	MV_MOD_GONE,
+};
+
+
+/* adapter descriptor */
+struct mv_adp_desc {
+	struct list_head  hba_entry;
+	struct list_head  online_module_list;
+
+	spinlock_t        lock;
+	spinlock_t        global_lock;
+
+	/* adapter information */
+	MV_U8             Adapter_Bus_Number;
+	MV_U8             Adapter_Device_Number;
+	MV_U8             Revision_Id;
+	MV_U8             id;             /* multi-hba support, start from 0 */
+	MV_U8             pad0;
+	MV_U8             running_mod_num;/* number of up & running modules */
+
+
+	MV_U16            vendor;
+	MV_U16            device;
+	MV_U16            subsystem_vendor;
+	MV_U16            subsystem_device;
+#ifdef ODIN_DRIVER
+	MV_U8				RunAsNonRAID;
+#endif
+	/* System resource */
+	MV_PVOID          Base_Address[ROUNDING(MAX_BASE_ADDRESS, 2)];
+
+	MV_U32            max_io;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,11)
+		unsigned int   pci_config_space[16];
+#endif
+
+#ifdef _OS_LINUX
+	dev_t             dev_no;
+
+	struct pci_dev    *dev;
+	MV_PVOID          extension;      /* hba_ext */
+#endif /* _OS_LINUX */
+};
+
+struct mv_mod_desc;
+
+
+struct mv_mod_res {
+	struct list_head       res_entry;
+	MV_PHYSICAL_ADDR       bus_addr;
+	MV_PVOID               virt_addr;
+
+	MV_U32                 size;
+
+	MV_U16                 type;          /* enum Resource_Type */
+	MV_U16                 align;
+};
+
+typedef struct _Module_Interface
+{
+	MV_U8      module_id;
+	MV_U32     (*get_res_desc)(enum Resource_Type type, MV_U16 maxIo);
+	MV_VOID    (*module_initialize)(MV_PVOID extension,
+					MV_U32   size,
+					MV_U16   max_io);
+	MV_VOID    (*module_start)(MV_PVOID extension);
+	MV_VOID    (*module_stop)(MV_PVOID extension);
+	MV_VOID    (*module_notification)(MV_PVOID extension,
+					  enum Module_Event event,
+					  struct mod_notif_param *param);
+	MV_VOID    (*module_sendrequest)(MV_PVOID extension,
+					 PMV_Request pReq);
+	MV_VOID    (*module_reset)(MV_PVOID extension);
+	MV_VOID    (*module_monitor)(MV_PVOID extension);
+	MV_BOOLEAN (*module_service_isr)(MV_PVOID extension);
+#ifdef RAID_DRIVER
+	MV_VOID    (*module_send_xor_request)(MV_PVOID This,
+					      PMV_XOR_Request pXORReq);
+#endif /* RAID_DRIVER */
+} Module_Interface, *PModule_Interface;
+
+#define mv_module_ops _Module_Interface
+
+#define mv_set_mod_ops(_ops, _id, _get_res_desc, _init, _start,            \
+		       _stop, _send, _reset, _mon, _send_eh, _isr, _xor)   \
+           {                                                               \
+		   _ops->id                      = id;                     \
+		   _ops->get_res_desc            = _get_res_desc;          \
+		   _ops->module_initialize       = _init;                  \
+		   _ops->module_start            = _start;                 \
+		   _ops->module_stop             = _stop;                  \
+		   _ops->module_sendrequest      = _send;                  \
+		   _ops->module_reset            = _reset;                 \
+		   _ops->module_monitor          = _mon;                   \
+		   _ops->module_send_eh_request  = _send_eh;               \
+		   _ops->module_service_isr      = _isr;                   \
+		   _ops->module_send_xor_request = _xor;                   \
+	   }
+
+
+/* module descriptor */
+struct mv_mod_desc {
+	struct list_head           mod_entry;      /* kept in a list */
+
+	struct mv_mod_desc         *parent;
+	struct mv_mod_desc         *child;
+
+	MV_U32                     extension_size;
+	MV_U8                      status;
+	MV_U8                      ref_count;
+	MV_U8                      module_id;
+	MV_U8                      res_entry;
+
+	MV_PVOID                   extension;      /* module extention */
+	struct mv_module_ops       *ops;           /* interface operations */
+
+	struct mv_adp_desc         *hba_desc;
+	struct list_head           res_list;
+};
+
+#endif /* __MV_MODULE_MGMT__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/com_nvram.h
@@ -0,0 +1,106 @@
+typedef union {
+    struct {
+        MV_U32 low;
+        MV_U32 high;
+    } parts;
+    MV_U8       b[8];
+    MV_U16      w[4];
+    MV_U32      d[2];
+} SAS_ADDR, *PSAS_ADDR;
+
+typedef struct _PHY_TUNING {
+	MV_U8	AMP:4;						  /* 4 bits,  amplitude  */
+	MV_U8	Pre_Emphasis:3;				  /* 3 bits,  pre-emphasis */
+	MV_U8	Reserved_1bit_1:1;				  /* 1 bit,   reserved space */
+    MV_U8	Drive_En:6;						/* 6 bits,	drive enable */
+	MV_U8	Pre_Half_En:1;					/* 1 bit,	Half Pre-emphasis Enable*/
+	MV_U8	Reserved_1bit_2:1;				/* 1 bit, 	reserved space */
+    MV_U8	Reserved[2];					/* 2 bytes, reserved space */
+} PHY_TUNING, *PPHY_TUNING;
+
+
+/* HBA_FLAG_XX */
+#define HBA_FLAG_INT13_ENABLE				MV_BIT(0)	//int 13h enable/disable
+#define HBA_FLAG_SILENT_MODE_ENABLE			MV_BIT(1)	//silent mode enable/disable
+#define HBA_FLAG_ERROR_STOP					MV_BIT(2)	//if error then stop
+
+#define NVRAM_DATA_MAJOR_VERSION		0
+#define NVRAM_DATA_MINOR_VERSION		1
+
+/*
+	HBA_Info_Page is saved in Flash/NVRAM, total 256 bytes.
+	The data area is valid only Signature="MRVL".
+	If any member fills with 0xFF, the member is invalid.
+*/
+typedef struct _HBA_Info_Page{
+	// Dword 0
+	MV_U8     	Signature[4];                 	/* 4 bytes, structure signature,should be "MRVL" at first initial */
+
+	// Dword 1
+	MV_U8     	MinorRev;                 		/* 2 bytes, NVRAM data structure version */
+	MV_U8		MajorRev;
+	MV_U16    	Next_Page;					  	/* 2 bytes, For future data structure expansion, 0 is for NULL pointer and current page is the last page. */
+
+	// Dword 2
+	MV_U8     	Major;                   		/* 1 byte,  BIOS major version */
+	MV_U8     	Minor;                  		/* 1 byte,	BIOS minor version */
+	MV_U8     	OEM_Num;                     	/* 1 byte,  OEM number */
+	MV_U8     	Build_Num;                    	/* 1 byte,  Build number */
+
+	// Dword 3
+	MV_U8     	Page_Code;					  	/* 1 byte,  eg. 0 for the 1st page  */
+	MV_U8     	Max_PHY_Num;				  	/* 1 byte,   maximum PHY number */
+	MV_U8		Reserved2[2];
+
+	// Dword 4
+	MV_U32     	HBA_Flag;                     	/*
+													4 bytes, should be 0x0000,0000 at first initial
+	                                                                HBA flag:  refers to HBA_FLAG_XX
+	                                                                bit 0   --- HBA_FLAG_BBS_ENABLE
+	                                                                bit 1   --- HBA_FLAG_SILENT_MODE_ENABLE
+	                                                                bit 2   --- HBA_FLAG_ERROR_STOP
+	                                                                bit 3   --- HBA_FLAG_INT13_ENABLE
+	                                                                bit 4   --- HBA_FLAG_ERROR_STOP
+	                                                                bit 5   --- HBA_FLAG_ERROR_PASS
+	                                                                bit 6   --- HBA_FLAG_SILENT_MODE_ENABLE
+	                                           */
+	// Dword 5
+	MV_U32     	Boot_Device;					/* 4 bytes, select boot device */
+												/* for ata device, it is CRC of the serial number + model number. */
+												/* for sas device, it is CRC of sas address */
+												/* for VD, it is VD GUI */
+
+	// Dword 6-8
+	MV_U32     	Reserved3[3];				  	/* 12 bytes, reserved	*/
+
+	// Dword 9-13
+	MV_U8     	Serial_Num[20];				  	/* 20 bytes, controller serial number */
+
+	// Dword 14-29
+	SAS_ADDR	SAS_Address[8];               /* 64 bytes, SAS address for each port */
+
+	// Dword 30-43
+	MV_U8     	Reserved4[56];                  /* 56 bytes, reserve space for future,initial as 0xFF */
+
+	// Dword 44-45
+	MV_U8     	PHY_Rate[8];                  	/* 8 bytes,  0:  1.5G, 1: 3.0G, should be 0x01 at first initial */
+
+	// Dword 46-53
+	PHY_TUNING    PHY_Tuning[8];				/* 32 bytes, PHY tuning parameters for each PHY*/
+
+	// Dword 54-62
+	MV_U32     	Reserved5[9];                 	/* 9 dword, reserve space for future,initial as 0xFF */
+
+	// Dword 63
+	MV_U8     	Reserved6[3];                 	/* 3 bytes, reserve space for future,initial as 0xFF */
+	MV_U8     	Check_Sum;                    	/* 1 byte,   checksum for this structure,Satisfy sum of every 8-bit value of this structure */
+}HBA_Info_Page, *pHBA_Info_Page;			/* total 256 bytes */
+
+#define FLASH_PARAM_SIZE 	(sizeof(HBA_Info_Page))
+#define ODIN_FLASH_SIZE		0x40000  				//.256k bytes
+#define PARAM_OFFSET		ODIN_FLASH_SIZE - 0x100 //.255k bytes
+
+MV_U8 mvui_init_param(MV_PVOID This, pHBA_Info_Page pHBAInfo);//get initial data from flash
+
+MV_U8	mvCaculateChecksum(MV_PU8	Address, MV_U32 Size);
+MV_U8	mvVerifyChecksum(MV_PU8	Address, MV_U32 Size);
--- /dev/null
+++ b/drivers/scsi/thor/include/com_scsi.h
@@ -0,0 +1,299 @@
+#ifndef __MV_COM_SCSI_H__
+#define __MV_COM_SCSI_H__
+
+/*
+ * SCSI command
+ */
+#define SCSI_CMD_INQUIRY                        0x12
+#define SCSI_CMD_START_STOP_UNIT                0x1B
+#define SCSI_CMD_TEST_UNIT_READY                0x00
+#define SCSI_CMD_RESERVE_6                      0x16
+#define SCSI_CMD_RELEASE_6                      0x17
+
+#define SCSI_CMD_READ_6                         0x08
+#define SCSI_CMD_READ_10                        0x28
+#define SCSI_CMD_READ_12                        0xA8
+#define SCSI_CMD_READ_16                        0x88
+#define SCSI_CMD_READ_LONG_10				0x3E
+#define  SCSI_CMD_READ_DEFECT_DATA_10        0x37
+
+#define SCSI_CMD_WRITE_6                        0x0A
+#define SCSI_CMD_WRITE_10                       0x2A
+#define SCSI_CMD_WRITE_12                       0xAA
+#define SCSI_CMD_WRITE_16                       0x8A
+#define SCSI_CMD_WRITE_LONG_10					0x3F
+
+#define SCSI_CMD_READ_CAPACITY_10               0x25
+#define SCSI_CMD_READ_CAPACITY_16               0x9E    /* 9Eh/10h */
+/* values for service action in */
+#define SCSI_CMD_SAI_READ_CAPACITY_16  			0x10
+
+#define SCSI_CMD_VERIFY_10                      0x2F
+#define SCSI_CMD_VERIFY_12                      0xAF
+#define SCSI_CMD_VERIFY_16                      0x8F
+
+#define SCSI_CMD_REQUEST_SENSE                  0x03
+#define SCSI_CMD_MODE_SENSE_6                   0x1A
+#define SCSI_CMD_MODE_SENSE_10                  0x5A
+#define SCSI_CMD_MODE_SELECT_6                  0x15
+#define SCSI_CMD_MODE_SELECT_10                 0x55
+
+#define SCSI_CMD_LOG_SELECT                     0x4C
+#define SCSI_CMD_LOG_SENSE                      0x4D
+
+#define SCSI_CMD_WRITE_VERIFY_10                0x2E
+#define SCSI_CMD_WRITE_VERIFY_12                0xAE
+#define SCSI_CMD_WRITE_VERIFY_16                0x8E
+#define SCSI_CMD_SYNCHRONIZE_CACHE_10           0x35
+#define SCSI_CMD_SYNCHRONIZE_CACHE_16			0x91
+
+#define SCSI_CMD_WRITE_SAME_10                  0x41
+#define SCSI_CMD_WRITE_SAME_16                  0x93
+
+#define SCSI_CMD_XDWRITE_10                     0x50
+#define SCSI_CMD_XPWRITE_10                     0x51
+#define SCSI_CMD_XDREAD_10                      0x52
+#define SCSI_CMD_XDWRITEREAD_10                 0x53
+
+#define SCSI_CMD_FORMAT_UNIT                    0x04
+
+#define SCSI_CMD_RCV_DIAG_RSLT                  0x1C
+#define SCSI_CMD_SND_DIAG                       0x1D
+
+/* MMC */
+#define SCSI_CMD_REPORT_LUN                     0xA0
+#define SCSI_CMD_PREVENT_MEDIUM_REMOVAL         0x1E
+#define SCSI_CMD_READ_SUB_CHANNEL               0x42
+#define SCSI_CMD_READ_TOC                       0x43
+#define SCSI_CMD_READ_DISC_STRUCTURE            0xAD
+#define SCSI_CMD_READ_CD                        0xBE
+#define SCSI_CMD_GET_EVENT_STATUS_NOTIFICATION  0x4A
+#define SCSI_CMD_BLANK                          0xA1
+#define SCSI_CMD_READ_DISC_INFO                 0x51
+
+#ifndef SMART_CMD
+#define SMART_CMD                               0xb0
+#endif /* SMART_CMD */
+
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+#define ATA_CMD_SLEEP               0xe6
+#define ATA_CMD_CHK_POWER           0xe5
+#define ATA_CMD_IDLE                0xe3
+#define ATA_CMD_STANDBY             0xe2
+#define ATA_CMD_IDLEIMMEDIATE       0xe1
+#define ATA_CMD_STANDBYNOW1         0xe0
+#define ATA_CMD_DEV_RESET           0x08
+#endif
+
+#define SCSI_IS_READ(cmd)                       \
+           (((cmd) == SCSI_CMD_READ_6) ||       \
+	    ((cmd) == SCSI_CMD_READ_10)  ||     \
+            ((cmd) == SCSI_CMD_READ_12)  ||     \
+	    ((cmd) == SCSI_CMD_READ_16))
+
+#define SCSI_IS_WRITE(cmd)                      \
+           (((cmd) == SCSI_CMD_WRITE_6)  ||     \
+	    ((cmd) == SCSI_CMD_WRITE_10) ||     \
+	    ((cmd) == SCSI_CMD_WRITE_12) ||     \
+	    ((cmd) == SCSI_CMD_WRITE_16))
+
+#define SCSI_IS_MODE_SENSE(cmd)                 \
+           (((cmd) == SCSI_CMD_MODE_SENSE_6) || \
+	    ((cmd) == SCSI_CMD_MODE_SENSE_10))
+
+#define SCSI_IS_REQUEST_SENSE(cmd)              \
+           (((cmd) == SCSI_CMD_REQUEST_SENSE))
+
+#define SCSI_IS_VERIFY(cmd)                     \
+           (((cmd) == SCSI_CMD_VERIFY_10) ||    \
+	    ((cmd) == SCSI_CMD_VERIFY_16))
+
+#define SCSI_CMD_MARVELL_SPECIFIC               0xE1
+#   define CDB_CORE_MODULE                      0x1
+#      define CDB_CORE_SOFT_RESET_1				0x1
+#      define CDB_CORE_SOFT_RESET_0				0x2
+#      define CDB_CORE_IDENTIFY                 0x3
+#      define CDB_CORE_SET_UDMA_MODE            0x4
+#      define CDB_CORE_SET_PIO_MODE             0x5
+#      define CDB_CORE_ENABLE_WRITE_CACHE       0x6
+#      define CDB_CORE_DISABLE_WRITE_CACHE      0x7
+#      define CDB_CORE_ENABLE_SMART             0x8
+#      define CDB_CORE_DISABLE_SMART            0x9
+#      define CDB_CORE_SMART_RETURN_STATUS      0xA
+#      define CDB_CORE_SHUTDOWN                 0xB
+#      define CDB_CORE_ENABLE_READ_AHEAD        0xC
+#      define CDB_CORE_DISABLE_READ_AHEAD       0xD
+#      define CDB_CORE_READ_LOG_EXT             0xE
+#      define CDB_CORE_TASK_MGMT                0xF
+#      define CDB_CORE_SMP                      0x10
+#      define CDB_CORE_PM_READ_REG				0x11
+#      define CDB_CORE_PM_WRITE_REG				0x12
+#	   define CDB_CORE_RESET_DEVICE				0x13
+#	   define CDB_CORE_RESET_PORT				0x14
+#      define CDB_CORE_OS_SMART_CMD				0x15
+
+#      define CDB_CORE_ATA_SLEEP                                0x16
+#      define CDB_CORE_ATA_IDLE                                 0x17
+#      define CDB_CORE_ATA_STANDBY                              0x18
+#      define CDB_CORE_ATA_IDLE_IMMEDIATE                       0x19
+#      define CDB_CORE_ATA_STANDBY_IMMEDIATE                    0x1A
+#      define CDB_CORE_ATA_CHECK_POWER_MODE                     0x1B
+
+
+#      define    CDB_CORE_ATA_IDENTIFY_DEVICE   0x1C
+#      define    CDB_CORE_ATA_IDENTIFY_PACKET_DEVICE    0x1D
+#      define   CDB_CORE_ATA_SMART_READ_VALUES     0x1E
+#      define   CDB_CORE_ATA_SMART_READ_THRESHOLDS     0x1F
+#      define   CDB_CORE_ATA_SMART_READ_LOG_SECTOR     0x20
+#      define   CDB_CORE_ATA_SMART_WRITE_LOG_SECTOR      0x21
+#      define   CDB_CORE_ATA_SMART_AUTO_OFFLINE    0x22
+#      define   CDB_CORE_ATA_SMART_AUTOSAVE          0x23
+#      define   CDB_CORE_ATA_SMART_IMMEDIATE_OFFLINE    0x24
+#      define   CDB_CORE_ATA_IDENTIFY                 0x25
+#      define   CDB_CORE_ATA_DOWNLOAD_MICROCODE       0x26
+
+
+#      define SMP_CDB_USE_ADDRESS               0x01
+
+#define SCSI_IS_INTERNAL(cmd)        ((cmd) == SCSI_CMD_MARVELL_SPECIFIC)
+
+#ifdef SIMULATOR
+#   define SCSI_CMD_READ_SCATTER				0xEE
+#   define SCSI_CMD_WRITE_SCATTER				0xEF
+#endif	// SIMULATOR
+
+/*
+ * SCSI status
+ */
+#define SCSI_STATUS_GOOD                        0x00
+#define SCSI_STATUS_CHECK_CONDITION             0x02
+#define SCSI_STATUS_CONDITION_MET               0x04
+#define SCSI_STATUS_BUSY                        0x08
+#define SCSI_STATUS_INTERMEDIATE                0x10
+#define SCSI_STATUS_INTERMEDIATE_MET            0x14
+#define SCSI_STATUS_RESERVATION_CONFLICT        0x18
+#define SCSI_STATUS_FULL                        0x28
+#define SCSI_STATUS_ACA_ACTIVE                  0x30
+#define SCSI_STATUS_ABORTED                     0x40
+
+/*
+ * SCSI sense key
+ */
+#define SCSI_SK_NO_SENSE                        0x00
+#define SCSI_SK_RECOVERED_ERROR                 0x01
+#define SCSI_SK_NOT_READY                       0x02
+#define SCSI_SK_MEDIUM_ERROR                    0x03
+#define SCSI_SK_HARDWARE_ERROR                  0x04
+#define SCSI_SK_ILLEGAL_REQUEST                 0x05
+#define SCSI_SK_UNIT_ATTENTION                  0x06
+#define SCSI_SK_DATA_PROTECT                    0x07
+#define SCSI_SK_BLANK_CHECK                     0x08
+#define SCSI_SK_VENDOR_SPECIFIC                 0x09
+#define SCSI_SK_COPY_ABORTED                    0x0A
+#define SCSI_SK_ABORTED_COMMAND                 0x0B
+#define SCSI_SK_VOLUME_OVERFLOW                 0x0D
+#define SCSI_SK_MISCOMPARE                      0x0E
+#ifdef _XOR_DMA
+#define SCSI_SK_DMA					0x0F
+#endif
+
+/*
+ * SCSI additional sense code
+ */
+#define SCSI_ASC_NO_ASC                         0x00
+#define SCSI_ASC_LUN_NOT_READY                  0x04
+#define SCSI_ASC_ECC_ERROR                      0x10
+#define SCSI_ASC_ID_ADDR_MARK_NOT_FOUND         0x12
+#define SCSI_ASC_INVALID_OPCODE                 0x20
+#define SCSI_ASC_LBA_OUT_OF_RANGE               0x21
+#define SCSI_ASC_INVALID_FEILD_IN_CDB           0x24
+#define SCSI_ASC_LOGICAL_UNIT_NOT_SUPPORTED     0x25
+#define SCSI_ASC_SAVING_PARAMETERS_NOT_SUPPORT  0x39
+#define SCSI_ASC_LOGICAL_UNIT_NOT_RESP_TO_SEL	0x05
+#define SCSI_ASC_INVALID_FIELD_IN_PARAMETER     0x26
+#define SCSI_ASC_INTERNAL_TARGET_FAILURE        0x44
+#define SCSI_ASC_FAILURE_PREDICTION_THRESHOLD_EXCEEDED	0x5D
+
+#ifdef _OS_LINUX	/* below is defined in Windows DDK scsi.h */
+#define SCSI_ADSENSE_NO_SENSE  0x98
+#define SCSI_ADSENSE_INVALID_CDB 0x99
+#endif
+
+/*
+ * SCSI additional sense code qualifier
+ */
+#define SCSI_ASCQ_NO_ASCQ                       0x00
+#define SCSI_ASCQ_INTERVENTION_REQUIRED         0x03
+#define SCSI_ASCQ_MAINTENANCE_IN_PROGRESS       0x80
+#define SCSI_ASCQ_HIF_GENERAL_HD_FAILURE		0x10
+
+/* SCSI command CDB helper functions. */
+#define SCSI_CDB10_GET_LBA(cdb)                  \
+           ((MV_U32) (((MV_U32) cdb[2] << 24) |  \
+		      ((MV_U32) cdb[3] << 16) |  \
+		      ((MV_U32) cdb[4] << 8)  |  \
+		      (MV_U32) cdb[5]))
+
+#define SCSI_CDB10_SET_LBA(cdb, lba)             \
+           {                                     \
+              cdb[2] = (MV_U8)(lba >> 24);       \
+              cdb[3] = (MV_U8)(lba >> 16);       \
+              cdb[4] = (MV_U8)(lba >> 8);        \
+              cdb[5] = (MV_U8)lba;               \
+           }
+#define SCSI_CDB10_GET_SECTOR(cdb)    ((cdb[7] << 8) | cdb[8])
+
+#define SCSI_CDB10_SET_SECTOR(cdb, sector)      \
+           {                                    \
+              cdb[7] = (MV_U8)(sector >> 8);    \
+              cdb[8] = (MV_U8)sector;           \
+           }
+
+#define MV_SCSI_RESPONSE_CODE                   0x70
+#define MV_SCSI_DIRECT_ACCESS_DEVICE            0x00
+
+typedef struct _MV_Sense_Data
+{
+	MV_U8 ErrorCode:7;
+	MV_U8 Valid:1;
+	MV_U8 SegmentNumber;
+	MV_U8 SenseKey:4;
+	MV_U8 Reserved:1;
+	MV_U8 IncorrectLength:1;
+	MV_U8 EndOfMedia:1;
+	MV_U8 FileMark:1;
+	MV_U8 Information[4];
+	MV_U8 AdditionalSenseLength;
+	MV_U8 CommandSpecificInformation[4];
+	MV_U8 AdditionalSenseCode;
+	MV_U8 AdditionalSenseCodeQualifier;
+	MV_U8 FieldReplaceableUnitCode;
+	MV_U8 SenseKeySpecific[3];
+}MV_Sense_Data, *PMV_Sense_Data;
+
+MV_VOID MV_SetSenseData(
+	IN PMV_Sense_Data pSense,
+	IN MV_U8 SenseKey,
+	IN MV_U8 AdditionalSenseCode,
+	IN MV_U8 ASCQ
+	);
+
+/* Virtual Device Inquiry Related */
+#define VIRTUALD_INQUIRY_DATA_SIZE		36
+#define VPD_PAGE0_VIRTUALD_SIZE			7
+#define VPD_PAGE80_VIRTUALD_SIZE		12
+#define VPD_PAGE83_VIRTUALD_SIZE		24
+
+#ifndef SUPPORT_VIRTUAL_DEVICE
+extern MV_U8 BASEATTR MV_INQUIRY_VIRTUALD_DATA[];
+#define MV_INQUIRY_VPD_PAGE0_VIRTUALD_DATA	MV_INQUIRY_VPD_PAGE0_DEVICE_DATA
+extern MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE80_VIRTUALD_DATA[];
+#define MV_INQUIRY_VPD_PAGE83_VIRTUALD_DATA	MV_INQUIRY_VPD_PAGE83_DEVICE_DATA
+#else
+extern MV_U8 BASEATTR MV_INQUIRY_VIRTUALD_DATA[];
+extern MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE0_VIRTUALD_DATA[];
+extern MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE80_VIRTUALD_DATA[];
+extern MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE83_VIRTUALD_DATA[];
+#endif
+
+#endif /*  __MV_COM_SCSI_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/com_sgd.h
@@ -0,0 +1,383 @@
+#ifndef __MV_COM_SGD_H__
+#define __MV_COM_SGD_H__
+
+struct _sgd_tbl_t;
+struct _sgd_t;
+
+#define SGD_DOMAIN_MASK	0xF0000000L
+
+#define SGD_EOT			(1L<<27)	/* End of table */
+#define SGD_COMPACT		(1L<<26)	/* Compact (12 bytes) SG format, not verified yet */
+#define SGD_WIDE		(1L<<25)	/* 32 byte SG format */
+#define SGD_X64			(1L<<24)	/* the 2nd part of SGD_WIDE */
+#define SGD_NEXT_TBL	(1L<<23)	/* Next SG table format */
+#define SGD_VIRTUAL		(1L<<22)	/* Virtual SG format, either 32 or 64 bit is determined during compile time. */
+#define SGD_REFTBL		(1L<<21)	/* sg table reference format, either 32 or 64 bit is determined during compile time. */
+#define SGD_REFSGD		(1L<<20)	/* sg item reference format */
+#define SGD_VP			(1L<<19)	/* virtual and physical, not verified yet */
+#define SGD_VWOXCTX		(1L<<18)	/* virtual without translation context */
+#define SGD_PCTX		(1L<<17)	/* sgd_pctx_t, 64 bit only */
+
+typedef struct _sg_common_t
+{
+	MV_U32	dword1;
+	MV_U32	dword2;
+	MV_U32	flags;	/* SGD_xxx */
+	MV_U32	dword3;
+} sg_common_t;
+
+/*---------------------------------------------------------------------------*/
+
+typedef struct _sgd_t
+{
+	MV_U64	baseAddr;
+	MV_U32	flags;
+	MV_U32	size;
+} sgd_t;
+
+/*---------------------------------------------------------------------------*/
+
+#define GET_COMPACT_SGD_SIZE(sgd)	\
+	((((sgd_compact_t*)(sgd))->flags) & 0x3FFFFFL)
+
+#define SET_COMPACT_SGD_SIZE(sgd,sz) do {			\
+	(((sgd_compact_t*)(sgd))->flags) &= ~0x3FFFFFL;	\
+	(((sgd_compact_t*)(sgd))->flags) |= (sz);		\
+} while(0)
+
+#define SIZEOF_COMPACT_SGD	12
+
+typedef struct _sgd_compact_t
+{
+	MV_U64	baseAddr;
+	MV_U32	flags;
+} sgd_compact_t;
+
+/*---------------------------------------------------------------------------*/
+
+typedef struct _sgd_v32_t
+{
+	MV_PVOID	vaddr;
+	MV_PVOID	xctx;
+	MV_U32		flags;
+	MV_U32		size;
+} sgd_v32_t;
+
+/* sgd_v_t defines 32/64 bit virtual sgd without translation context */
+typedef struct _sgd_v_t
+{
+	union {
+		MV_PVOID	vaddr;
+		MV_U64		dummy;
+	} u;
+	MV_U32		flags;
+	MV_U32		size;
+} sgd_v_t;
+
+typedef struct _sgd_v64_t
+{
+	union {
+		MV_PVOID	vaddr;
+		MV_U64		dummy;
+	} u1;
+	MV_U32	flags;
+	MV_U32	size;
+
+	union {
+		MV_PVOID	xctx;
+		MV_U64		dummy;
+	} u2;
+	MV_U32	flagsEx;
+	MV_U32	rsvd;
+} sgd_v64_t;
+
+/*---------------------------------------------------------------------------*/
+
+typedef struct _sgd_ref32_t
+{
+	MV_PVOID	ref;
+	MV_U32		offset;
+	MV_U32		flags;
+	MV_U32		size;
+} sgd_ref32_t;
+
+typedef struct _sgd_ref64_t
+{
+	union {
+		MV_PVOID	ref;
+		MV_U64		dummy;
+	} u;
+	MV_U32	flags;
+	MV_U32	size;
+
+	MV_U32	offset;
+	MV_U32	rsvd1;
+	MV_U32	flagsEx;
+	MV_U32	rsvd2;
+} sgd_ref64_t;
+
+/*---------------------------------------------------------------------------*/
+
+typedef struct _sgd_nt_t
+{
+	union {
+		struct _sgd_tbl_t*	next;
+		MV_U64	dummy;
+	} u;
+	MV_U32	flags;
+	MV_U32	rsvd;
+} sgd_nt_t;
+
+/*---------------------------------------------------------------------------*/
+
+typedef struct _sgd_vp_t
+{
+	MV_U64	baseAddr;
+	MV_U32	flags;		// SGD_VP | SGD_WIDE
+	MV_U32	size;
+
+	union {
+		MV_PVOID vaddr;
+		MV_U64   dummy;
+	} u;
+	MV_U32	flagsEx;	// SGD_X64
+	MV_U32	rsvd;
+} sgd_vp_t;
+
+/*---------------------------------------------------------------------------*/
+
+typedef struct _sgd_pctx_t
+{
+	MV_U64	baseAddr;
+	MV_U32	flags;		// SGD_PCTX | SGD_WIDE
+	MV_U32	size;
+
+	union {
+		MV_PVOID xctx;
+		MV_U64   dummy;
+	} u;
+	MV_U32	flagsEx;	// SGD_X64
+	MV_U32	rsvd;
+} sgd_pctx_t;
+
+/*---------------------------------------------------------------------------*/
+
+typedef struct _sgd_tbl_t
+{
+	MV_U8 Max_Entry_Count;
+	MV_U8 Valid_Entry_Count;
+	MV_U8 Flag;
+	MV_U8 Reserved0;
+	MV_U32 Byte_Count;
+	sgd_t* Entry_Ptr;
+} sgd_tbl_t;
+
+#define sgd_table_init(sgdt,maxCnt,entries) do {	\
+	MV_ZeroMemory(sgdt,sizeof(sgd_tbl_t));		\
+	(sgdt)->Max_Entry_Count = (maxCnt);				\
+	(sgdt)->Entry_Ptr = (sgd_t*)(entries);			\
+} while(0)
+
+/*---------------------------------------------------------------------------*/
+
+#define sgd_inc(sgd) do {	\
+	if( (sgd)->flags & SGD_COMPACT )					\
+		sgd = (sgd_t*)(((unsigned char*) (sgd)) + 12);	\
+	else if( (sgd)->flags & SGD_WIDE )					\
+		sgd = (sgd_t*)(((unsigned char*) (sgd)) + 32);	\
+	else sgd = (sgd_t*)(((unsigned char*) (sgd)) + 16);	\
+} while(0)
+
+#define sgd_get_vaddr(sgd,v) do {				\
+	if( (sgd)->flags & SGD_VIRTUAL ) {			\
+		if( (sgd)->flags & SGD_WIDE )			\
+			(v) = ((sgd_v64_t*)(sgd))->u1.vaddr;\
+		else (v) = ((sgd_v32_t*)(sgd))->vaddr;	\
+	}											\
+	else if( (sgd)->flags & SGD_VWOXCTX )		\
+		(v) = ((sgd_v_t*)sgd)->u.vaddr;			\
+	else if( (sgd)->flags & SGD_VP )			\
+		(v) = ((sgd_vp_t*)(sgd))->u.vaddr;		\
+	else										\
+		MV_ASSERT(MV_FALSE);					\
+} while(0)
+
+#define sgd_get_xctx(sgd,v) do {	\
+	if( (sgd)->flags & SGD_WIDE )	(v) = ((sgd_v64_t*)(sgd))->u2.xctx;	\
+	else (v) = ((sgd_v32_t*)(sgd))->xctx;	\
+} while(0)
+
+#define sgd_get_ref(sgd,_ref) do {	\
+	if( (sgd)->flags & SGD_WIDE ) (_ref) = ((sgd_ref64_t*)(sgd))->u.ref;	\
+	else (_ref) = ((sgd_ref32_t*)(sgd))->ref;	\
+} while(0)
+
+#define sgd_set_ref(sgd,_ref) do {	\
+	if( (sgd)->flags & SGD_WIDE ) ((sgd_ref64_t*)(sgd))->u.ref = (_ref);	\
+	else ((sgd_ref32_t*)(sgd))->ref = (_ref);	\
+} while(0)
+
+#define sgd_get_reftbl(sgd,reft) do {	\
+	if( (sgd)->flags & SGD_WIDE )		\
+		(reft) = (sgd_tbl_t*) (((sgd_ref64_t*)(sgd))->u.ref);	\
+	else (reft) = (sgd_tbl_t*)(((sgd_ref32_t*)(sgd))->ref);	\
+} while(0)
+
+#define sgd_get_refsgd(sgd,reft) do {	\
+	if( (sgd)->flags & SGD_WIDE )		\
+		(reft) = (sgd_t*) (((sgd_ref64_t*)(sgd))->u.ref);	\
+	else (reft) = (sgd_t*)(((sgd_ref32_t*)(sgd))->ref);	\
+} while(0)
+
+#define sgd_get_refoff(sgd,off) do {	\
+	if( (sgd)->flags & SGD_WIDE )	(off) = ((sgd_ref64_t*)(sgd))->offset;	\
+	else (off) = ((sgd_ref32_t*)(sgd))->offset;	\
+} while(0)
+
+#define sgd_set_refoff(sgd,off) do {	\
+	if( (sgd)->flags & SGD_WIDE )	((sgd_ref64_t*)(sgd))->offset = (off);	\
+	else ((sgd_ref32_t*)(sgd))->offset = (off);	\
+} while(0)
+
+#define sgd_get_nexttbl(sgd,n) do {	\
+	n = ((sgd_nt_t*)(sgd))->u.next;	\
+} while(0)
+
+#define sgd_mark_eot(sgd) \
+	((sgd)->flags |= SGD_EOT)
+
+#define sgd_clear_eot(sgd) \
+	((sgd)->flags &= ~SGD_EOT)
+
+#define sgd_eot(sgd)	\
+	((sgd)->flags & SGD_EOT)
+
+#define sgd_copy(sgdDst,sgdSrc) do {	\
+	*(sgdDst) = *(sgdSrc);	\
+	if( (sgdSrc)->flags & SGD_WIDE )	\
+		(sgdDst)[1] = (sgdSrc)[1];	\
+} while(0)
+
+#define sgd_getsz(sgd,sz) do {				\
+	if( (sgd)->flags & SGD_COMPACT )		\
+		(sz) = GET_COMPACT_SGD_SIZE(sgd);	\
+	else (sz) = (sgd)->size;				\
+} while(0)
+
+#define sgd_setsz(sgd,sz) do {				\
+	if( (sgd)->flags & SGD_COMPACT )		\
+		SET_COMPACT_SGD_SIZE(sgd,sz);		\
+	else (sgd)->size = (sz);				\
+} while(0)
+
+#define sgdt_get_lastsgd(sgdt,sgd) do {		\
+	(sgd) = &(sgdt)->Entry_Ptr[(sgdt)->Valid_Entry_Count];	\
+	(sgd)--;								\
+	if( (sgd)->flags & SGD_X64 ) (sgd)--;	\
+} while(0)
+
+/*---------------------------------------------------------------------------*/
+
+typedef int (*sgd_visitor_t)(sgd_t* sgd, MV_PVOID pContext);
+
+int sgd_table_walk(
+	sgd_tbl_t*		sgdt,
+	sgd_visitor_t	visitor,
+	MV_PVOID		ctx
+	);
+
+/*---------------------------------------------------------------------------*/
+
+typedef struct _sgd_iter_t
+{
+	sgd_t*	sgd;		/* current SG */
+	MV_U32	offset;		/* offset in the SG */
+	MV_U32	remainCnt;
+} sgd_iter_t;
+
+void  sgd_iter_init(
+	sgd_iter_t*	iter,
+	sgd_t*		sgd,
+	MV_U32		offset,
+	MV_U32		count
+	);
+
+int sgd_iter_get_next(
+	sgd_iter_t*	iter,
+	sgd_t*		sgd
+	);
+
+/*---------------------------------------------------------------------------*/
+
+void sgd_dump(sgd_t* sg, char* prefix);
+void sgdt_dump(sgd_tbl_t *SgTbl, char* prefix);
+
+/*---------------------------------------------------------------------------*/
+
+void sgdt_append(
+	sgd_tbl_t*	sgdt,
+	MV_U32		address,
+	MV_U32		addressHigh,
+	MV_U32		size
+	);
+
+void sgdt_append_pctx(
+	sgd_tbl_t*	sgdt,
+	MV_U32		address,
+	MV_U32		addressHigh,
+	MV_U32		size,
+	MV_PVOID	xctx
+	);
+
+int sgdt_append_virtual(
+	sgd_tbl_t* sgdt,
+	MV_PVOID virtual_address,
+	MV_PVOID translation_ctx,
+	MV_U32 size
+	);
+
+int sgdt_append_ref(
+	sgd_tbl_t*	sgdt,
+	MV_PVOID	ref,
+	MV_U32		offset,
+	MV_U32		size,
+	MV_BOOLEAN	refTbl
+	);
+
+int sgdt_append_vp(
+	sgd_tbl_t*	sgdt,
+	MV_PVOID	virtual_address,
+	MV_U32		size,
+	MV_U32		address,
+	MV_U32		addressHigh
+	);
+
+void
+sgdt_copy_partial(
+	sgd_tbl_t* sgdt,
+	sgd_t**	ppsgd,
+	MV_PU32	poff,
+	MV_U32	size
+	);
+
+void sgdt_append_sgd(
+	sgd_tbl_t*	sgdt,
+	sgd_t*		sgd
+	);
+
+#define sgdt_append_reftbl(sgdt,refSgdt,offset,size)	\
+	sgdt_append_ref(sgdt,refSgdt,offset,size,MV_TRUE)
+
+#define sgdt_append_refsgd(sgdt,refSgd,offset,size)	\
+	sgdt_append_ref(sgdt,refSgd,offset,size,MV_FALSE)
+
+/*---------------------------------------------------------------------------*/
+
+int sgdt_prepare_hwprd(
+	MV_PVOID		pCore,
+	sgd_tbl_t*		pSource,
+	sgd_t*			pSg,
+	int				availSgEntry
+	);
+
+#endif	/*__MV_COM_SGD_H__*/
--- /dev/null
+++ b/drivers/scsi/thor/include/com_tag.h
@@ -0,0 +1,33 @@
+#ifndef __MV_COM_TAG_H__
+#define __MV_COM_TAG_H__
+
+#include "com_define.h"
+
+typedef struct _Tag_Stack Tag_Stack, *PTag_Stack;
+
+#define FILO_TAG 0x00
+#define FIFO_TAG 0x01
+
+/* if TagStackType!=FIFO_TAG, use FILO, */
+/* if TagStackType==FIFO_TAG, use FIFO, PtrOut is the next tag to get */
+/*  and Top is the number of available tags in the stack */
+/* when use FIFO, get tag from PtrOut and free tag to (PtrOut+Top)%Size */
+struct _Tag_Stack
+{
+	MV_PU16  Stack;
+	MV_U16   Top;
+	MV_U16   Size;
+	MV_U16   PtrOut;
+	MV_U8    TagStackType;
+#ifndef _OS_BIOS
+	MV_U8    Reserved[1];
+#endif
+};
+
+MV_U16 Tag_GetOne(PTag_Stack pTagStack);
+MV_VOID Tag_ReleaseOne(PTag_Stack pTagStack, MV_U16 tag);
+MV_VOID Tag_Init(PTag_Stack pTagStack, MV_U16 size);
+MV_VOID Tag_Init_FIFO( PTag_Stack pTagStack, MV_U16 size );
+MV_BOOLEAN Tag_IsEmpty(PTag_Stack pTagStack);
+
+#endif /*  __MV_COM_TAG_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/com_type.h
@@ -0,0 +1,427 @@
+#ifndef __MV_COM_TYPE_H__
+#define __MV_COM_TYPE_H__
+
+#include "com_define.h"
+#include "com_list.h"
+
+#include "mv_config.h"	// USE_NEW_SGTABLE is defined in mv_config.h
+
+/*
+ * Data Structure
+ */
+#define MAX_CDB_SIZE                            16
+
+struct _MV_Request;
+typedef struct _MV_Request MV_Request, *PMV_Request;
+
+#ifdef RAID_DRIVER
+typedef struct _MV_XOR_Request MV_XOR_Request, *PMV_XOR_Request;
+#endif /* RAID_DRIVER */
+
+#define REQ_STATUS_SUCCESS                      0x0
+#define REQ_STATUS_NOT_READY                    0x1
+#define REQ_STATUS_MEDIA_ERROR                  0x2
+#define REQ_STATUS_BUSY                         0x3
+#define REQ_STATUS_INVALID_REQUEST              0x4
+#define REQ_STATUS_INVALID_PARAMETER            0x5
+#define REQ_STATUS_NO_DEVICE                    0x6
+/* Sense data structure is the SCSI "Fixed format sense datat" format. */
+#define REQ_STATUS_HAS_SENSE                    0x7
+#define REQ_STATUS_ERROR                        0x8
+#define REQ_STATUS_ERROR_WITH_SENSE             0x10
+/* Request initiator must set the status to REQ_STATUS_PENDING. */
+#define REQ_STATUS_PENDING                      0x80
+#define REQ_STATUS_RETRY                        0x81
+#define REQ_STATUS_REQUEST_SENSE                0x82
+#define REQ_STATUS_ABORT                        0x83
+
+/*
+ * Don't change the order here.
+ * Module_StartAll will start from big id to small id.
+ * Make sure module_set setting matches the Module_Id
+ * MODULE_HBA must be the first one. Refer to Module_AssignModuleExtension.
+ * And HBA_GetNextModuleSendFunction has an assumption that the next level
+ * has larger ID.
+ */
+enum Module_Id
+{
+        MODULE_HBA = 0,
+#ifdef CACHE_MODULE_SUPPORT
+        MODULE_CACHE,
+#endif /*  CACHE_MODULE_SUPPORT */
+
+#ifdef RAID_DRIVER
+        MODULE_RAID,
+#endif /*  RAID_DRIVER */
+        MODULE_CORE,
+        MAX_MODULE_NUMBER
+};
+#define MAX_POSSIBLE_MODULE_NUMBER              MAX_MODULE_NUMBER
+
+#ifdef USE_NEW_SGTABLE
+#include "com_sgd.h"
+
+typedef sgd_tbl_t MV_SG_Table, *PMV_SG_Table;
+typedef sgd_t MV_SG_Entry, *PMV_SG_Entry;
+
+#else
+
+struct _MV_SG_Table;
+typedef struct _MV_SG_Table MV_SG_Table, *PMV_SG_Table;
+
+struct _MV_SG_Entry;
+typedef struct _MV_SG_Entry MV_SG_Entry, *PMV_SG_Entry;
+
+
+
+/* SG Table and SG Entry */
+struct _MV_SG_Entry
+{
+	MV_U32 Base_Address;
+	MV_U32 Base_Address_High;
+	MV_U32 Reserved0;
+	MV_U32 Size;
+};
+
+struct _MV_SG_Table
+{
+	MV_U8 Max_Entry_Count;
+	MV_U8 Valid_Entry_Count;
+	MV_U8 Flag;
+	MV_U8 Reserved0;
+	MV_U32 Byte_Count;
+	PMV_SG_Entry Entry_Ptr;
+};
+
+#endif
+
+#ifdef SIMULATOR
+#define	MV_REQ_COMMON_SCSI	          0
+#define	MV_REQ_COMMON_XOR	          1
+#endif	/*SIMULATOR*/
+
+/*
+ * MV_Request is the general request type passed through different modules.
+ * Must be 64 bit aligned.
+ */
+
+#define DEV_ID_TO_TARGET_ID(_dev_id)    ((MV_U8)((_dev_id) & 0x00FF))
+#define DEV_ID_TO_LUN(_dev_id)                ((MV_U8) (((_dev_id) & 0xFF00) >> 8))
+#define __MAKE_DEV_ID(_target_id, _lun)   (((MV_U16)(_target_id)) | (((MV_U16)(_lun)) << 8))
+
+/*Work around free disks suporting temporarily*/
+#define RESTORE_FREE_DISK_TID 1
+
+typedef void (*MV_ReqCompletion)(MV_PVOID,PMV_Request);
+
+struct _MV_Request {
+#ifdef SIMULATOR
+	MV_U32 CommonType;	// please keep it as the first field
+#endif	/* SIMULATOR */
+#ifdef __RES_MGMT__
+	List_Head pool_entry;     /* don't bother, res_mgmt use only */
+#endif /* __RES_MGMT__ */
+	List_Head Queue_Pointer;
+	List_Head Complete_Queue_Pointer;
+
+#if defined(_OS_LINUX) || defined(__QNXNTO__)
+	List_Head hba_eh_entry;           /* hba's request queue  */
+#endif /* _OS_LINUX || __QNXNTO__ */
+
+	MV_U16 Device_Id;
+
+	MV_U16 Req_Flag;                  /* Check the REQ_FLAG definition */
+	MV_U8 Scsi_Status;
+	MV_U8 Tag;                        /* Request tag */
+	MV_U8 Req_Type;                   /* Check the REQ_TYPE definition */
+#ifdef _OS_WINDOWS
+	MV_U8 Reserved0[1];
+#elif defined(SUPPORT_ERROR_HANDLING) && defined(_OS_LINUX)
+	MV_U8 eh_flag;    /* mark a req after it is re-inserted into
+			   * waiting_list due to error handling.
+			   */
+#else
+	MV_U8 Reserved0[1];
+#endif /* _OS_WINDOWS */
+
+	MV_PVOID Cmd_Initiator;           /* Which module(extension pointer)
+					     creates this request. */
+
+	MV_U8 Sense_Info_Buffer_Length;
+#if RESTORE_FREE_DISK_TID
+	MV_U8 org_target_id;
+#else
+	MV_U8 Reserved1;
+#endif
+	MV_U16 SlotNo;
+	MV_U32 Data_Transfer_Length;
+
+	MV_U8 Cdb[MAX_CDB_SIZE];
+	MV_PVOID Data_Buffer;
+	MV_PVOID Sense_Info_Buffer;
+
+	MV_SG_Table SG_Table;
+
+	MV_PVOID Org_Req;                /* The original request. */
+
+	/* Each module should only use Context to store module information. */
+	MV_PVOID Context[MAX_POSSIBLE_MODULE_NUMBER];
+
+	MV_PVOID Scratch_Buffer;          /* pointer to the scratch buffer
+										 that this request used */
+	MV_PVOID SG_Buffer;
+	MV_PVOID pRaid_Request;
+
+	MV_LBA LBA;
+	MV_U32 Sector_Count;
+	MV_U32 Cmd_Flag;
+
+	MV_U32 Time_Out;                  /* how many seconds we should wait
+					     before treating request as
+					     timed-out */
+	MV_U32 Splited_Count;
+
+#if ERROR_HANDLING_SUPPORT
+	MV_U8   bh_eh_flag;
+	MV_U8   reserved[3];
+	MV_PVOID bh_eh_ctx; /*The type should be MV_PVOID*/
+#endif /* ERROR_HANDLING_SUPPORT */
+#ifdef _OS_LINUX
+	MV_DECLARE_TIMER(eh_timeout);
+ #ifdef SUPPORT_REQUEST_TIMER
+	MV_PVOID	err_request_ctx;
+ #endif
+#endif /* _OS_LINUX */
+#ifdef _OS_LINUX
+#ifdef _32_LEGACY_
+	MV_U8 dummy[4];
+#endif
+#endif /* _OS_LINUX */
+	MV_ReqCompletion	Completion; /* call back function */
+};
+
+#define MV_REQUEST_SIZE                   sizeof(MV_Request)
+/*
+ * Request flag is the flag for the MV_Request data structure.
+ */
+#define REQ_FLAG_LBA_VALID                MV_BIT(0)
+#define REQ_FLAG_CMD_FLAG_VALID           MV_BIT(1)
+#define REQ_FLAG_RETRY                    MV_BIT(2)
+#define REQ_FLAG_INTERNAL_SG              MV_BIT(3)
+#ifndef USE_NEW_SGTABLE
+#define REQ_FLAG_USE_PHYSICAL_SG          MV_BIT(4)
+#define REQ_FLAG_USE_LOGICAL_SG           MV_BIT(5)
+#else
+/*temporarily reserve bit 4 & 5 until NEW_SG is enabled fixedly for all OS */
+#endif
+#define REQ_FLAG_FLUSH                    MV_BIT(6)
+#define REQ_FLAG_CONSOLIDATE			  MV_BIT(8)
+#define REQ_FLAG_NO_CONSOLIDATE           MV_BIT(9)
+#define REQ_FLAG_EXTERNAL				  MV_BIT(10)
+#define REQ_FLAG_CORE_SUB                 MV_BIT(11)
+#define REQ_FLAG_BYPASS_HYBRID            MV_BIT(12)	// hybrid disk simulation
+
+/*
+ * Request Type is the type of MV_Request.
+ */
+enum {
+	/* use a value other than 0, and now they're bit-mapped */
+	REQ_TYPE_OS       = 0x01,
+	REQ_TYPE_RAID     = 0x02,
+	REQ_TYPE_CACHE    = 0x04,
+	REQ_TYPE_INTERNAL = 0x08,
+	REQ_TYPE_SUBLD    = 0x10,
+	REQ_TYPE_SUBBGA   = 0x20,
+	REQ_TYPE_MP       = 0x40,
+};
+
+/*
+ * Command flag is the flag for the CDB command itself
+ */
+/* The first 16 bit can be determined by the initiator. */
+#define CMD_FLAG_NON_DATA                 MV_BIT(0)  /* 1-non data;
+							0-data command */
+#define CMD_FLAG_DMA                      MV_BIT(1)  /* 1-DMA */
+#define CMD_FLAG_PIO					  MV_BIT(2)  /* 1-PIO */
+#define CMD_FLAG_DATA_IN                  MV_BIT(3)  /* 1-host read data */
+#define CMD_FLAG_DATA_OUT                 MV_BIT(4)	 /* 1-host write data */
+#define CMD_FLAG_SMART                    MV_BIT(5)  /* 1-SMART command;0-non SMART command*/
+#define CMD_FLAG_ATA_12      	  	  MV_BIT(6)  /* ATA_12  */
+#define CMD_FLAG_ATA_16      	  	  MV_BIT(7)  /* ATA_16; */
+
+/*
+ * The last 16 bit only can be set by the target. Only core driver knows
+ * the device characteristic.
+ */
+#define CMD_FLAG_NCQ                      MV_BIT(16)
+#define CMD_FLAG_TCQ                      MV_BIT(17)
+#define CMD_FLAG_48BIT                    MV_BIT(18)
+#define CMD_FLAG_PACKET                   MV_BIT(19)  /* ATAPI packet cmd */
+#define CMD_FLAG_SCSI_PASS_THRU           MV_BIT(20)
+#define CMD_FLAG_ATA_PASS_THRU            MV_BIT(21)
+
+#ifdef RAID_DRIVER
+/* XOR request types */
+#define    XOR_REQUEST_WRITE              0
+#define    XOR_REQUEST_COMPARE            1
+#define    XOR_REQUEST_DMA                2
+
+/* XOR request status */
+#define XOR_STATUS_SUCCESS                0
+#define XOR_STATUS_INVALID_REQUEST        1
+#define XOR_STATUS_ERROR                  2
+#define XOR_STATUS_INVALID_PARAMETER      3
+#define XOR_SOURCE_SG_COUNT               11
+#ifdef RAID6_MULTIPLE_PARITY
+#   define XOR_TARGET_SG_COUNT               3
+#else
+#   define XOR_TARGET_SG_COUNT               1
+#endif
+
+typedef MV_U8    XOR_COEF, *PXOR_COEF;        /* XOR Coefficient */
+
+struct _MV_XOR_Request {
+#ifdef SIMULATOR
+	MV_U32 CommonType;	// please keep it as the first field
+#endif	/*SIMULATOR*/
+	List_Head Queue_Pointer;
+
+	MV_U16 Device_Id;
+
+	MV_U8 Request_Type;
+	MV_U8 Request_Status;
+
+	MV_U8 Source_SG_Table_Count;        /* how many items in the
+					       SG_Table_List */
+	MV_U8 Target_SG_Table_Count;
+#ifdef SOFTWARE_XOR
+	MV_U8 Reserved[2];
+#else
+	MV_U16 SlotNo;
+#endif /* SOFTWARE_XOR */
+
+	MV_SG_Table Source_SG_Table_List[XOR_SOURCE_SG_COUNT];
+	MV_SG_Table Target_SG_Table_List[XOR_TARGET_SG_COUNT];
+
+
+	XOR_COEF    Coef[XOR_TARGET_SG_COUNT][XOR_SOURCE_SG_COUNT];
+
+	MV_U32 Error_Offset;                 /* byte, not sector */
+	MV_PVOID Cmd_Initiator;              /* Which module(extension pointer
+						) creates this request. */
+#ifdef RAID6_HARDWARE_XOR
+	MV_PVOID Context[MAX_POSSIBLE_MODULE_NUMBER];
+#else
+	MV_PVOID Context;
+#endif
+
+	MV_PVOID SG_Buffer;
+	void (*Completion)(MV_PVOID, PMV_XOR_Request);    /* callback */
+};
+#endif /* RAID_DRIVER */
+
+typedef struct _MV_Target_ID_Map
+{
+	MV_U16   Device_Id;
+	MV_U8    Type;                    /* 0:LD, 1:Free Disk */
+	MV_U8    Reserved;
+} MV_Target_ID_Map, *PMV_Target_ID_Map;
+
+/* Resource type */
+enum Resource_Type
+{
+	RESOURCE_CACHED_MEMORY = 0,
+#ifdef SUPPORT_DISCARDABLE_MEM
+	RESOURCE_DISCARDABLE_MEMORY,
+#endif
+	RESOURCE_UNCACHED_MEMORY
+};
+
+/* Module event type */
+enum Module_Event
+{
+	EVENT_MODULE_ALL_STARTED = 0,
+#ifdef CACHE_MODULE_SUPPORT
+	EVENT_DEVICE_CACHE_MODE_CHANGED,
+#endif /* CACHE_MODULE_SUPPORT */
+#ifdef SUPPORT_DISCARDABLE_MEM
+	EVENT_SPECIFY_RUNTIME_DEVICE,
+	EVENT_DISCARD_RESOURCE,
+#endif
+	EVENT_DEVICE_ARRIVAL,
+	EVENT_DEVICE_REMOVAL,
+	EVENT_LOG_GENERATED,
+	EVENT_HOT_PLUG,
+};
+
+/* Error_Handling_State */
+enum EH_State
+{
+	EH_NONE = 0,
+	EH_ABORT_REQUEST,
+	EH_LU_RESET,
+	EH_DEVICE_RESET,
+	EH_PORT_RESET,
+	EH_CHIP_RESET,
+	EH_SET_DISK_DOWN
+};
+
+typedef enum
+{
+	EH_REQ_NOP = 0,
+	EH_REQ_ABORT_REQUEST,
+	EH_REQ_HANDLE_TIMEOUT,
+	EH_REQ_RESET_BUS,
+	EH_REQ_RESET_CHANNEL,
+	EH_REQ_RESET_DEVICE,
+	EH_REQ_RESET_ADAPTER
+}eh_req_type_t;
+
+struct mod_notif_param {
+        MV_PVOID  p_param;
+        MV_U16    hi;
+        MV_U16    lo;
+
+        /* for event processing */
+        MV_U32    event_id;
+        MV_U16    dev_id;
+        MV_U8     severity_lvl;
+        MV_U8     param_count;
+};
+
+/*
+ * Exposed Functions
+ */
+
+/*
+ *
+ * Miscellaneous Definitions
+ *
+ */
+/* Rounding */
+
+/* Packed */
+
+#define MV_MAX(x,y)        (((x) > (y)) ? (x) : (y))
+#define MV_MIN(x,y)        (((x) < (y)) ? (x) : (y))
+
+#define MV_MAX_U64(x, y)   ((((x).value) > ((y).value)) ? (x) : (y))
+#define MV_MIN_U64(x, y)   ((((x).value) < ((y).value)) ? (x) : (y))
+
+#define MV_MAX_U8          0xFF
+#define MV_MAX_U16         0xFFFF
+#define MV_MAX_U32         0xFFFFFFFFL
+
+#ifdef _OS_LINUX
+#   define ROUNDING_MASK(x, mask)  (((x)+(mask))&~(mask))
+#   define ROUNDING(value, align)  ROUNDING_MASK(value,   \
+						 (typeof(value)) (align-1))
+#   define OFFSET_OF(type, member) offsetof(type, member)
+#else
+#   define ROUNDING(value, align)  ( ((value)+(align)-1)/(align)*(align) )
+#   define OFFSET_OF(type, member)    ((MV_U32)(MV_PTR_INTEGER)&(((type *) 0)->member))
+#   define ALIGN ROUNDING
+#endif /* _OS_LINUX */
+
+
+#endif /* __MV_COM_TYPE_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/com_u64.h
@@ -0,0 +1,22 @@
+#ifndef _U64_H__
+#define _U64_H__
+
+MV_U64 U64_ADD_U32(MV_U64 v64, MV_U32 v32);
+MV_U64 U64_SUBTRACT_U32(MV_U64 v64, MV_U32 v32);
+MV_U64 U64_MULTIPLY_U32(MV_U64 v64, MV_U32 v32);
+MV_U64 U64_DIVIDE_U32(MV_U64 v64, MV_U32 v32);
+MV_I32 U64_COMPARE_U32(MV_U64 v64, MV_U32 v32);
+MV_U32 U64_MOD_U32(MV_U64 v64, MV_U32 v32);
+
+MV_U64 U64_ADD_U64(MV_U64 v1, MV_U64 v2);
+MV_U64 U64_SUBTRACT_U64(MV_U64 v1, MV_U64 v2);
+MV_U32 U64_DIVIDE_U64(MV_U64 v1, MV_U64 v2);
+MV_I32 U64_COMPARE_U64(MV_U64 v1, MV_U64 v2);
+
+#define U64_SET_VALUE(v64, v32)	do { v64.value = v32; } while(0)
+#define U64_SET_MAX_VALUE(v64)	do { v64.parts.low = v64.parts.high = 0xFFFFFFFFL; } while(0);
+#ifdef _OS_BIOS
+MV_U64 ZeroU64(MV_U64 v1);
+#endif
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/include/com_util.h
@@ -0,0 +1,241 @@
+#ifndef __MV_COM_UTIL_H__
+#define __MV_COM_UTIL_H__
+
+#include "com_define.h"
+#include "com_type.h"
+
+#define MV_ZeroMemory(buf, len)           memset(buf, 0, len)
+#define MV_FillMemory(buf, len, pattern)  memset(buf, pattern, len)
+#define MV_CopyMemory(dest, source, len)  memcpy(dest, source, len)
+
+void MV_ZeroMvRequest(PMV_Request pReq);
+void MV_CopySGTable(PMV_SG_Table pTargetSGTable, PMV_SG_Table pSourceSGTable);
+
+MV_BOOLEAN MV_Equals(MV_PU8 des, MV_PU8 src, MV_U8 len);
+
+#ifndef _OS_BIOS
+#define	U64_ASSIGN(x,y)				  	((x).value = (y))
+#define	U64_ASSIGN_U64(x,y)			  	((x).value = (y).value)
+#define	U64_COMP_U64(x,y)			  	((x) == (y).value)
+#define U64_COMP_U64_VALUE(x,y)			((x).value == (y).value)
+#define U32_ASSIGN_U64(v64, v32)		((v64).value = (v32))
+#define	U64_SHIFT_LEFT(v64, v32)		((v64).value=(v64).value << (v32))
+#define	U64_SHIFT_RIGHT(v64, v32)		((v64).value=(v64).value >> (v32))
+#define U64_ZERO_VALUE(v64)				((v64).value = 0)
+#else
+#define	U64_ASSIGN(x,y)					((x) = (y))
+#define	U64_ASSIGN_U64(x,y)				((x) = (y))
+#define	U64_COMP_U64(x,y)			  	(U64_COMPARE_U64(x, y)==0)
+#define U64_COMP_U64_VALUE(x,y)			(U64_COMPARE_U64(x, y)==0)
+#define U32_ASSIGN_U64(v64, v32)		do { (v64).parts.low = v32; (v64).parts.high = 0; } while(0);
+#define	U64_SHIFT_LEFT(v64, v32)		do { (v64).parts.low=(v64).parts.low << v32; v64.parts.high = 0; } while(0);
+#define	U64_SHIFT_RIGHT(v64, v32)		do { (v64).parts.low=(v64).parts.low >> v32; v64.parts.high = 0; } while(0);
+#define U64_ZERO_VALUE(v64)				do { (v64).parts.low = (v64).parts.high = 0; } while(0);
+#endif
+
+#define MV_SWAP_32(x)                             \
+           (((MV_U32)((MV_U8)(x)))<<24 |          \
+            ((MV_U32)((MV_U8)((x)>>8)))<<16 |     \
+            ((MV_U32)((MV_U8)((x)>>16)))<<8 |     \
+            ((MV_U32)((MV_U8)((x)>>24))) )
+#define MV_SWAP_64(x)                             \
+           (((_MV_U64) (MV_SWAP_32((x).parts.low))) << 32 | \
+	    MV_SWAP_32((x).parts.high))
+#define MV_SWAP_16(x)                             \
+           (((MV_U16) ((MV_U8) (x))) << 8 |       \
+	    (MV_U16) ((MV_U8) ((x) >> 8)))
+
+#if !defined(_OS_LINUX) && !defined(__QNXNTO__)
+#   ifndef __MV_BIG_ENDIAN__
+
+#      define MV_CPU_TO_BE32(x)     MV_SWAP_32(x)
+#      define MV_CPU_TO_BE64(x)     MV_SWAP_64(x)
+#      define MV_CPU_TO_BE16(x)     MV_SWAP_16(x)
+#      define MV_BE16_TO_CPU(x)     MV_SWAP_16(x)
+#      define MV_BE32_TO_CPU(x)     MV_SWAP_32(x)
+#      define MV_BE64_TO_CPU(x)     MV_SWAP_64(x)
+
+#      define MV_CPU_TO_LE16(x)     x
+#      define MV_CPU_TO_LE32(x)     x
+#      define MV_CPU_TO_LE64(x)     x
+#      define MV_LE16_TO_CPU(x)     x
+#      define MV_LE32_TO_CPU(x)     x
+#      define MV_LE64_TO_CPU(x)     x
+
+#   else  /* __MV_BIG_ENDIAN__ */
+
+#      define MV_CPU_TO_BE32(x)     x
+#      define MV_CPU_TO_BE64(x)     x
+#      define MV_CPU_TO_BE16(x)     x
+#      define MV_BE16_TO_CPU(x)     x
+#      define MV_BE32_TO_CPU(x)     x
+#      define MV_BE64_TO_CPU(x)     x
+
+#      define MV_CPU_TO_LE16(x)     MV_SWAP_16(x)
+#      define MV_CPU_TO_LE32(x)     MV_SWAP_32(x)
+#      define MV_CPU_TO_LE64(x)     MV_SWAP_64(x)
+#      define MV_LE16_TO_CPU(x)     MV_SWAP_16(x)
+#      define MV_LE32_TO_CPU(x)     MV_SWAP_32(x)
+#      define MV_LE64_TO_CPU(x)     MV_SWAP_64(x)
+
+#   endif /* __MV_BIG_ENDIAN__ */
+
+#else /* !_OS_LINUX */
+
+#define MV_CPU_TO_LE16      cpu_to_le16
+#define MV_CPU_TO_LE32      cpu_to_le32
+#define MV_CPU_TO_LE64(x)   cpu_to_le64((x).value)
+#define MV_CPU_TO_BE16      cpu_to_be16
+#define MV_CPU_TO_BE32      cpu_to_be32
+#define MV_CPU_TO_BE64(x)   cpu_to_be64((x).value)
+
+#define MV_LE16_TO_CPU      le16_to_cpu
+#define MV_LE32_TO_CPU      le32_to_cpu
+#define MV_LE64_TO_CPU(x)   le64_to_cpu((x).value)
+#define MV_BE16_TO_CPU      be16_to_cpu
+#define MV_BE32_TO_CPU      be32_to_cpu
+#define MV_BE64_TO_CPU(x)   be64_to_cpu((x).value)
+
+#endif /* !_OS_LINUX */
+
+/*
+ * big endian bit-field structs that are larger than a single byte
+ * need swapping
+ */
+#ifdef __MV_BIG_ENDIAN__
+#define MV_CPU_TO_LE16_PTR(pu16)        \
+   *((MV_PU16)(pu16)) = MV_CPU_TO_LE16(*(MV_PU16) (pu16))
+#define MV_CPU_TO_LE32_PTR(pu32)        \
+   *((MV_PU32)(pu32)) = MV_CPU_TO_LE32(*(MV_PU32) (pu32))
+
+#define MV_LE16_TO_CPU_PTR(pu16)        \
+   *((MV_PU16)(pu16)) = MV_LE16_TO_CPU(*(MV_PU16) (pu16))
+#define MV_LE32_TO_CPU_PTR(pu32)        \
+   *((MV_PU32)(pu32)) = MV_LE32_TO_CPU(*(MV_PU32) (pu32))
+# else  /* __MV_BIG_ENDIAN__ */
+#define MV_CPU_TO_LE16_PTR(pu16)        /* Nothing */
+#define MV_CPU_TO_LE32_PTR(pu32)        /* Nothing */
+#define MV_LE16_TO_CPU_PTR(pu32)
+#define MV_LE32_TO_CPU_PTR(pu32)
+#endif /* __MV_BIG_ENDIAN__ */
+
+/* definitions - following macro names are used by RAID module
+   must keep consistent */
+#define CPU_TO_BIG_ENDIAN_16(x)        MV_CPU_TO_BE16(x)
+#define CPU_TO_BIG_ENDIAN_32(x)        MV_CPU_TO_BE32(x)
+#define CPU_TO_BIG_ENDIAN_64(x)        MV_CPU_TO_BE64(x)
+
+void SGTable_Init(
+    OUT PMV_SG_Table pSGTable,
+    IN MV_U8 flag
+    );
+
+#ifndef USE_NEW_SGTABLE
+void SGTable_Append(
+    OUT PMV_SG_Table pSGTable,
+    MV_U32 address,
+    MV_U32 addressHigh,
+    MV_U32 size
+    );
+#else
+#define SGTable_Append sgdt_append
+#endif
+
+MV_BOOLEAN SGTable_Available(
+    IN PMV_SG_Table pSGTable
+    );
+
+void MV_InitializeTargetIDTable(
+    IN PMV_Target_ID_Map pMapTable
+    );
+
+MV_U16 MV_MapTargetID(
+    IN PMV_Target_ID_Map    pMapTable,
+    IN MV_U16                deviceId,
+    IN MV_U8                deviceType
+    );
+
+MV_U16 MV_MapToSpecificTargetID(
+	IN PMV_Target_ID_Map	pMapTable,
+	IN MV_U16				specificId,
+	IN MV_U16				deviceId,
+	IN MV_U8				deviceType
+	);
+
+MV_U16 MV_RemoveTargetID(
+    IN PMV_Target_ID_Map    pMapTable,
+    IN MV_U16                deviceId,
+    IN MV_U8                deviceType
+    );
+
+MV_U16 MV_GetMappedID(
+	IN PMV_Target_ID_Map	pMapTable,
+	IN MV_U16				deviceId,
+	IN MV_U8				deviceType
+	);
+
+void MV_DecodeReadWriteCDB(
+	IN MV_PU8 Cdb,
+	OUT MV_LBA *pLBA,
+	OUT MV_U32 *pSectorCount);
+
+#define MV_SetLBAandSectorCount(pReq) do {								\
+	MV_DecodeReadWriteCDB(pReq->Cdb, &pReq->LBA, &pReq->Sector_Count);	\
+	pReq->Req_Flag |= REQ_FLAG_LBA_VALID;								\
+} while (0)
+
+void MV_DumpRequest(PMV_Request pReq, MV_BOOLEAN detail);
+#if defined(SUPPORT_RAID6) && defined(RAID_DRIVER)
+void MV_DumpXORRequest(PMV_XOR_Request pXORReq, MV_BOOLEAN detail);
+#endif /* SUPPORT_RAID6 */
+void MV_DumpSGTable(PMV_SG_Table pSGTable);
+const char* MV_DumpSenseKey(MV_U8 sense);
+
+MV_U32 MV_CRC(
+	IN MV_PU8  pData,
+	IN MV_U16  len
+);
+
+#define MV_MOD_ADD(value, mod)                    \
+           do {                                   \
+              (value)++;                          \
+              if ((value) >= (mod))               \
+                 (value) = 0;                     \
+           } while (0);
+
+#ifdef MV_DEBUG
+void MV_CHECK_OS_SG_TABLE(
+    IN PMV_SG_Table pSGTable
+    );
+#endif /* MV_DEBUG */
+
+/* used for endian-ness conversion */
+static inline MV_VOID mv_swap_bytes(MV_PVOID buf, MV_U32 len)
+{
+	MV_U32 i;
+	MV_U8  tmp, *p;
+
+	/* we expect len to be in multiples of 2 */
+	if (len & 0x1)
+		return;
+
+	p = (MV_U8 *) buf;
+	for (i = 0; i < len / 2; i++)
+	{
+		tmp = p[i];
+		p[i] = p[len - i - 1];
+		p[len - i - 1] = tmp;
+	}
+}
+
+
+#ifdef _OS_LINUX
+void List_Add(List_Head *new_one, List_Head *head);
+void List_AddTail(List_Head *new_one, List_Head *head);
+void List_Del(List_Head *entry);
+struct list_head *List_GetFirst(struct list_head *head);
+struct list_head *List_GetLast(struct list_head *head);
+
+#endif
+
+#endif /*  __MV_COM_UTIL_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/csmisas.h
@@ -0,0 +1,1223 @@
+/**************************************************************************
+
+Module Name:
+
+   CSMISAS.H
+
+
+Abstract:
+
+   This file contains constants and data structure definitions used by drivers
+   that support the Common Storage Management Interface specification for
+   SAS or SATA in either the Windows or Linux.
+
+   This should be considered as a reference implementation only.  Changes may
+   be necessary to accommodate a specific build environment or target OS.
+
+Revision History:
+
+   001  SEF   8/12/03  Initial release.
+   002  SEF   8/20/03  Cleanup to match documentation.
+   003  SEF   9/12/03  Additional cleanup, created combined header
+   004  SEF   9/23/03  Changed base types to match linux defaults
+                       Added RAID signature
+                       Added bControllerFlags to CSMI_SAS_CNTLR_CONFIG
+                       Changed CSMI_SAS_BEGIN_PACK to 8 for common structures
+                       Fixed other typos identified in first compilation test
+   005  SEF  10/03/03  Additions to match first version of CSMI document
+   006  SEF  10/14/03  Fixed typedef struct _CSMI_SAS_SMP_PASSTHRU_BUFFER
+                       Added defines for bConnectionRate
+   007  SEF  10/15/03  Added Firmware Download Control Code and support
+                       Added CSMI revision support
+   008  SEF  10/30/03  No functional change, just updated version to track
+                       spec changes
+   009  SEF  12/09/03  No functional change, just updated version to track
+                       spec changes
+   010  SEF   3/11/04  Fixed typedef struct CSMI_SAS_RAID_DRIVES to include the
+                       bFirmware member that is defined in the spec, but
+                       was missing in this file,
+                       added CC_CSMI_SAS_TASK_MANAGEMENT
+   011  SEF   4/02/04  No functional change, added comment line before
+                       CC_CSMI_SAS_TASK_MANAGEMENT
+   012  SEF   4/16/04  Added IOControllerNumber to linux header,
+                       Modified linux control codes to have upper word of
+                       0xCC77.... to indicate CSMI version 77
+                       Added bSignalClass to CC_CSMI_SET_PHY_INFO
+                       Added CC_CSMI_SAS_PHY_CONTROL support
+   013  SEF   5/14/04  Added CC_CSMI_SAS_GET_CONNECTOR_INFO support
+   014  SEF   5/24/04  No functional change, just updated version to track spec
+                       changes
+   015  SEF   6/16/04  changed bPinout to uPinout to reflect proper size,
+                       changed width of bLocation defines to reflect size
+   016  SEF   6/17/04  changed bLengthOfControls in CSMI_SAS_PHY_CONTROL
+                       to be proper size
+   017  SEF   9/17/04  added CSMI_SAS_SATA_PORT_SELECTOR,
+                       CSMI_SAS_LINK_VIRTUAL, CSMI_SAS_CON_NOT_PRESENT, and
+                       CSMI_SAS_CON_NOT_CONNECTED
+   018  SEF   9/20/04  added CSMI_SAS_PHY_USER_PATTERN,
+                       changed definition of CSMI_SAS_PHY_FIXED_PATTERN to not
+                       conflict with activate definition
+   019  SEF  12/06/04  added CSMI_SAS_GET_LOCATION
+                       added bSSPStatus to CSMI_SAS_SSP_PASSTHRU_STATUS
+                       structure
+
+**************************************************************************/
+
+#ifndef _CSMI_SAS_H_
+#define _CSMI_SAS_H_
+
+// CSMI Specification Revision, the intent is that all versions of the
+// specification will be backward compatible after the 1.00 release.
+// Major revision number, corresponds to xxxx. of CSMI specification
+// Minor revision number, corresponds to .xxxx of CSMI specification
+#define CSMI_MAJOR_REVISION   0
+#define CSMI_MINOR_REVISION   83
+
+/*************************************************************************/
+/* TARGET OS LINUX SPECIFIC CODE                                         */
+/*************************************************************************/
+
+// EDM #ifdef _linux
+#ifdef __KERNEL__
+
+
+// Linux base types
+
+#include <linux/types.h>
+
+// pack definition
+
+// EDM #define CSMI_SAS_BEGIN_PACK(x)    pack(x)
+// EDM #define CSMI_SAS_END_PACK         pack()
+
+// IOCTL Control Codes
+// (IoctlHeader.ControlCode)
+
+// Control Codes prior to 0.77
+
+// Control Codes requiring CSMI_ALL_SIGNATURE
+
+// #define CC_CSMI_SAS_GET_DRIVER_INFO    0x12345678
+// #define CC_CSMI_SAS_GET_CNTLR_CONFIG   0x23456781
+// #define CC_CSMI_SAS_GET_CNTLR_STATUS   0x34567812
+// #define CC_CSMI_SAS_FIRMWARE_DOWNLOAD  0x92345678
+
+// Control Codes requiring CSMI_RAID_SIGNATURE
+
+// #define CC_CSMI_SAS_GET_RAID_INFO      0x45678123
+// #define CC_CSMI_SAS_GET_RAID_CONFIG    0x56781234
+
+// Control Codes requiring CSMI_SAS_SIGNATURE
+
+// #define CC_CSMI_SAS_GET_PHY_INFO       0x67812345
+// #define CC_CSMI_SAS_SET_PHY_INFO       0x78123456
+// #define CC_CSMI_SAS_GET_LINK_ERRORS    0x81234567
+// #define CC_CSMI_SAS_SMP_PASSTHRU       0xA1234567
+// #define CC_CSMI_SAS_SSP_PASSTHRU       0xB1234567
+// #define CC_CSMI_SAS_STP_PASSTHRU       0xC1234567
+// #define CC_CSMI_SAS_GET_SATA_SIGNATURE 0xD1234567
+// #define CC_CSMI_SAS_GET_SCSI_ADDRESS   0xE1234567
+// #define CC_CSMI_SAS_GET_DEVICE_ADDRESS 0xF1234567
+// #define CC_CSMI_SAS_TASK_MANAGEMENT    0xA2345678
+
+// Control Codes for 0.77 and later
+
+// Control Codes requiring CSMI_ALL_SIGNATURE
+
+#define CC_CSMI_SAS_GET_DRIVER_INFO    0xCC770001
+#define CC_CSMI_SAS_GET_CNTLR_CONFIG   0xCC770002
+#define CC_CSMI_SAS_GET_CNTLR_STATUS   0xCC770003
+#define CC_CSMI_SAS_FIRMWARE_DOWNLOAD  0xCC770004
+
+// Control Codes requiring CSMI_RAID_SIGNATURE
+
+#define CC_CSMI_SAS_GET_RAID_INFO      0xCC77000A
+#define CC_CSMI_SAS_GET_RAID_CONFIG    0xCC77000B
+
+// Control Codes requiring CSMI_SAS_SIGNATURE
+
+#define CC_CSMI_SAS_GET_PHY_INFO       0xCC770014
+#define CC_CSMI_SAS_SET_PHY_INFO       0xCC770015
+#define CC_CSMI_SAS_GET_LINK_ERRORS    0xCC770016
+#define CC_CSMI_SAS_SMP_PASSTHRU       0xCC770017
+#define CC_CSMI_SAS_SSP_PASSTHRU       0xCC770018
+#define CC_CSMI_SAS_STP_PASSTHRU       0xCC770019
+#define CC_CSMI_SAS_GET_SATA_SIGNATURE 0xCC770020
+#define CC_CSMI_SAS_GET_SCSI_ADDRESS   0xCC770021
+#define CC_CSMI_SAS_GET_DEVICE_ADDRESS 0xCC770022
+#define CC_CSMI_SAS_TASK_MANAGEMENT    0xCC770023
+#define CC_CSMI_SAS_GET_CONNECTOR_INFO 0xCC770024
+#define CC_CSMI_SAS_GET_LOCATION       0xCC770025
+
+// Control Codes requiring CSMI_PHY_SIGNATURE
+
+#define CC_CSMI_SAS_PHY_CONTROL        0xCC77003C
+
+// EDM #pragma CSMI_SAS_BEGIN_PACK(8)
+#pragma pack(8)
+
+// IOCTL_HEADER
+typedef struct _IOCTL_HEADER {
+	__u32 IOControllerNumber;
+	__u32 Length;
+	__u32 ReturnCode;
+	__u32 Timeout;
+	__u16 Direction;
+} IOCTL_HEADER, *PIOCTL_HEADER;
+
+// EDM #pragma CSMI_SAS_END_PACK
+#pragma pack()
+
+#endif
+
+#define __i8    char
+
+/*************************************************************************/
+/* TARGET OS WINDOWS SPECIFIC CODE                                       */
+/*************************************************************************/
+
+#ifdef _WIN32
+
+// windows IOCTL definitions
+
+#ifndef _NTDDSCSIH_
+#include <ntddscsi.h>
+#endif
+
+// pack definition
+
+#if defined _MSC_VER
+   #define CSMI_SAS_BEGIN_PACK(x)    pack(push,x)
+   #define CSMI_SAS_END_PACK         pack(pop)
+#elif defined __BORLANDC__
+   #define CSMI_SAS_BEGIN_PACK(x)    option -a##x
+   #define CSMI_SAS_END_PACK         option -a.
+#else
+   #error "CSMISAS.H - Must externally define a pack compiler designator."
+#endif
+
+// base types
+
+#define __u8    unsigned char
+#define __u32   unsigned long
+#define __u16   unsigned short
+
+#define __i8    char
+
+// IOCTL Control Codes
+// (IoctlHeader.ControlCode)
+
+// Control Codes requiring CSMI_ALL_SIGNATURE
+
+#define CC_CSMI_SAS_GET_DRIVER_INFO    1
+#define CC_CSMI_SAS_GET_CNTLR_CONFIG   2
+#define CC_CSMI_SAS_GET_CNTLR_STATUS   3
+#define CC_CSMI_SAS_FIRMWARE_DOWNLOAD  4
+
+// Control Codes requiring CSMI_RAID_SIGNATURE
+
+#define CC_CSMI_SAS_GET_RAID_INFO      10
+#define CC_CSMI_SAS_GET_RAID_CONFIG    11
+
+// Control Codes requiring CSMI_SAS_SIGNATURE
+
+#define CC_CSMI_SAS_GET_PHY_INFO       20
+#define CC_CSMI_SAS_SET_PHY_INFO       21
+#define CC_CSMI_SAS_GET_LINK_ERRORS    22
+#define CC_CSMI_SAS_SMP_PASSTHRU       23
+#define CC_CSMI_SAS_SSP_PASSTHRU       24
+#define CC_CSMI_SAS_STP_PASSTHRU       25
+#define CC_CSMI_SAS_GET_SATA_SIGNATURE 26
+#define CC_CSMI_SAS_GET_SCSI_ADDRESS   27
+#define CC_CSMI_SAS_GET_DEVICE_ADDRESS 28
+#define CC_CSMI_SAS_TASK_MANAGEMENT    29
+#define CC_CSMI_SAS_GET_CONNECTOR_INFO 30
+#define CC_CSMI_SAS_GET_LOCATION       31
+
+// Control Codes requiring CSMI_PHY_SIGNATURE
+
+#define CC_CSMI_SAS_PHY_CONTROL        60
+
+#define IOCTL_HEADER SRB_IO_CONTROL
+#define PIOCTL_HEADER PSRB_IO_CONTROL
+
+#else /* _WIN32 */
+#define _WIN32                          0
+#endif /* _WIN32 */
+
+/*************************************************************************/
+/* TARGET OS NOT DEFINED ERROR                                           */
+/*************************************************************************/
+
+// EDM #if (!_WIN32 && !_linux)
+#if (!_WIN32 && !__KERNEL__)
+   #error "Unknown target OS."
+#endif
+
+/*************************************************************************/
+/* OS INDEPENDENT CODE                                                   */
+/*************************************************************************/
+
+/* * * * * * * * * * Class Independent IOCTL Constants * * * * * * * * * */
+
+// Return codes for all IOCTL's regardless of class
+// (IoctlHeader.ReturnCode)
+
+#define CSMI_SAS_STATUS_SUCCESS              0
+#define CSMI_SAS_STATUS_FAILED               1
+#define CSMI_SAS_STATUS_BAD_CNTL_CODE        2
+#define CSMI_SAS_STATUS_INVALID_PARAMETER    3
+#define CSMI_SAS_STATUS_WRITE_ATTEMPTED      4
+
+// Signature value
+// (IoctlHeader.Signature)
+
+#define CSMI_ALL_SIGNATURE    "CSMIALL"
+
+// Timeout value default of 60 seconds
+// (IoctlHeader.Timeout)
+
+#define CSMI_ALL_TIMEOUT      60
+
+//  Direction values for data flow on this IOCTL
+// (IoctlHeader.Direction, Linux only)
+#define CSMI_SAS_DATA_READ    0
+#define CSMI_SAS_DATA_WRITE   1
+
+// I/O Bus Types
+// ISA and EISA bus types are not supported
+// (bIoBusType)
+
+#define CSMI_SAS_BUS_TYPE_PCI       3
+#define CSMI_SAS_BUS_TYPE_PCMCIA    4
+
+// Controller Status
+// (uStatus)
+
+#define CSMI_SAS_CNTLR_STATUS_GOOD     1
+#define CSMI_SAS_CNTLR_STATUS_FAILED   2
+#define CSMI_SAS_CNTLR_STATUS_OFFLINE  3
+#define CSMI_SAS_CNTLR_STATUS_POWEROFF 4
+
+// Offline Status Reason
+// (uOfflineReason)
+
+#define CSMI_SAS_OFFLINE_REASON_NO_REASON             0
+#define CSMI_SAS_OFFLINE_REASON_INITIALIZING          1
+#define CSMI_SAS_OFFLINE_REASON_BACKSIDE_BUS_DEGRADED 2
+#define CSMI_SAS_OFFLINE_REASON_BACKSIDE_BUS_FAILURE  3
+
+// Controller Class
+// (bControllerClass)
+
+#define CSMI_SAS_CNTLR_CLASS_HBA    5
+
+// Controller Flag bits
+// (uControllerFlags)
+
+#define CSMI_SAS_CNTLR_SAS_HBA   0x00000001
+#define CSMI_SAS_CNTLR_SAS_RAID  0x00000002
+#define CSMI_SAS_CNTLR_SATA_HBA  0x00000004
+#define CSMI_SAS_CNTLR_SATA_RAID 0x00000008
+
+// for firmware download
+#define CSMI_SAS_CNTLR_FWD_SUPPORT  0x00010000
+#define CSMI_SAS_CNTLR_FWD_ONLINE   0x00020000
+#define CSMI_SAS_CNTLR_FWD_SRESET   0x00040000
+#define CSMI_SAS_CNTLR_FWD_HRESET   0x00080000
+#define CSMI_SAS_CNTLR_FWD_RROM     0x00100000
+
+// Download Flag bits
+// (uDownloadFlags)
+#define CSMI_SAS_FWD_VALIDATE       0x00000001
+#define CSMI_SAS_FWD_SOFT_RESET     0x00000002
+#define CSMI_SAS_FWD_HARD_RESET     0x00000004
+
+// Firmware Download Status
+// (usStatus)
+#define CSMI_SAS_FWD_SUCCESS        0
+#define CSMI_SAS_FWD_FAILED         1
+#define CSMI_SAS_FWD_USING_RROM     2
+#define CSMI_SAS_FWD_REJECT         3
+#define CSMI_SAS_FWD_DOWNREV        4
+
+// Firmware Download Severity
+// (usSeverity>
+#define CSMI_SAS_FWD_INFORMATION    0
+#define CSMI_SAS_FWD_WARNING        1
+#define CSMI_SAS_FWD_ERROR          2
+#define CSMI_SAS_FWD_FATAL          3
+
+/* * * * * * * * * * SAS RAID Class IOCTL Constants  * * * * * * * * */
+
+// Return codes for the RAID IOCTL's regardless of class
+// (IoctlHeader.ControlCode)
+
+#define CSMI_SAS_RAID_SET_OUT_OF_RANGE       1000
+
+// Signature value
+// (IoctlHeader.Signature)
+
+#define CSMI_RAID_SIGNATURE    "CSMIARY"
+
+// Timeout value default of 60 seconds
+// (IoctlHeader.Timeout)
+
+#define CSMI_RAID_TIMEOUT      60
+
+// RAID Types
+// (bRaidType)
+#define CSMI_SAS_RAID_TYPE_NONE     0
+#define CSMI_SAS_RAID_TYPE_0        1
+#define CSMI_SAS_RAID_TYPE_1        2
+#define CSMI_SAS_RAID_TYPE_10       3
+#define CSMI_SAS_RAID_TYPE_5        4
+#define CSMI_SAS_RAID_TYPE_15       5
+#define CSMI_SAS_RAID_TYPE_OTHER    255
+
+// RAID Status
+// (bStatus)
+#define CSMI_SAS_RAID_SET_STATUS_OK          0
+#define CSMI_SAS_RAID_SET_STATUS_DEGRADED    1
+#define CSMI_SAS_RAID_SET_STATUS_REBUILDING  2
+#define CSMI_SAS_RAID_SET_STATUS_FAILED      3
+
+// RAID Drive Status
+// (bDriveStatus)
+#define CSMI_SAS_DRIVE_STATUS_OK          0
+#define CSMI_SAS_DRIVE_STATUS_REBUILDING  1
+#define CSMI_SAS_DRIVE_STATUS_FAILED      2
+#define CSMI_SAS_DRIVE_STATUS_DEGRADED    3
+
+// RAID Drive Usage
+// (bDriveUsage)
+#define CSMI_SAS_DRIVE_CONFIG_NOT_USED 0
+#define CSMI_SAS_DRIVE_CONFIG_MEMBER   1
+#define CSMI_SAS_DRIVE_CONFIG_SPARE    2
+
+/* * * * * * * * * * SAS HBA Class IOCTL Constants * * * * * * * * * */
+
+// Return codes for SAS IOCTL's
+// (IoctlHeader.ReturnCode)
+
+#define CSMI_SAS_PHY_INFO_CHANGED            CSMI_SAS_STATUS_SUCCESS
+#define CSMI_SAS_PHY_INFO_NOT_CHANGEABLE     2000
+#define CSMI_SAS_LINK_RATE_OUT_OF_RANGE      2001
+
+#define CSMI_SAS_PHY_DOES_NOT_EXIST          2002
+#define CSMI_SAS_PHY_DOES_NOT_MATCH_PORT     2003
+#define CSMI_SAS_PHY_CANNOT_BE_SELECTED      2004
+#define CSMI_SAS_SELECT_PHY_OR_PORT          2005
+#define CSMI_SAS_PORT_DOES_NOT_EXIST         2006
+#define CSMI_SAS_PORT_CANNOT_BE_SELECTED     2007
+#define CSMI_SAS_CONNECTION_FAILED           2008
+
+#define CSMI_SAS_NO_SATA_DEVICE              2009
+#define CSMI_SAS_NO_SATA_SIGNATURE           2010
+#define CSMI_SAS_SCSI_EMULATION              2011
+#define CSMI_SAS_NOT_AN_END_DEVICE           2012
+#define CSMI_SAS_NO_SCSI_ADDRESS             2013
+#define CSMI_SAS_NO_DEVICE_ADDRESS           2014
+
+// Signature value
+// (IoctlHeader.Signature)
+
+#define CSMI_SAS_SIGNATURE    "CSMISAS"
+
+// Timeout value default of 60 seconds
+// (IoctlHeader.Timeout)
+
+#define CSMI_SAS_TIMEOUT      60
+
+// Device types
+// (bDeviceType)
+
+#define CSMI_SAS_PHY_UNUSED               0x00
+#define CSMI_SAS_NO_DEVICE_ATTACHED       0x00
+#define CSMI_SAS_END_DEVICE               0x10
+#define CSMI_SAS_EDGE_EXPANDER_DEVICE     0x20
+#define CSMI_SAS_FANOUT_EXPANDER_DEVICE   0x30
+
+// Protocol options
+// (bInitiatorPortProtocol, bTargetPortProtocol)
+
+#define CSMI_SAS_PROTOCOL_SATA   0x01
+#define CSMI_SAS_PROTOCOL_SMP    0x02
+#define CSMI_SAS_PROTOCOL_STP    0x04
+#define CSMI_SAS_PROTOCOL_SSP    0x08
+
+// Negotiated and hardware link rates
+// (bNegotiatedLinkRate, bMinimumLinkRate, bMaximumLinkRate)
+
+#define CSMI_SAS_LINK_RATE_UNKNOWN  0x00
+#define CSMI_SAS_PHY_DISABLED       0x01
+#define CSMI_SAS_LINK_RATE_FAILED   0x02
+#define CSMI_SAS_SATA_SPINUP_HOLD   0x03
+#define CSMI_SAS_SATA_PORT_SELECTOR 0x04
+#define CSMI_SAS_LINK_RATE_1_5_GBPS 0x08
+#define CSMI_SAS_LINK_RATE_3_0_GBPS 0x09
+#define CSMI_SAS_LINK_VIRTUAL       0x10
+
+// Discover state
+// (bAutoDiscover)
+
+#define CSMI_SAS_DISCOVER_NOT_SUPPORTED   0x00
+#define CSMI_SAS_DISCOVER_NOT_STARTED     0x01
+#define CSMI_SAS_DISCOVER_IN_PROGRESS     0x02
+#define CSMI_SAS_DISCOVER_COMPLETE        0x03
+#define CSMI_SAS_DISCOVER_ERROR           0x04
+
+// Programmed link rates
+// (bMinimumLinkRate, bMaximumLinkRate)
+// (bProgrammedMinimumLinkRate, bProgrammedMaximumLinkRate)
+
+#define CSMI_SAS_PROGRAMMED_LINK_RATE_UNCHANGED 0x00
+#define CSMI_SAS_PROGRAMMED_LINK_RATE_1_5_GBPS  0x08
+#define CSMI_SAS_PROGRAMMED_LINK_RATE_3_0_GBPS  0x09
+
+// Link rate
+// (bNegotiatedLinkRate in CSMI_SAS_SET_PHY_INFO)
+
+#define CSMI_SAS_LINK_RATE_NEGOTIATE      0x00
+#define CSMI_SAS_LINK_RATE_PHY_DISABLED   0x01
+
+// Signal class
+// (bSignalClass in CSMI_SAS_SET_PHY_INFO)
+
+#define CSMI_SAS_SIGNAL_CLASS_UNKNOWN     0x00
+#define CSMI_SAS_SIGNAL_CLASS_DIRECT      0x01
+#define CSMI_SAS_SIGNAL_CLASS_SERVER      0x02
+#define CSMI_SAS_SIGNAL_CLASS_ENCLOSURE   0x03
+
+// Link error reset
+// (bResetCounts)
+
+#define CSMI_SAS_LINK_ERROR_DONT_RESET_COUNTS   0x00
+#define CSMI_SAS_LINK_ERROR_RESET_COUNTS        0x01
+
+// Phy identifier
+// (bPhyIdentifier)
+
+#define CSMI_SAS_USE_PORT_IDENTIFIER   0xFF
+
+// Port identifier
+// (bPortIdentifier)
+
+#define CSMI_SAS_IGNORE_PORT           0xFF
+
+// Programmed link rates
+// (bConnectionRate)
+
+#define CSMI_SAS_LINK_RATE_NEGOTIATED  0x00
+#define CSMI_SAS_LINK_RATE_1_5_GBPS    0x08
+#define CSMI_SAS_LINK_RATE_3_0_GBPS    0x09
+
+// Connection status
+// (bConnectionStatus)
+
+#define CSMI_SAS_OPEN_ACCEPT                          0
+#define CSMI_SAS_OPEN_REJECT_BAD_DESTINATION          1
+#define CSMI_SAS_OPEN_REJECT_RATE_NOT_SUPPORTED       2
+#define CSMI_SAS_OPEN_REJECT_NO_DESTINATION           3
+#define CSMI_SAS_OPEN_REJECT_PATHWAY_BLOCKED          4
+#define CSMI_SAS_OPEN_REJECT_PROTOCOL_NOT_SUPPORTED   5
+#define CSMI_SAS_OPEN_REJECT_RESERVE_ABANDON          6
+#define CSMI_SAS_OPEN_REJECT_RESERVE_CONTINUE         7
+#define CSMI_SAS_OPEN_REJECT_RESERVE_INITIALIZE       8
+#define CSMI_SAS_OPEN_REJECT_RESERVE_STOP             9
+#define CSMI_SAS_OPEN_REJECT_RETRY                    10
+#define CSMI_SAS_OPEN_REJECT_STP_RESOURCES_BUSY       11
+#define CSMI_SAS_OPEN_REJECT_WRONG_DESTINATION        12
+
+// SSP Status
+// (bSSPStatus)
+
+#define CSMI_SAS_SSP_STATUS_UNKNOWN     0x00
+#define CSMI_SAS_SSP_STATUS_WAITING     0x01
+#define CSMI_SAS_SSP_STATUS_COMPLETED   0x02
+#define CSMI_SAS_SSP_STATUS_FATAL_ERROR 0x03
+#define CSMI_SAS_SSP_STATUS_RETRY       0x04
+#define CSMI_SAS_SSP_STATUS_NO_TAG      0x05
+
+// SSP Flags
+// (uFlags)
+
+#define CSMI_SAS_SSP_READ           0x00000001
+#define CSMI_SAS_SSP_WRITE          0x00000002
+#define CSMI_SAS_SSP_UNSPECIFIED    0x00000004
+
+#define CSMI_SAS_SSP_TASK_ATTRIBUTE_SIMPLE         0x00000000
+#define CSMI_SAS_SSP_TASK_ATTRIBUTE_HEAD_OF_QUEUE  0x00000010
+#define CSMI_SAS_SSP_TASK_ATTRIBUTE_ORDERED        0x00000020
+#define CSMI_SAS_SSP_TASK_ATTRIBUTE_ACA            0x00000040
+
+// SSP Data present
+// (bDataPresent)
+
+#define CSMI_SAS_SSP_NO_DATA_PRESENT         0x00
+#define CSMI_SAS_SSP_RESPONSE_DATA_PRESENT   0x01
+#define CSMI_SAS_SSP_SENSE_DATA_PRESENT      0x02
+
+// STP Flags
+// (uFlags)
+
+#define CSMI_SAS_STP_READ           0x00000001
+#define CSMI_SAS_STP_WRITE          0x00000002
+#define CSMI_SAS_STP_UNSPECIFIED    0x00000004
+#define CSMI_SAS_STP_PIO            0x00000010
+#define CSMI_SAS_STP_DMA            0x00000020
+#define CSMI_SAS_STP_PACKET         0x00000040
+#define CSMI_SAS_STP_DMA_QUEUED     0x00000080
+#define CSMI_SAS_STP_EXECUTE_DIAG   0x00000100
+#define CSMI_SAS_STP_RESET_DEVICE   0x00000200
+
+// Task Management Flags
+// (uFlags)
+
+#define CSMI_SAS_TASK_IU               0x00000001
+#define CSMI_SAS_HARD_RESET_SEQUENCE   0x00000002
+#define CSMI_SAS_SUPPRESS_RESULT       0x00000004
+
+// Task Management Functions
+// (bTaskManagement)
+
+#define CSMI_SAS_SSP_ABORT_TASK           0x01
+#define CSMI_SAS_SSP_ABORT_TASK_SET       0x02
+#define CSMI_SAS_SSP_CLEAR_TASK_SET       0x04
+#define CSMI_SAS_SSP_LOGICAL_UNIT_RESET   0x08
+#define CSMI_SAS_SSP_CLEAR_ACA            0x40
+#define CSMI_SAS_SSP_QUERY_TASK           0x80
+
+// Task Management Information
+// (uInformation)
+
+#define CSMI_SAS_SSP_TEST           1
+#define CSMI_SAS_SSP_EXCEEDED       2
+#define CSMI_SAS_SSP_DEMAND         3
+#define CSMI_SAS_SSP_TRIGGER        4
+
+// Connector Pinout Information
+// (uPinout)
+
+#define CSMI_SAS_CON_UNKNOWN              0x00000001
+#define CSMI_SAS_CON_SFF_8482             0x00000002
+#define CSMI_SAS_CON_SFF_8470_LANE_1      0x00000100
+#define CSMI_SAS_CON_SFF_8470_LANE_2      0x00000200
+#define CSMI_SAS_CON_SFF_8470_LANE_3      0x00000400
+#define CSMI_SAS_CON_SFF_8470_LANE_4      0x00000800
+#define CSMI_SAS_CON_SFF_8484_LANE_1      0x00010000
+#define CSMI_SAS_CON_SFF_8484_LANE_2      0x00020000
+#define CSMI_SAS_CON_SFF_8484_LANE_3      0x00040000
+#define CSMI_SAS_CON_SFF_8484_LANE_4      0x00080000
+
+// Connector Location Information
+// (bLocation)
+
+// same as uPinout above...
+// #define CSMI_SAS_CON_UNKNOWN              0x01
+#define CSMI_SAS_CON_INTERNAL             0x02
+#define CSMI_SAS_CON_EXTERNAL             0x04
+#define CSMI_SAS_CON_SWITCHABLE           0x08
+#define CSMI_SAS_CON_AUTO                 0x10
+#define CSMI_SAS_CON_NOT_PRESENT          0x20
+#define CSMI_SAS_CON_NOT_CONNECTED        0x80
+
+// Device location identification
+// (bIdentify)
+
+#define CSMI_SAS_LOCATE_UNKNOWN           0x00
+#define CSMI_SAS_LOCATE_FORCE_OFF         0x01
+#define CSMI_SAS_LOCATE_FORCE_ON          0x02
+
+// Location Valid flags
+// (uLocationFlags)
+
+#define CSMI_SAS_LOCATE_SAS_ADDRESS_VALID           0x00000001
+#define CSMI_SAS_LOCATE_SAS_LUN_VALID               0x00000002
+#define CSMI_SAS_LOCATE_ENCLOSURE_IDENTIFIER_VALID  0x00000004
+#define CSMI_SAS_LOCATE_ENCLOSURE_NAME_VALID        0x00000008
+#define CSMI_SAS_LOCATE_BAY_PREFIX_VALID            0x00000010
+#define CSMI_SAS_LOCATE_BAY_IDENTIFIER_VALID        0x00000020
+#define CSMI_SAS_LOCATE_LOCATION_STATE_VALID        0x00000040
+
+/* * * * * * * * SAS Phy Control Class IOCTL Constants * * * * * * * * */
+
+// Return codes for SAS Phy Control IOCTL's
+// (IoctlHeader.ReturnCode)
+
+// Signature value
+// (IoctlHeader.Signature)
+
+#define CSMI_PHY_SIGNATURE    "CSMIPHY"
+
+// Phy Control Functions
+// (bFunction)
+
+// values 0x00 to 0xFF are consistent in definition with the SMP PHY CONTROL
+// function defined in the SAS spec
+#define CSMI_SAS_PC_NOP                   0x00000000
+#define CSMI_SAS_PC_LINK_RESET            0x00000001
+#define CSMI_SAS_PC_HARD_RESET            0x00000002
+#define CSMI_SAS_PC_PHY_DISABLE           0x00000003
+// 0x04 to 0xFF reserved...
+#define CSMI_SAS_PC_GET_PHY_SETTINGS      0x00000100
+
+// Link Flags
+#define CSMI_SAS_PHY_ACTIVATE_CONTROL     0x00000001
+#define CSMI_SAS_PHY_UPDATE_SPINUP_RATE   0x00000002
+#define CSMI_SAS_PHY_AUTO_COMWAKE         0x00000004
+
+// Device Types for Phy Settings
+// (bType)
+#define CSMI_SAS_UNDEFINED 0x00
+#define CSMI_SAS_SATA      0x01
+#define CSMI_SAS_SAS       0x02
+
+// Transmitter Flags
+// (uTransmitterFlags)
+#define CSMI_SAS_PHY_PREEMPHASIS_DISABLED    0x00000001
+
+// Receiver Flags
+// (uReceiverFlags)
+#define CSMI_SAS_PHY_EQUALIZATION_DISABLED   0x00000001
+
+// Pattern Flags
+// (uPatternFlags)
+// #define CSMI_SAS_PHY_ACTIVATE_CONTROL     0x00000001
+#define CSMI_SAS_PHY_DISABLE_SCRAMBLING      0x00000002
+#define CSMI_SAS_PHY_DISABLE_ALIGN           0x00000004
+#define CSMI_SAS_PHY_DISABLE_SSC             0x00000008
+
+#define CSMI_SAS_PHY_FIXED_PATTERN           0x00000010
+#define CSMI_SAS_PHY_USER_PATTERN            0x00000020
+
+// Fixed Patterns
+// (bFixedPattern)
+#define CSMI_SAS_PHY_CJPAT                   0x00000001
+#define CSMI_SAS_PHY_ALIGN                   0x00000002
+
+// Type Flags
+// (bTypeFlags)
+#define CSMI_SAS_PHY_POSITIVE_DISPARITY      0x01
+#define CSMI_SAS_PHY_NEGATIVE_DISPARITY      0x02
+#define CSMI_SAS_PHY_CONTROL_CHARACTER       0x04
+
+// Miscellaneous
+#define SLOT_NUMBER_UNKNOWN   0xFFFF
+
+/*************************************************************************/
+/* DATA STRUCTURES                                                       */
+/*************************************************************************/
+
+/* * * * * * * * * * Class Independent Structures * * * * * * * * * */
+
+// EDM #pragma CSMI_SAS_BEGIN_PACK(8)
+#pragma pack(8)
+
+// CC_CSMI_SAS_DRIVER_INFO
+
+typedef struct _CSMI_SAS_DRIVER_INFO {
+   __u8  szName[81];
+   __u8  szDescription[81];
+   __u16 usMajorRevision;
+   __u16 usMinorRevision;
+   __u16 usBuildRevision;
+   __u16 usReleaseRevision;
+   __u16 usCSMIMajorRevision;
+   __u16 usCSMIMinorRevision;
+} CSMI_SAS_DRIVER_INFO,
+  *PCSMI_SAS_DRIVER_INFO;
+
+typedef struct _CSMI_SAS_DRIVER_INFO_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_DRIVER_INFO Information;
+} CSMI_SAS_DRIVER_INFO_BUFFER,
+  *PCSMI_SAS_DRIVER_INFO_BUFFER;
+
+// CC_CSMI_SAS_CNTLR_CONFIGURATION
+
+typedef struct _CSMI_SAS_PCI_BUS_ADDRESS {
+   __u8  bBusNumber;
+   __u8  bDeviceNumber;
+   __u8  bFunctionNumber;
+   __u8  bReserved;
+} CSMI_SAS_PCI_BUS_ADDRESS,
+  *PCSMI_SAS_PCI_BUS_ADDRESS;
+
+typedef union _CSMI_SAS_IO_BUS_ADDRESS {
+   CSMI_SAS_PCI_BUS_ADDRESS PciAddress;
+   __u8  bReserved[32];
+} CSMI_SAS_IO_BUS_ADDRESS,
+  *PCSMI_SAS_IO_BUS_ADDRESS;
+
+typedef struct _CSMI_SAS_CNTLR_CONFIG {
+   __u32 uBaseIoAddress;
+   struct {
+      __u32 uLowPart;
+      __u32 uHighPart;
+   } BaseMemoryAddress;
+   __u32 uBoardID;
+   __u16 usSlotNumber;
+   __u8  bControllerClass;
+   __u8  bIoBusType;
+   CSMI_SAS_IO_BUS_ADDRESS BusAddress;
+   __u8  szSerialNumber[81];
+   __u16 usMajorRevision;
+   __u16 usMinorRevision;
+   __u16 usBuildRevision;
+   __u16 usReleaseRevision;
+   __u16 usBIOSMajorRevision;
+   __u16 usBIOSMinorRevision;
+   __u16 usBIOSBuildRevision;
+   __u16 usBIOSReleaseRevision;
+   __u32 uControllerFlags;
+   __u16 usRromMajorRevision;
+   __u16 usRromMinorRevision;
+   __u16 usRromBuildRevision;
+   __u16 usRromReleaseRevision;
+   __u16 usRromBIOSMajorRevision;
+   __u16 usRromBIOSMinorRevision;
+   __u16 usRromBIOSBuildRevision;
+   __u16 usRromBIOSReleaseRevision;
+   __u8  bReserved[7];
+} CSMI_SAS_CNTLR_CONFIG,
+  *PCSMI_SAS_CNTLR_CONFIG;
+
+typedef struct _CSMI_SAS_CNTLR_CONFIG_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_CNTLR_CONFIG Configuration;
+} CSMI_SAS_CNTLR_CONFIG_BUFFER,
+  *PCSMI_SAS_CNTLR_CONFIG_BUFFER;
+
+// CC_CSMI_SAS_CNTLR_STATUS
+
+typedef struct _CSMI_SAS_CNTLR_STATUS {
+   __u32 uStatus;
+   __u32 uOfflineReason;
+   __u8  bReserved[28];
+} CSMI_SAS_CNTLR_STATUS,
+  *PCSMI_SAS_CNTLR_STATUS;
+
+typedef struct _CSMI_SAS_CNTLR_STATUS_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_CNTLR_STATUS Status;
+} CSMI_SAS_CNTLR_STATUS_BUFFER,
+  *PCSMI_SAS_CNTLR_STATUS_BUFFER;
+
+// CC_CSMI_SAS_FIRMWARE_DOWNLOAD
+
+typedef struct _CSMI_SAS_FIRMWARE_DOWNLOAD {
+   __u32 uBufferLength;
+   __u32 uDownloadFlags;
+   __u8  bReserved[32];
+   __u16 usStatus;
+   __u16 usSeverity;
+} CSMI_SAS_FIRMWARE_DOWNLOAD,
+  *PCSMI_SAS_FIRMWARE_DOWNLOAD;
+
+typedef struct _CSMI_SAS_FIRMWARE_DOWNLOAD_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_FIRMWARE_DOWNLOAD Information;
+   __u8  bDataBuffer[1];
+} CSMI_SAS_FIRMWARE_DOWNLOAD_BUFFER,
+  *PCSMI_SAS_FIRMWARE_DOWNLOAD_BUFFER;
+
+// CC_CSMI_SAS_RAID_INFO
+
+typedef struct _CSMI_SAS_RAID_INFO {
+   __u32 uNumRaidSets;
+   __u32 uMaxDrivesPerSet;
+   __u8  bReserved[92];
+} CSMI_SAS_RAID_INFO,
+  *PCSMI_SAS_RAID_INFO;
+
+typedef struct _CSMI_SAS_RAID_INFO_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_RAID_INFO Information;
+} CSMI_SAS_RAID_INFO_BUFFER,
+  *PCSMI_SAS_RAID_INFO_BUFFER;
+
+// CC_CSMI_SAS_GET_RAID_CONFIG
+
+typedef struct _CSMI_SAS_RAID_DRIVES {
+   __u8  bModel[40];
+   __u8  bFirmware[8];
+   __u8  bSerialNumber[40];
+   __u8  bSASAddress[8];
+   __u8  bSASLun[8];
+   __u8  bDriveStatus;
+   __u8  bDriveUsage;
+   __u8  bReserved[30];
+} CSMI_SAS_RAID_DRIVES,
+   *PCSMI_SAS_RAID_DRIVES;
+
+typedef struct _CSMI_SAS_RAID_CONFIG {
+   __u32 uRaidSetIndex;
+   __u32 uCapacity;
+   __u32 uStripeSize;
+   __u8  bRaidType;
+   __u8  bStatus;
+   __u8  bInformation;
+   __u8  bDriveCount;
+   __u8  bReserved[20];
+   CSMI_SAS_RAID_DRIVES Drives[1];
+} CSMI_SAS_RAID_CONFIG,
+   *PCSMI_SAS_RAID_CONFIG;
+
+typedef struct _CSMI_SAS_RAID_CONFIG_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_RAID_CONFIG Configuration;
+} CSMI_SAS_RAID_CONFIG_BUFFER,
+  *PCSMI_SAS_RAID_CONFIG_BUFFER;
+
+
+/* * * * * * * * * * SAS HBA Class Structures * * * * * * * * * */
+
+// CC_CSMI_SAS_GET_PHY_INFO
+
+typedef struct _CSMI_SAS_IDENTIFY {
+   __u8  bDeviceType;
+   __u8  bRestricted;
+   __u8  bInitiatorPortProtocol;
+   __u8  bTargetPortProtocol;
+   __u8  bRestricted2[8];
+   __u8  bSASAddress[8];
+   __u8  bPhyIdentifier;
+   __u8  bSignalClass;
+   __u8  bReserved[6];
+} CSMI_SAS_IDENTIFY,
+  *PCSMI_SAS_IDENTIFY;
+
+typedef struct _CSMI_SAS_PHY_ENTITY {
+   CSMI_SAS_IDENTIFY Identify;
+   __u8  bPortIdentifier;
+   __u8  bNegotiatedLinkRate;
+   __u8  bMinimumLinkRate;
+   __u8  bMaximumLinkRate;
+   __u8  bPhyChangeCount;
+   __u8  bAutoDiscover;
+   __u8  bReserved[2];
+   CSMI_SAS_IDENTIFY Attached;
+} CSMI_SAS_PHY_ENTITY,
+  *PCSMI_SAS_PHY_ENTITY;
+
+typedef struct _CSMI_SAS_PHY_INFO {
+   __u8  bNumberOfPhys;
+   __u8  bReserved[3];
+   CSMI_SAS_PHY_ENTITY Phy[32];
+} CSMI_SAS_PHY_INFO,
+  *PCSMI_SAS_PHY_INFO;
+
+typedef struct _CSMI_SAS_PHY_INFO_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_PHY_INFO Information;
+} CSMI_SAS_PHY_INFO_BUFFER,
+  *PCSMI_SAS_PHY_INFO_BUFFER;
+
+// CC_CSMI_SAS_SET_PHY_INFO
+
+typedef struct _CSMI_SAS_SET_PHY_INFO {
+   __u8  bPhyIdentifier;
+   __u8  bNegotiatedLinkRate;
+   __u8  bProgrammedMinimumLinkRate;
+   __u8  bProgrammedMaximumLinkRate;
+   __u8  bSignalClass;
+   __u8  bReserved[3];
+} CSMI_SAS_SET_PHY_INFO,
+  *PCSMI_SAS_SET_PHY_INFO;
+
+typedef struct _CSMI_SAS_SET_PHY_INFO_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_SET_PHY_INFO Information;
+} CSMI_SAS_SET_PHY_INFO_BUFFER,
+  *PCSMI_SAS_SET_PHY_INFO_BUFFER;
+
+// CC_CSMI_SAS_GET_LINK_ERRORS
+
+typedef struct _CSMI_SAS_LINK_ERRORS {
+   __u8  bPhyIdentifier;
+   __u8  bResetCounts;
+   __u8  bReserved[2];
+   __u32 uInvalidDwordCount;
+   __u32 uRunningDisparityErrorCount;
+   __u32 uLossOfDwordSyncCount;
+   __u32 uPhyResetProblemCount;
+} CSMI_SAS_LINK_ERRORS,
+  *PCSMI_SAS_LINK_ERRORS;
+
+typedef struct _CSMI_SAS_LINK_ERRORS_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_LINK_ERRORS Information;
+} CSMI_SAS_LINK_ERRORS_BUFFER,
+  *PCSMI_SAS_LINK_ERRORS_BUFFER;
+
+// CC_CSMI_SAS_SMP_PASSTHRU
+
+typedef struct _CSMI_SAS_SMP_REQUEST {
+   __u8  bFrameType;
+   __u8  bFunction;
+   __u8  bReserved[2];
+   __u8  bAdditionalRequestBytes[1016];
+} CSMI_SAS_SMP_REQUEST,
+  *PCSMI_SAS_SMP_REQUEST;
+
+typedef struct _CSMI_SAS_SMP_RESPONSE {
+   __u8  bFrameType;
+   __u8  bFunction;
+   __u8  bFunctionResult;
+   __u8  bReserved;
+   __u8  bAdditionalResponseBytes[1016];
+} CSMI_SAS_SMP_RESPONSE,
+  *PCSMI_SAS_SMP_RESPONSE;
+
+typedef struct _CSMI_SAS_SMP_PASSTHRU {
+   __u8  bPhyIdentifier;
+   __u8  bPortIdentifier;
+   __u8  bConnectionRate;
+   __u8  bReserved;
+   __u8  bDestinationSASAddress[8];
+   __u32 uRequestLength;
+   CSMI_SAS_SMP_REQUEST Request;
+   __u8  bConnectionStatus;
+   __u8  bReserved2[3];
+   __u32 uResponseBytes;
+   CSMI_SAS_SMP_RESPONSE Response;
+} CSMI_SAS_SMP_PASSTHRU,
+  *PCSMI_SAS_SMP_PASSTHRU;
+
+typedef struct _CSMI_SAS_SMP_PASSTHRU_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_SMP_PASSTHRU Parameters;
+} CSMI_SAS_SMP_PASSTHRU_BUFFER,
+  *PCSMI_SAS_SMP_PASSTHRU_BUFFER;
+
+// CC_CSMI_SAS_SSP_PASSTHRU
+
+typedef struct _CSMI_SAS_SSP_PASSTHRU {
+   __u8  bPhyIdentifier;
+   __u8  bPortIdentifier;
+   __u8  bConnectionRate;
+   __u8  bReserved;
+   __u8  bDestinationSASAddress[8];
+   __u8  bLun[8];
+   __u8  bCDBLength;
+   __u8  bAdditionalCDBLength;
+   __u8  bReserved2[2];
+   __u8  bCDB[16];
+   __u32 uFlags;
+   __u8  bAdditionalCDB[24];
+   __u32 uDataLength;
+} CSMI_SAS_SSP_PASSTHRU,
+  *PCSMI_SAS_SSP_PASSTHRU;
+
+typedef struct _CSMI_SAS_SSP_PASSTHRU_STATUS {
+   __u8  bConnectionStatus;
+   __u8  bSSPStatus;
+   __u8  bReserved[2];
+   __u8  bDataPresent;
+   __u8  bStatus;
+   __u8  bResponseLength[2];
+   __u8  bResponse[256];
+   __u32 uDataBytes;
+} CSMI_SAS_SSP_PASSTHRU_STATUS,
+  *PCSMI_SAS_SSP_PASSTHRU_STATUS;
+
+typedef struct _CSMI_SAS_SSP_PASSTHRU_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_SSP_PASSTHRU Parameters;
+   CSMI_SAS_SSP_PASSTHRU_STATUS Status;
+   __u8  bDataBuffer[1];
+} CSMI_SAS_SSP_PASSTHRU_BUFFER,
+  *PCSMI_SAS_SSP_PASSTHRU_BUFFER;
+
+// CC_CSMI_SAS_STP_PASSTHRU
+
+typedef struct _CSMI_SAS_STP_PASSTHRU {
+   __u8  bPhyIdentifier;
+   __u8  bPortIdentifier;
+   __u8  bConnectionRate;
+   __u8  bReserved;
+   __u8  bDestinationSASAddress[8];
+   __u8  bReserved2[4];
+   __u8  bCommandFIS[20];
+   __u32 uFlags;
+   __u32 uDataLength;
+} CSMI_SAS_STP_PASSTHRU,
+  *PCSMI_SAS_STP_PASSTHRU;
+
+typedef struct _CSMI_SAS_STP_PASSTHRU_STATUS {
+   __u8  bConnectionStatus;
+   __u8  bReserved[3];
+   __u8  bStatusFIS[20];
+   __u32 uSCR[16];
+   __u32 uDataBytes;
+} CSMI_SAS_STP_PASSTHRU_STATUS,
+  *PCSMI_SAS_STP_PASSTHRU_STATUS;
+
+typedef struct _CSMI_SAS_STP_PASSTHRU_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_STP_PASSTHRU Parameters;
+   CSMI_SAS_STP_PASSTHRU_STATUS Status;
+   __u8  bDataBuffer[1];
+} CSMI_SAS_STP_PASSTHRU_BUFFER,
+  *PCSMI_SAS_STP_PASSTHRU_BUFFER;
+
+// CC_CSMI_SAS_GET_SATA_SIGNATURE
+
+typedef struct _CSMI_SAS_SATA_SIGNATURE {
+   __u8  bPhyIdentifier;
+   __u8  bReserved[3];
+   __u8  bSignatureFIS[20];
+} CSMI_SAS_SATA_SIGNATURE,
+  *PCSMI_SAS_SATA_SIGNATURE;
+
+typedef struct _CSMI_SAS_SATA_SIGNATURE_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_SATA_SIGNATURE Signature;
+} CSMI_SAS_SATA_SIGNATURE_BUFFER,
+  *PCSMI_SAS_SATA_SIGNATURE_BUFFER;
+
+// CC_CSMI_SAS_GET_SCSI_ADDRESS
+
+typedef struct _CSMI_SAS_GET_SCSI_ADDRESS_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   __u8  bSASAddress[8];
+   __u8  bSASLun[8];
+   __u8  bHostIndex;
+   __u8  bPathId;
+   __u8  bTargetId;
+   __u8  bLun;
+} CSMI_SAS_GET_SCSI_ADDRESS_BUFFER,
+   *PCSMI_SAS_GET_SCSI_ADDRESS_BUFFER;
+
+// CC_CSMI_SAS_GET_DEVICE_ADDRESS
+
+typedef struct _CSMI_SAS_GET_DEVICE_ADDRESS_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   __u8  bHostIndex;
+   __u8  bPathId;
+   __u8  bTargetId;
+   __u8  bLun;
+   __u8  bSASAddress[8];
+   __u8  bSASLun[8];
+} CSMI_SAS_GET_DEVICE_ADDRESS_BUFFER,
+  *PCSMI_SAS_GET_DEVICE_ADDRESS_BUFFER;
+
+// CC_CSMI_SAS_TASK_MANAGEMENT
+
+typedef struct _CSMI_SAS_SSP_TASK_IU {
+   __u8  bHostIndex;
+   __u8  bPathId;
+   __u8  bTargetId;
+   __u8  bLun;
+   __u32 uFlags;
+   __u32 uQueueTag;
+   __u32 uReserved;
+   __u8  bTaskManagementFunction;
+   __u8  bReserved[7];
+   __u32 uInformation;
+} CSMI_SAS_SSP_TASK_IU,
+  *PCSMI_SAS_SSP_TASK_IU;
+
+typedef struct _CSMI_SAS_SSP_TASK_IU_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_SSP_TASK_IU Parameters;
+   CSMI_SAS_SSP_PASSTHRU_STATUS Status;
+} CSMI_SAS_SSP_TASK_IU_BUFFER,
+  *PCSMI_SAS_SSP_TASK_IU_BUFFER;
+
+// CC_CSMI_SAS_GET_CONNECTOR_INFO
+
+typedef struct _CSMI_SAS_GET_CONNECTOR_INFO {
+   __u32 uPinout;
+   __u8  bConnector[16];
+   __u8  bLocation;
+   __u8  bReserved[15];
+} CSMI_SAS_CONNECTOR_INFO,
+  *PCSMI_SAS_CONNECTOR_INFO;
+
+typedef struct _CSMI_SAS_CONNECTOR_INFO_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   CSMI_SAS_CONNECTOR_INFO Reference[32];
+} CSMI_SAS_CONNECTOR_INFO_BUFFER,
+  *PCSMI_SAS_CONNECTOR_INFO_BUFFER;
+
+// CC_CSMI_SAS_GET_LOCATION
+
+typedef struct _CSMI_SAS_LOCATION_IDENTIFIER {
+   __u32 bLocationFlags;
+   __u8  bSASAddress[8];
+   __u8  bSASLun[8];
+   __u8  bEnclosureIdentifier[8];
+   __u8  bEnclosureName[32];
+   __u8  bBayPrefix[32];
+   __u8  bBayIdentifier;
+   __u8  bLocationState;
+   __u8  bReserved[2];
+} CSMI_SAS_LOCATION_IDENTIFIER,
+  *PCSMI_SAS_LOCATION_IDENTIFIER;
+
+typedef struct _CSMI_SAS_GET_LOCATION_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   __u8  bHostIndex;
+   __u8  bPathId;
+   __u8  bTargetId;
+   __u8  bLun;
+   __u8  bIdentify;
+   __u8  bNumberOfLocationIdentifiers;
+   __u8  bLengthOfLocationIdentifier;
+   CSMI_SAS_LOCATION_IDENTIFIER Location[1];
+} CSMI_SAS_GET_LOCATION_BUFFER,
+  *PCSMI_SAS_GET_LOCATION_BUFFER;
+
+// CC_CSMI_SAS_PHY_CONTROL
+
+typedef struct _CSMI_SAS_CHARACTER {
+   __u8  bTypeFlags;
+   __u8  bValue;
+} CSMI_SAS_CHARACTER,
+  *PCSMI_SAS_CHARACTER;
+
+typedef struct _CSMI_SAS_PHY_CONTROL {
+   __u8  bType;
+   __u8  bRate;
+   __u8  bReserved[6];
+   __u32 uVendorUnique[8];
+   __u32 uTransmitterFlags;
+   __i8  bTransmitAmplitude;
+   __i8  bTransmitterPreemphasis;
+   __i8  bTransmitterSlewRate;
+   __i8  bTransmitterReserved[13];
+   __u8  bTransmitterVendorUnique[64];
+   __u32 uReceiverFlags;
+   __i8  bReceiverThreshold;
+   __i8  bReceiverEqualizationGain;
+   __i8  bReceiverReserved[14];
+   __u8  bReceiverVendorUnique[64];
+   __u32 uPatternFlags;
+   __u8  bFixedPattern;
+   __u8  bUserPatternLength;
+   __u8  bPatternReserved[6];
+   CSMI_SAS_CHARACTER UserPatternBuffer[16];
+} CSMI_SAS_PHY_CONTROL,
+  *PCSMI_SAS_PHY_CONTROL;
+
+typedef struct _CSMI_SAS_PHY_CONTROL_BUFFER {
+   IOCTL_HEADER IoctlHeader;
+   __u32 uFunction;
+   __u8  bPhyIdentifier;
+   __u16 usLengthOfControl;
+   __u8  bNumberOfControls;
+   __u8  bReserved[4];
+   __u32 uLinkFlags;
+   __u8  bSpinupRate;
+   __u8  bLinkReserved[7];
+   __u32 uVendorUnique[8];
+   CSMI_SAS_PHY_CONTROL Control[1];
+} CSMI_SAS_PHY_CONTROL_BUFFER,
+  *PCSMI_SAS_PHY_CONTROL_BUFFER;
+
+// EDM #pragma CSMI_SAS_END_PACK
+#pragma pack()
+
+#endif // _CSMI_SAS_H_
--- /dev/null
+++ b/drivers/scsi/thor/include/generic/com_define.h
@@ -0,0 +1,345 @@
+#ifndef COM_DEFINE_H
+#define COM_DEFINE_H
+
+/*
+ *  This file defines Marvell OS independent primary data type for all OS.
+ *
+ *  We have macros to differentiate different CPU and OS.
+ *
+ *  CPU definitions:
+ *  _CPU_X86_16B
+ *  Specify 16bit x86 platform, this is used for BIOS and DOS utility.
+ *  _CPU_X86_32B
+ *  Specify 32bit x86 platform, this is used for most OS drivers.
+ *  _CPU_IA_64B
+ *  Specify 64bit IA64 platform, this is used for IA64 OS drivers.
+ *  _CPU_AMD_64B
+ *  Specify 64bit AMD64 platform, this is used for AMD64 OS drivers.
+ *
+ *  OS definitions:
+ *  _OS_WINDOWS
+ *  _OS_LINUX
+ *  _OS_FREEBSD
+ *  _OS_BIOS
+ *  __QNXNTO__
+ */
+
+
+#include "mv_os.h"
+
+#if !defined(IN)
+#   define IN
+#endif
+
+#if !defined(OUT)
+#   define OUT
+#endif
+
+#if defined(_OS_LINUX) || defined(__QNXNTO__)
+#   define    BUFFER_PACKED               __attribute__((packed))
+#elif defined(_OS_WINDOWS)
+#   define    BUFFER_PACKED
+#elif defined(_OS_BIOS)
+#   define    BUFFER_PACKED
+#endif /* defined(_OS_LINUX) || defined(__QNXNTO__) */
+
+#define MV_BIT(x)                         (1L << (x))
+
+#if !defined(NULL)
+#   define NULL 0
+#endif  /* NULL */
+
+#define MV_TRUE                           1
+#define MV_FALSE                          0
+
+typedef unsigned char  MV_BOOLEAN, *MV_PBOOLEAN;
+typedef unsigned char  MV_U8, *MV_PU8;
+typedef signed char  MV_I8, *MV_PI8;
+
+typedef unsigned short  MV_U16, *MV_PU16;
+typedef signed short  MV_I16, *MV_PI16;
+
+typedef void    MV_VOID, *MV_PVOID;
+
+#ifdef _OS_BIOS
+    typedef MV_U8 GEN_FAR*  MV_LPU8;
+    typedef MV_I8 GEN_FAR*  MV_LPI8;
+    typedef MV_U16 GEN_FAR* MV_LPU16;
+    typedef MV_I16 GEN_FAR* MV_LPI16;
+
+    typedef MV_U32 GEN_FAR* MV_LPU32;
+    typedef MV_I32 GEN_FAR* MV_LPI32;
+    typedef void GEN_FAR*   MV_LPVOID;
+#else
+    typedef void            *MV_LPVOID;
+#endif /* _OS_BIOS */
+
+/* Pre-define segment in C code*/
+#if defined(_OS_BIOS)
+#   define BASEATTR         __based(__segname("_CODE"))
+#   define BASEATTRData     __based(__segname("_CODE"))
+#else
+#   define BASEATTR
+#endif /* _OS_BIOS */
+
+#ifdef DEBUG_COM_SPECIAL
+	#define MV_DUMP_COM_SPECIAL(pString)  						{bDbgPrintStr(pString);bCOMEnter();}
+	#define MV_DUMP32_COM_SPECIAL(pString, value) 				bDbgPrintStr_U32(pString, value)
+	#define MV_DUMP16_COM_SPECIAL(pString, value)  				bDbgPrintStr_U16(pString, value)
+	#define MV_DUMP32_COM_SPECIAL3(pString, value1, value2)  	bDbgPrintStr_U32_3(pString, value1, value2)
+	#define MV_DUMP8_COM_SPECIAL(pString, value)  				bDbgPrintStr_U8(pString, value)
+	#define MV_HALTKEY_SPECIAL									waitForKeystroke()
+
+#else
+	#define MV_DUMP_COM_SPECIAL(pString)
+	#define MV_DUMP32_COM_SPECIAL(pString, value)
+	#define MV_DUMP16_COM_SPECIAL(pString, value)
+	#define MV_DUMP32_COM_SPECIAL3(pString, value1, value2)
+	#define MV_DUMP8_COM_SPECIAL(pString, value)
+	#define MV_HALTKEY_SPECIAL
+#endif
+/* For debug version only */
+#ifdef DEBUG_BIOS
+ #ifdef DEBUG_SHOW_ALL
+	#define MV_DUMP32(_x_) 		{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMP16(_x_)  	{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMP8(_x_)  		{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMPC32(_x_)  	{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMPC16(_x_)  	{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMPC8(_x_)  	{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMPE32(_x_) 	{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMPE16(_x_) 	{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMPE8(_x_)  	{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMP_COM(pString)  						{bDbgPrintStr(pString);bCOMEnter();}
+	#define MV_DUMP32_COM(pString, value) 				bDbgPrintStr_U32(pString, value)
+	#define MV_DUMP16_COM(pString, value)  				bDbgPrintStr_U16(pString, value)
+	#define MV_DUMP32_COM3(pString, value1, value2)  	bDbgPrintStr_U32_3(pString, value1, value2)
+	#define MV_DUMP8_COM(pString, value)  				bDbgPrintStr_U8(pString, value)
+ #else
+	#define MV_DUMP32(_x_) 		//{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMP16(_x_)  	//{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMP8(_x_)  		//{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMPC32(_x_)  	//{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMPC16(_x_)  	//{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMPC8(_x_)  	//{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMPE32(_x_) 	//{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMPE16(_x_) 	//{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMPE8(_x_)  	//{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMP_COM(pString)  						//{bDbgPrintStr(pString);bCOMEnter();}
+	#define MV_DUMP32_COM(pString, value) 				//bDbgPrintStr_U32(pString, value)
+	#define MV_DUMP16_COM(pString, value)  				//bDbgPrintStr_U16(pString, value)
+	#define MV_DUMP32_COM3(pString, value1, value2)  	//bDbgPrintStr_U32_3(pString, value1, value2)
+	#define MV_DUMP8_COM(pString, value)  				//bDbgPrintStr_U8(pString, value)
+
+ #endif
+
+	#define MV_BIOSDEBUG(pString, value)				bDbgPrintStr_U32(pString, value)
+	#define MV_HALTKEY			waitForKeystroke()
+	#define MV_ENTERLINE		//mvChangLine()
+#else
+	#define MV_DUMP32(_x_) 		//{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMP16(_x_)  	//{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMP8(_x_)  		//{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMPC32(_x_)  	//{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMPC16(_x_)  	//{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMPC8(_x_)  	//{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMPE32(_x_) 	//{mvDebugDumpU32(_x_);bCOMEnter();}
+	#define MV_DUMPE16(_x_) 	//{mvDebugDumpU16(_x_);bCOMEnter();}
+	#define MV_DUMPE8(_x_)  	//{mvDebugDumpU8(_x_);bCOMEnter();}
+	#define MV_DUMP_COM(pString)  						//{bDbgPrintStr(pString);bCOMEnter();}
+	#define MV_DUMP32_COM(pString, value) 				//bDbgPrintStr_U32(pString, value)
+	#define MV_DUMP16_COM(pString, value)  				//bDbgPrintStr_U16(pString, value)
+	#define MV_DUMP32_COM3(pString, value1, value2)  	//bDbgPrintStr_U32_3(pString, value1, value2)
+	#define MV_DUMP8_COM(pString, value)  				//bDbgPrintStr_U8(pString, value)
+
+	#define MV_BIOSDEBUG(pString, value)
+	#define MV_HALTKEY
+	#define MV_ENTERLINE
+
+#endif
+
+#if defined(_OS_LINUX) || defined(__QNXNTO__)
+    /* unsigned/signed long is 64bit for AMD64, so use unsigned int instead */
+    typedef unsigned int MV_U32, *MV_PU32;
+    typedef   signed int MV_I32, *MV_PI32;
+    typedef unsigned long MV_ULONG, *MV_PULONG;
+    typedef   signed long MV_ILONG, *MV_PILONG;
+#else
+    /* unsigned/signed long is 32bit for x86, IA64 and AMD64 */
+    typedef unsigned long MV_U32, *MV_PU32;
+    typedef   signed long MV_I32, *MV_PI32;
+#endif /*  defined(_OS_LINUX) || defined(__QNXNTO__) */
+
+#if defined(_OS_WINDOWS)
+    typedef unsigned __int64 _MV_U64;
+    typedef   signed __int64 _MV_I64;
+#elif defined(_OS_LINUX) || defined(__QNXNTO__)
+    typedef unsigned long long _MV_U64;
+    typedef   signed long long _MV_I64;
+#elif defined(_OS_FREEBSD)
+#   error "No Where"
+#else
+#   error "No Where"
+#endif /* _OS_WINDOWS */
+
+typedef _MV_U64 BUS_ADDRESS;
+
+#ifdef _OS_LINUX
+#   if defined(__KCONF_64BIT__)
+#      define _SUPPORT_64_BIT
+#   else
+#      ifdef _SUPPORT_64_BIT
+#         error Error 64_BIT CPU Macro
+#      endif
+#   endif /* defined(__KCONF_64BIT__) */
+#elif defined(_OS_BIOS) || defined(__QNXNTO__)
+#   undef  _SUPPORT_64_BIT
+#else
+#   define _SUPPORT_64_BIT
+#endif /* _OS_LINUX */
+
+/*
+ * Primary Data Type
+ */
+#if defined(_OS_WINDOWS)
+    typedef union {
+        struct {
+            MV_U32 low;
+            MV_U32 high;
+        } parts;
+        _MV_U64 value;
+    } MV_U64, *PMV_U64, *MV_PU64;
+#elif defined(_OS_LINUX) || defined(__QNXNTO__)
+    typedef union {
+        struct {
+#   if defined (__MV_LITTLE_ENDIAN__)
+            MV_U32 low;
+            MV_U32 high;
+#   elif defined (__MV_BIG_ENDIAN__)
+            MV_U32 high;
+            MV_U32 low;
+#   else
+#           error "undefined endianness"
+#   endif /* __MV_LITTLE_ENDIAN__ */
+        } parts;
+        _MV_U64 value;
+    } MV_U64, *PMV_U64, *MV_PU64;
+#else
+/* BIOS compiler doesn't support 64 bit data structure. */
+    typedef union {
+        struct {
+             MV_U32 low;
+             MV_U32 high;
+        };
+        struct {
+             MV_U32 value;
+             MV_U32 value1;
+        };
+    } _MV_U64,MV_U64, *MV_PU64, *PMV_U64;
+#endif /* defined(_OS_LINUX) || defined(_OS_WINDOWS) || defined(__QNXNTO__)*/
+
+/* PTR_INTEGER is necessary to convert between pointer and integer. */
+#if defined(_SUPPORT_64_BIT)
+   typedef _MV_U64 MV_PTR_INTEGER;
+#else
+   typedef MV_U32 MV_PTR_INTEGER;
+#endif /* defined(_SUPPORT_64_BIT) */
+
+/* LBA is the logical block access */
+typedef MV_U64 MV_LBA;
+
+#if defined(_CPU_16B)
+    typedef MV_U32 MV_PHYSICAL_ADDR;
+#else
+    typedef MV_U64 MV_PHYSICAL_ADDR;
+#endif /* defined(_CPU_16B) */
+
+#if defined (_OS_WINDOWS)
+typedef PVOID MV_FILE_HANDLE;
+#elif defined(_OS_LINUX) || defined(__QNXNTO__)
+typedef MV_I32 MV_FILE_HANDLE;
+#endif
+
+/* Product device id */
+#define VENDOR_ID					0x11AB
+
+#define DEVICE_ID_THORLITE_2S1P				0x6121
+#define DEVICE_ID_THORLITE_0S1P				0x6101
+#define DEVICE_ID_THORLITE_1S1P				0x6111
+#define DEVICE_ID_THOR_4S1P				0x6141
+#define DEVICE_ID_THOR_4S1P_NEW				0x6145
+/* Revision ID starts from B1 */
+#define DEVICE_ID_THORLITE_2S1P_WITH_FLASH              0x6122
+
+ /* Odin lite version */
+#define DEVICE_ID_6320					0x6320
+#define DEVICE_ID_6340					0x6340
+
+/* mule-board */
+#define DEVICE_ID_6440					0x6440
+
+/* Non-RAID Odin */
+#define DEVICE_ID_6445					0x6445
+
+/* mule-board */
+#define DEVICE_ID_6480					0x6480
+#define DEVICE_ID_UNKNOWN				0xFFFF
+
+
+/* OS_LINUX depedent definition*/
+
+#if defined(_OS_LINUX) || defined(__QNXNTO__)
+/* os-dependent data structures */
+#define OSSW_DECLARE_SPINLOCK(x)  spinlock_t x
+#define OSSW_DECLARE_TIMER(x)   struct timer_list x
+#define MV_DECLARE_TIMER(x)   struct timer_list x
+/* expect pointers */
+#define OSSW_INIT_SPIN_LOCK(plock) spin_lock_init(plock)
+
+#define OSSW_SPIN_LOCK_IRQ(plock)            \
+             do {                            \
+	              spin_lock_irq(plock);  \
+             } while (0)
+
+#define OSSW_SPIN_UNLOCK_IRQ(plock)           \
+             do {                             \
+	              spin_unlock_irq(plock); \
+             } while (0)
+
+#define OSSW_SPIN_LOCK_IRQSAVE(plock, flag)           \
+             do {                                     \
+	             spin_lock_irqsave(plock, flag);  \
+             } while (0)
+
+#define OSSW_SPIN_UNLOCK_IRQRESTORE(plock, flag)           \
+             do {                                          \
+	             spin_unlock_irqrestore(plock, flag);  \
+             } while (0)
+
+
+/* Delayed Execution Services */
+#define OSSW_INIT_TIMER(ptimer) init_timer(ptimer)
+
+
+#else
+
+#define OSSW_DECLARE_SPINLOCK(x)
+#define OSSW_DECLARE_TIMER(x)
+#define MV_DECLARE_TIMER(x)
+/* expect pointers */
+#define OSSW_INIT_SPIN_LOCK(plock)
+
+#define OSSW_SPIN_LOCK_IRQ(plock)
+
+#define OSSW_SPIN_UNLOCK_IRQ(plock)
+
+#define OSSW_SPIN_LOCK_IRQSAVE(plock, flag)
+
+#define OSSW_SPIN_UNLOCK_IRQRESTORE(plock, flag)
+
+/* Delayed Execution Services */
+#define OSSW_INIT_TIMER(ptimer)
+
+#endif
+
+#endif /* COM_DEFINE_H */
--- /dev/null
+++ b/drivers/scsi/thor/include/generic/com_error.h
@@ -0,0 +1,61 @@
+#ifndef __COM_ERROR_H__
+#define __COM_ERROR_H__
+#define ERR_GENERIC                              2
+#define ERR_RAID                                 50
+#define ERR_CORE                                 100
+#define ERR_API                                  150
+
+#define ERR_NONE                                 0
+#define ERR_FAIL                                 1
+/* generic error */
+#define ERR_UNKNOWN                              (ERR_GENERIC + 1)
+#define ERR_NO_RESOURCE                          (ERR_GENERIC + 2)
+#define ERR_REQ_OUT_OF_RANGE                     (ERR_GENERIC + 3)
+#define ERR_INVALID_REQUEST                      (ERR_GENERIC + 4)
+#define ERR_INVALID_PARAMETER                    (ERR_GENERIC + 5)
+#define ERR_INVALID_LD_ID                        (ERR_GENERIC + 6)
+#define ERR_INVALID_HD_ID                        (ERR_GENERIC + 7)
+#define ERR_INVALID_EXP_ID                       (ERR_GENERIC + 8)
+#define ERR_INVALID_PM_ID                        (ERR_GENERIC + 9)
+#define ERR_INVALID_BLOCK_ID                     (ERR_GENERIC + 10)
+#define ERR_INVALID_ADAPTER_ID                   (ERR_GENERIC + 11)
+#define ERR_INVALID_RAID_MODE                    (ERR_GENERIC + 12)
+
+/* RAID errors */
+#define ERR_TARGET_IN_LD_FUNCTIONAL              (ERR_RAID + 1)
+#define ERR_TARGET_NO_ENOUGH_SPACE               (ERR_RAID + 2)
+#define ERR_HD_IS_NOT_SPARE                      (ERR_RAID + 3)
+#define ERR_HD_IS_SPARE                          (ERR_RAID + 4)
+#define ERR_HD_NOT_EXIST                         (ERR_RAID + 5)
+#define ERR_HD_IS_ASSIGNED_ALREADY               (ERR_RAID + 6)
+#define ERR_INVALID_HD_COUNT                     (ERR_RAID + 7)
+#define ERR_LD_NOT_READY                         (ERR_RAID + 8)
+#define ERR_LD_NOT_EXIST                         (ERR_RAID + 9)
+#define ERR_LD_IS_FUNCTIONAL                     (ERR_RAID + 10)
+#define ERR_HAS_BGA_ACTIVITY                     (ERR_RAID + 11)
+#define ERR_NO_BGA_ACTIVITY                      (ERR_RAID + 12)
+#define ERR_BGA_RUNNING                          (ERR_RAID + 13)
+#define ERR_RAID_NO_AVAILABLE_ID                 (ERR_RAID + 14)
+#define ERR_LD_NO_ATAPI                          (ERR_RAID + 15)
+#define ERR_INVALID_RAID6_PARITY_DISK_COUNT      (ERR_RAID + 16)
+#define ERR_INVALID_BLOCK_SIZE                   (ERR_RAID + 17)
+#define ERR_MIGRATION_NOT_NEED                   (ERR_RAID + 18)
+#define ERR_STRIPE_BLOCK_SIZE_MISMATCH           (ERR_RAID + 19)
+#define ERR_MIGRATION_NOT_SUPPORT                (ERR_RAID + 20)
+#define ERR_LD_NOT_FULLY_INITED                  (ERR_RAID + 21)
+#define ERR_LD_NAME_INVALID	                     (ERR_RAID + 22)
+
+/* API errors */
+#define ERR_INVALID_MATCH_ID                     (ERR_API + 1)
+#define ERR_INVALID_HDCOUNT                      (ERR_API + 2)
+#define ERR_INVALID_BGA_ACTION                   (ERR_API + 3)
+#define ERR_HD_IN_DIFF_CARD                      (ERR_API + 4)
+#define ERR_INVALID_FLASH_TYPE                   (ERR_API + 5)
+#define ERR_INVALID_FLASH_ACTION                 (ERR_API + 6)
+#define ERR_TOO_FEW_EVENT                        (ERR_API + 7)
+#define ERR_VD_HAS_RUNNING_OS                    (ERR_API + 8)
+#define ERR_DISK_HAS_RUNNING_OS                  (ERR_API + 9)
+#define ERR_COMMAND_NOT_SUPPURT                  (ERR_API + 10)
+#define ERR_MIGRATION_LIMIT	                     (ERR_API + 11)
+
+#endif /*  __COM_ERROR_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/generic/com_event_define.h
@@ -0,0 +1,353 @@
+#ifndef COM_EVENT_DEFINE_H
+#define COM_EVENT_DEFINE_H
+
+/****************************************
+ *         Perceived Severity
+ ****************************************/
+
+#define SEVERITY_UNKNOWN    0
+#define SEVERITY_OTHER      1
+#define SEVERITY_INFO       2
+#define SEVERITY_WARNING    3  /* used when its appropriate to let the
+				  user decide if action is needed */
+#define SEVERITY_MINOR      4  /* indicate action is needed, but the
+				  situation is not serious at this time */
+#define SEVERITY_MAJOR      5  /* indicate action is needed NOW */
+#define SEVERITY_CRITICAL   6  /* indicate action is needed NOW and the
+				  scope is broad */
+#define SEVERITY_FATAL      7  /* indicate an error occurred, but it's too
+				  late to take remedial action */
+
+/****************************************
+ *             Event Classes
+ ****************************************/
+#define EVT_CLASS_ADAPTER   0
+#define EVT_CLASS_LD        1  /* Logical Drive */
+#define EVT_CLASS_HD        2  /* Hard Drive */
+#define EVT_CLASS_PM        3  /* Port Multplier */
+#define EVT_CLASS_EXPANDER  4
+#define EVT_CLASS_MDD       5
+#define EVT_CLASS_BSL       6  /* Bad Sector Lock */
+
+/********************************************************
+ *                 Event Codes
+ *
+ *  !!!  When adding an EVT_CODE, Please put its severity level
+ *  !!!  and suggested mesage string as comments.  This is the
+ *  !!!  only place to document how 'Params' in 'DriverEvent'
+ *  !!!  structure is to be used.
+ *
+ ********************************************************/
+
+/* Event code for EVT_CLASS_LD (Logical Drive) */
+#define EVT_CODE_LD_OFFLINE                0
+#define EVT_CODE_LD_ONLINE                 1
+#define EVT_CODE_LD_CREATE                 2
+#define EVT_CODE_LD_DELETE                 3
+#define EVT_CODE_LD_DEGRADE                4
+#define EVT_CODE_LD_PARTIALLYOPTIMAL       5
+#define EVT_CODE_LD_CACHE_MODE_CHANGE      6
+#define EVT_CODE_LD_FIXED                  7
+#define EVT_CODE_LD_FOUND_ERROR            8
+#define EVT_CODE_LD_RESERVED1              9
+#define EVT_CODE_LD_RESERVED2              10
+#define EVT_CODE_LD_RESERVED3              11
+#define EVT_CODE_LD_RESERVED4              12
+#define EVT_CODE_LD_RESERVED5              13
+#define EVT_CODE_LD_RESERVED6              14
+#define EVT_CODE_LD_RESERVED7              15
+#define EVT_CODE_LD_RESERVED8              16
+#define EVT_CODE_LD_RESERVED9              17
+#define EVT_CODE_LD_RESERVED10             18
+#define EVT_CODE_LD_RESERVED11             19
+/*
+ *  NOTE: Don't change the following event code order in each event group!
+ *      See raid_get_bga_event_id() for detail.
+ */
+#define EVT_CODE_LD_CHECK_START            20
+#define EVT_CODE_LD_CHECK_RESTART          21
+#define EVT_CODE_LD_CHECK_PAUSE            22
+#define EVT_CODE_LD_CHECK_RESUME           23
+#define EVT_CODE_LD_CHECK_ABORT            24
+#define EVT_CODE_LD_CHECK_COMPLETE         25
+#define EVT_CODE_LD_CHECK_PROGRESS         26
+#define EVT_CODE_LD_CHECK_ERROR            27
+#define EVT_CODE_LD_CHECK_AUTO_PAUSED      28
+#define EVT_CODE_LD_CHECK_AUTO_RESUME      29
+
+#define EVT_CODE_LD_FIX_START              30
+#define EVT_CODE_LD_FIX_RESTART            31
+#define EVT_CODE_LD_FIX_PAUSE              32
+#define EVT_CODE_LD_FIX_RESUME             33
+#define EVT_CODE_LD_FIX_ABORT              34
+#define EVT_CODE_LD_FIX_COMPLETE           35
+#define EVT_CODE_LD_FIX_PROGRESS           36
+#define EVT_CODE_LD_FIX_ERROR              37
+#define EVT_CODE_LD_FIX_AUTO_PAUSED        38
+#define EVT_CODE_LD_FIX_AUTO_RESUME        39
+
+#define EVT_CODE_LD_INIT_QUICK_START       40
+#define EVT_CODE_LD_INIT_QUICK_RESTART     41
+#define EVT_CODE_LD_INIT_QUICK_PAUSE       42
+#define EVT_CODE_LD_INIT_QUICK_RESUME      43
+#define EVT_CODE_LD_INIT_QUICK_ABORT       44
+#define EVT_CODE_LD_INIT_QUICK_COMPLETE    45
+#define EVT_CODE_LD_INIT_QUICK_PROGRESS    46
+#define EVT_CODE_LD_INIT_QUICK_ERROR       47
+#define EVT_CODE_LD_INIT_QUICK_AUTO_PAUSED 48
+#define EVT_CODE_LD_INIT_QUICK_AUTO_RESUME 49
+
+#define EVT_CODE_LD_INIT_BACK_START        50
+#define EVT_CODE_LD_INIT_BACK_RESTART      51
+#define EVT_CODE_LD_INIT_BACK_PAUSE        52
+#define EVT_CODE_LD_INIT_BACK_RESUME       53
+#define EVT_CODE_LD_INIT_BACK_ABORT        54
+#define EVT_CODE_LD_INIT_BACK_COMPLETE     55
+#define EVT_CODE_LD_INIT_BACK_PROGRESS     56
+#define EVT_CODE_LD_INIT_BACK_ERROR        57
+#define EVT_CODE_LD_INIT_BACK_AUTO_PAUSED  58
+#define EVT_CODE_LD_INIT_BACK_AUTO_RESUME  59
+
+#if 0
+#define EVT_CODE_LD_INIT_FORE_START        60
+#define EVT_CODE_LD_INIT_FORE_RESTART      61
+#define EVT_CODE_LD_INIT_FORE_PAUSE        62
+#define EVT_CODE_LD_INIT_FORE_RESUME       63
+#define EVT_CODE_LD_INIT_FORE_ABORT        64
+#define EVT_CODE_LD_INIT_FORE_COMPLETE     65
+#define EVT_CODE_LD_INIT_FORE_PROGRESS     66
+#define EVT_CODE_LD_INIT_FORE_ERROR        67
+#define EVT_CODE_LD_INIT_FORE_AUTO_PAUSED  68
+#define EVT_CODE_LD_INIT_FORE_AUTO_RESUME  69
+#endif
+
+#define EVT_CODE_LD_REBUILD_START          70
+#define EVT_CODE_LD_REBUILD_RESTART        71
+#define EVT_CODE_LD_REBUILD_PAUSE          72
+#define EVT_CODE_LD_REBUILD_RESUME         73
+#define EVT_CODE_LD_REBUILD_ABORT          74
+#define EVT_CODE_LD_REBUILD_COMPLETE       75
+#define EVT_CODE_LD_REBUILD_PROGRESS       76
+#define EVT_CODE_LD_REBUILD_ERROR          77
+#define EVT_CODE_LD_REBUILD_AUTO_PAUSED    78
+#define EVT_CODE_LD_REBUILD_AUTO_RESUME	   79
+
+#define EVT_CODE_LD_MIGRATION_START        80
+#define EVT_CODE_LD_MIGRATION_RESTART      81
+#define EVT_CODE_LD_MIGRATION_PAUSE        82
+#define EVT_CODE_LD_MIGRATION_RESUME       83
+#define EVT_CODE_LD_MIGRATION_ABORT        84
+#define EVT_CODE_LD_MIGRATION_COMPLETE     85
+#define EVT_CODE_LD_MIGRATION_PROGRESS     86
+#define EVT_CODE_LD_MIGRATION_ERROR        87
+#define EVT_CODE_LD_MIGRATION_AUTO_PAUSED  88
+#define EVT_CODE_LD_MIGRATION_AUTO_RESUME  89
+
+/* event code for logging inconsistent LBA found in consistency check or synchronization fix */
+#define EVT_CODE_LD_INCONSISTENT_LBA       90
+
+/* only used in application */
+#define EVT_CODE_EVT_ERR                   0xffff
+#define EVT_CODE_SMART_FROM_OFF_TO_ON	   0  // SMART setting is changed from OFF-->ON
+#define EVT_CODE_SMART_FROM_ON_TO_OFF	   1  // SMART setting is changed from ON-->OFF
+#define EVT_CODE_ALARM_TURN_ON			   2
+#define EVT_CODE_ALARM_TURN_OFF			   3
+#define EVT_CODE_AUTO_REBUILD_ON		   4
+#define EVT_CODE_AUTO_REBUILD_OFF		   5
+#define EVT_CODE_HD_MP_RATE_CHANGE		   6
+
+/*
+ * Event code for EVT_CLASS_HD (Hard Disk)
+ */
+#define EVT_CODE_HD_OFFLINE                0
+#define EVT_CODE_HD_ONLINE                 1
+#define EVT_CODE_HD_SETDOWN                2
+#define EVT_CODE_HD_TIMEOUT                3
+#define EVT_CODE_HD_RW_ERROR               4
+#define EVT_CODE_HD_SMART                  5
+#define EVT_CODE_HD_ERROR_FIXED            6
+#define EVT_CODE_HD_PLUG_IN                7
+#define EVT_CODE_HD_PLUG_OUT               8
+#define EVT_CODE_HD_ASSIGN_SPARE           9
+#define EVT_CODE_HD_REMOVE_SPARE           10
+#define EVT_CODE_HD_SMART_THRESHOLD_OVER   11
+/*New events added in March 2007 from LSI event list.*/
+#define EVT_CODE_HD_SMART_POLLING_FAIL	   12  // SMART polling failed on %s (Error %02x)
+#define EVT_CODE_BAD_BLOCK_TBL_80_FULL	   13  // Bad block table on PD %s is 80% full
+#define EVT_CODE_BAD_BLOCK_TBL_FULL	       14  // Bad block table on PD %s is full; Unable to log block %x
+#define EVT_CODE_BAD_BLOCK_REASSIGNED	   15  // Bad block reassigned on %s at %lx to %lx
+#define EVT_CODE_HD_CACHE_MODE_CHANGE	   16
+/*New event for HD media patrol.*/
+#define EVT_CODE_HD_MP_START			   17
+#define EVT_CODE_HD_MP_RESTART			   18
+#define EVT_CODE_HD_MP_PAUSE			   19
+#define EVT_CODE_HD_MP_RESUME              20
+#define EVT_CODE_HD_MP_ABORT               21
+#define EVT_CODE_HD_MP_COMPLETE            22
+#define EVT_CODE_HD_MP_PROGRESS            23
+#define EVT_CODE_HD_MP_ERROR               24
+#define EVT_CODE_HD_MP_AUTO_PAUSED         25
+#define EVT_CODE_HD_MP_AUTO_RESUME         26
+
+/*
+ * code for EVT_CLASS_MDD
+ */
+#define EVT_CODE_MDD_ERROR                 0
+
+/**********************************
+ *                Event IDs
+ **********************************/
+
+/*
+ * Event Id for EVT_CLASS_LD
+ */
+#define _CLASS_LD(x)                (EVT_CLASS_LD << 16 | (x))
+
+#define EVT_ID_LD_OFFLINE            _CLASS_LD(EVT_CODE_LD_OFFLINE)
+#define EVT_ID_LD_ONLINE             _CLASS_LD(EVT_CODE_LD_ONLINE)
+#define EVT_ID_LD_CREATE             _CLASS_LD(EVT_CODE_LD_CREATE)
+#define EVT_ID_LD_DELETE             _CLASS_LD(EVT_CODE_LD_DELETE)
+#define EVT_ID_LD_DEGRADE            _CLASS_LD(EVT_CODE_LD_DEGRADE)
+#define EVT_ID_LD_PARTIALLYOPTIMAL   _CLASS_LD(EVT_CODE_LD_PARTIALLYOPTIMAL)
+#define EVT_ID_LD_CACHE_MODE_CHANGE  _CLASS_LD(EVT_CODE_LD_CACHE_MODE_CHANGE)
+#define EVT_ID_LD_FIXED              _CLASS_LD(EVT_CODE_LD_FIXED)
+#define EVT_ID_LD_FOUND_ERROR        _CLASS_LD(EVT_CODE_LD_FOUND_ERROR)
+
+#define EVT_ID_LD_CHECK_START        _CLASS_LD(EVT_CODE_LD_CHECK_START)
+#define EVT_ID_LD_CHECK_RESTART      _CLASS_LD(EVT_CODE_LD_CHECK_RESTART)
+#define EVT_ID_LD_CHECK_PAUSE        _CLASS_LD(EVT_CODE_LD_CHECK_PAUSE)
+#define EVT_ID_LD_CHECK_RESUME       _CLASS_LD(EVT_CODE_LD_CHECK_RESUME)
+#define EVT_ID_LD_CHECK_ABORT        _CLASS_LD(EVT_CODE_LD_CHECK_ABORT)
+#define EVT_ID_LD_CHECK_COMPLETE     _CLASS_LD(EVT_CODE_LD_CHECK_COMPLETE)
+#define EVT_ID_LD_CHECK_PROGRESS     _CLASS_LD(EVT_CODE_LD_CHECK_PROGRESS)
+#define EVT_ID_LD_CHECK_ERROR        _CLASS_LD(EVT_CODE_LD_CHECK_ERROR)
+#define EVT_ID_LD_CHECK_AUTO_PAUSED  _CLASS_LD(EVT_CODE_LD_CHECK_AUTO_PAUSED)
+#define EVT_ID_LD_CHECK_AUTO_RESUME  _CLASS_LD(EVT_CODE_LD_CHECK_AUTO_RESUME)
+
+#define EVT_ID_LD_FIXED_START        _CLASS_LD(EVT_CODE_LD_FIX_START)
+#define EVT_ID_LD_FIXED_RESTART      _CLASS_LD(EVT_CODE_LD_FIX_RESTART)
+#define EVT_ID_LD_FIXED_PAUSE        _CLASS_LD(EVT_CODE_LD_FIX_PAUSE)
+#define EVT_ID_LD_FIXED_RESUME       _CLASS_LD(EVT_CODE_LD_FIX_RESUME)
+#define EVT_ID_LD_FIXED_ABORT        _CLASS_LD(EVT_CODE_LD_FIX_ABORT)
+#define EVT_ID_LD_FIXED_COMPLETE     _CLASS_LD(EVT_CODE_LD_FIX_COMPLETE)
+#define EVT_ID_LD_FIXED_PROGRESS     _CLASS_LD(EVT_CODE_LD_FIX_PROGRESS)
+#define EVT_ID_LD_FIXED_ERROR        _CLASS_LD(EVT_CODE_LD_FIX_ERROR)
+#define EVT_ID_LD_FIXED_AUTO_PAUSED  _CLASS_LD(EVT_CODE_LD_FIXED_AUTO_PAUSED)
+#define EVT_ID_LD_FIXED_AUTO_RESUME  _CLASS_LD(EVT_CODE_LD_FIXED_AUTO_RESUME)
+
+#define EVT_ID_LD_INIT_QUICK_START   _CLASS_LD(EVT_CODE_LD_INIT_QUICK_START)
+#define EVT_ID_LD_INIT_QUICK_RESTART _CLASS_LD(EVT_CODE_LD_INIT_QUICK_RESTART)
+#define EVT_ID_LD_INIT_QUICK_PAUSE   _CLASS_LD(EVT_CODE_LD_INIT_QUICK_PAUSE)
+#define EVT_ID_LD_INIT_QUICK_RESUME  _CLASS_LD(EVT_CODE_LD_INIT_QUICK_RESUME)
+#define EVT_ID_LD_INIT_QUICK_ABORT   _CLASS_LD(EVT_CODE_LD_INIT_QUICK_ABORT)
+#define EVT_ID_LD_INIT_QUICK_COMPLETE _CLASS_LD(EVT_CODE_LD_INIT_QUICK_COMPLETE)
+#define EVT_ID_LD_INIT_QUICK_PROGRESS _CLASS_LD(EVT_CODE_LD_INIT_QUICK_PROGRESS)
+#define EVT_ID_LD_INIT_QUICK_ERROR   _CLASS_LD(EVT_CODE_LD_INIT_QUICK_ERROR)
+#define EVT_ID_LD_INIT_QUICK_AUTO_PAUSED   _CLASS_LD(EVT_CODE_LD_INIT_QUICK_AUTO_PAUSED)
+#define EVT_ID_LD_INIT_QUICK_AUTO_RESUME   _CLASS_LD(EVT_CODE_LD_INIT_QUICK_AUTO_RESUME)
+
+#define EVT_ID_LD_INIT_BACK_START    _CLASS_LD(EVT_CODE_LD_INIT_BACK_START)
+#define EVT_ID_LD_INIT_BACK_RESTART  _CLASS_LD(EVT_CODE_LD_INIT_BACK_RESTART)
+#define EVT_ID_LD_INIT_BACK_PAUSE    _CLASS_LD(EVT_CODE_LD_INIT_BACK_PAUSE)
+#define EVT_ID_LD_INIT_BACK_RESUME   _CLASS_LD(EVT_CODE_LD_INIT_BACK_RESUME)
+#define EVT_ID_LD_INIT_BACK_ABORT    _CLASS_LD(EVT_CODE_LD_INIT_BACK_ABORT)
+#define EVT_ID_LD_INIT_BACK_COMPLETE _CLASS_LD(EVT_CODE_LD_INIT_BACK_COMPLETE)
+#define EVT_ID_LD_INIT_BACK_PROGRESS _CLASS_LD(EVT_CODE_LD_INIT_BACK_PROGRESS)
+#define EVT_ID_LD_INIT_BACK_ERROR    _CLASS_LD(EVT_CODE_LD_INIT_BACK_ERROR)
+#define EVT_ID_LD_INIT_BACK_AUTO_PAUSED   _CLASS_LD(EVT_CODE_LD_INIT_BACK_AUTO_PAUSED)
+#define EVT_ID_LD_INIT_BACK_AUTO_RESUME   _CLASS_LD(EVT_CODE_LD_INIT_BACK_AUTO_RESUME)
+
+#if 0
+#define EVT_ID_LD_INIT_FORE_START    _CLASS_LD(EVT_CODE_LD_INIT_FORE_START)
+#define EVT_ID_LD_INIT_FORE_RESTART  _CLASS_LD(EVT_CODE_LD_INIT_FORE_RESTART)
+#define EVT_ID_LD_INIT_FORE_PAUSE    _CLASS_LD(EVT_CODE_LD_INIT_FORE_PAUSE)
+#define EVT_ID_LD_INIT_FORE_RESUME   _CLASS_LD(EVT_CODE_LD_INIT_FORE_RESUME)
+#define EVT_ID_LD_INIT_FORE_ABORT    _CLASS_LD(EVT_CODE_LD_INIT_FORE_ABORT)
+#define EVT_ID_LD_INIT_FORE_COMPLETE _CLASS_LD(EVT_CODE_LD_INIT_FORE_COMPLETE)
+#define EVT_ID_LD_INIT_FORE_PROGRESS _CLASS_LD(EVT_CODE_LD_INIT_FORE_PROGRESS)
+#define EVT_ID_LD_INIT_FORE_ERROR    _CLASS_LD(EVT_CODE_LD_INIT_FORE_ERROR)
+#define EVT_ID_LD_INIT_FORE_AUTO_PAUSED  _CLASS_LD(EVT_CODE_LD_INIT_FORE_AUTO_PAUSED)
+#define EVT_ID_LD_INIT_FORE_AUTO_RESUME  _CLASS_LD(EVT_CODE_LD_INIT_FORE_AUTO_RESUME)
+#endif
+
+#define EVT_ID_LD_REBUILD_START      _CLASS_LD(EVT_CODE_LD_REBUILD_START)
+#define EVT_ID_LD_REBUILD_RESTART    _CLASS_LD(EVT_CODE_LD_REBUILD_RESTART)
+#define EVT_ID_LD_REBUILD_PAUSE      _CLASS_LD(EVT_CODE_LD_REBUILD_PAUSE)
+#define EVT_ID_LD_REBUILD_RESUME     _CLASS_LD(EVT_CODE_LD_REBUILD_RESUME)
+#define EVT_ID_LD_REBUILD_ABORT      _CLASS_LD(EVT_CODE_LD_REBUILD_ABORT)
+#define EVT_ID_LD_REBUILD_COMPLETE   _CLASS_LD(EVT_CODE_LD_REBUILD_COMPLETE)
+#define EVT_ID_LD_REBUILD_PROGRESS   _CLASS_LD(EVT_CODE_LD_REBUILD_PROGRESS)
+#define EVT_ID_LD_REBUILD_ERROR      _CLASS_LD(EVT_CODE_LD_REBUILD_ERROR)
+#define EVT_ID_LD_REBUILD_AUTO_PAUSED _CLASS_LD(EVT_CODE_LD_REBUILD_AUTO_PAUSED)
+#define EVT_ID_LD_REBUILD_AUTO_RESUME _CLASS_LD(EVT_CODE_LD_REBUILD_AUTO_RESUME)
+
+#define EVT_ID_LD_MIGRATION_START    _CLASS_LD(EVT_CODE_LD_MIGRATION_START)
+#define EVT_ID_LD_MIGRATION_RESTART  _CLASS_LD(EVT_CODE_LD_MIGRATION_RESTART)
+#define EVT_ID_LD_MIGRATION_PAUSE    _CLASS_LD(EVT_CODE_LD_MIGRATION_PAUSE)
+#define EVT_ID_LD_MIGRATION_RESUME   _CLASS_LD(EVT_CODE_LD_MIGRATION_RESUME)
+#define EVT_ID_LD_MIGRATION_ABORT    _CLASS_LD(EVT_CODE_LD_MIGRATION_ABORT)
+#define EVT_ID_LD_MIGRATION_COMPLETE _CLASS_LD(EVT_CODE_LD_MIGRATION_COMPLETE)
+#define EVT_ID_LD_MIGRATION_PROGRESS _CLASS_LD(EVT_CODE_LD_MIGRATION_PROGRESS)
+#define EVT_ID_LD_MIGRATION_ERROR    _CLASS_LD(EVT_CODE_LD_MIGRATION_ERROR)
+#define EVT_ID_LD_MIGRATION_AUTO_PAUSED    _CLASS_LD(EVT_CODE_LD_MIGRATION_AUTO_PAUSED)
+#define EVT_ID_LD_MIGRATION_AUTO_RESUME    _CLASS_LD(EVT_CODE_LD_MIGRATION_AUTO_RESUME)
+
+#define EVT_ID_LD_INCONSISTENT_LBA   _CLASS_LD(EVT_CODE_LD_INCONSISTENT_LBA)
+
+/*
+ * Event Id for EVT_CLASS_HD
+ */
+#define _CLASS_HD(x)                    (EVT_CLASS_HD << 16 | (x))
+
+#define EVT_ID_HD_OFFLINE               _CLASS_HD(EVT_CODE_HD_OFFLINE)
+#define EVT_ID_HD_ONLINE                _CLASS_HD(EVT_CODE_HD_ONLINE)
+#define EVT_ID_HD_SETDOWN               _CLASS_HD(EVT_CODE_HD_SETDOWN)
+#define EVT_ID_HD_TIMEOUT               _CLASS_HD(EVT_CODE_HD_TIMEOUT)
+#define EVT_ID_HD_RW_ERROR              _CLASS_HD(EVT_CODE_HD_RW_ERROR)
+#define EVT_ID_HD_SMART                 _CLASS_HD(EVT_CODE_HD_SMART)
+#define EVT_ID_HD_ERROR_FIXED           _CLASS_HD(EVT_CODE_HD_ERROR_FIXED)
+#define EVT_ID_HD_PLUG_IN               _CLASS_HD(EVT_CODE_HD_PLUG_IN)
+#define EVT_ID_HD_PLUG_OUT              _CLASS_HD(EVT_CODE_HD_PLUG_OUT)
+#define EVT_ID_HD_ASSIGN_SPARE          _CLASS_HD(EVT_CODE_HD_ASSIGN_SPARE)
+#define EVT_ID_HD_REMOVE_SPARE          _CLASS_HD(EVT_CODE_HD_REMOVE_SPARE)
+#define EVT_ID_HD_SMART_THRESHOLD_OVER  _CLASS_HD(EVT_CODE_HD_SMART_THRESHOLD_OVER)
+#define EVT_ID_HD_SMART_POLLING_FAIL    _CLASS_HD(EVT_CODE_HD_SMART_POLLING_FAIL)
+#define EVT_ID_BAD_BLOCK_TBL_80_FULL    _CLASS_HD(EVT_CODE_BAD_BLOCK_TBL_80_FULL)
+#define EVT_ID_BAD_BLOCK_TBL_FULL       _CLASS_HD(EVT_CODE_BAD_BLOCK_TBL_FULL)
+#define EVT_ID_BAD_BLOCK_REASSIGNED     _CLASS_HD(EVT_CODE_BAD_BLOCK_REASSIGNED)
+#define EVT_ID_HD_CACHE_MODE_CHANGE		_CLASS_HD(EVT_CODE_HD_CACHE_MODE_CHANGE)
+
+#define EVT_ID_HD_MP_START				_CLASS_HD(EVT_CODE_HD_MP_START)
+#define EVT_ID_HD_MP_RESTART			_CLASS_HD(EVT_CODE_HD_MP_RESTART)
+#define EVT_ID_HD_MP_PAUSE				_CLASS_HD(EVT_CODE_HD_MP_PAUSE)
+#define EVT_ID_HD_MP_RESUME				_CLASS_HD(EVT_CODE_HD_MP_RESUME)
+#define EVT_ID_HD_MP_ABORT				_CLASS_HD(EVT_CODE_HD_MP_ABORT)
+#define EVT_ID_HD_MP_COMPLETE			_CLASS_HD(EVT_CODE_HD_MP_COMPLETE)
+#define EVT_ID_HD_MP_PROGRESS			_CLASS_HD(EVT_CODE_HD_MP_PROGRESS)
+#define EVT_ID_HD_MP_ERROR				_CLASS_HD(EVT_CODE_HD_MP_ERROR)
+#define EVT_ID_HD_MP_AUTO_PAUSED		_CLASS_HD(EVT_CODE_HD_MP_AUTO_PAUSED)
+#define EVT_ID_HD_MP_AUTO_RESUME		_CLASS_HD(EVT_CODE_HD_MP_AUTO_RESUME)
+
+
+/*
+ * Id for EVT_CLASS_MDD
+ */
+
+#define _CLASS_MDD(x)                    (EVT_CLASS_MDD << 16 | (x))
+#define EVT_ID_MDD_ERROR                 _CLASS_MDD(EVT_CODE_MDD_ERROR)
+
+/*
+ * Id for EVT_CLASS_ADAPTER
+ */
+
+#define _CLASS_ADPT(x)                   (EVT_CLASS_ADAPTER << 16 | (x))
+#define EVT_ID_EVT_LOST                  _CLASS_ADPT(EVT_CODE_EVT_ERR)
+#define EVT_ID_SMART_FROM_OFF_TO_ON		 _CLASS_ADPT(EVT_CODE_SMART_FROM_OFF_TO_ON)
+#define EVT_ID_SMART_FROM_ON_TO_OFF		 _CLASS_ADPT(EVT_CODE_SMART_FROM_ON_TO_OFF)
+#define EVT_ID_ALARM_TURN_ON			 _CLASS_ADPT(EVT_CODE_ALARM_TURN_ON)
+#define EVT_ID_ALARM_TURN_OFF		     _CLASS_ADPT(EVT_CODE_ALARM_TURN_OFF)
+#define EVT_ID_AUTO_REBUILD_ON		     _CLASS_ADPT(EVT_CODE_AUTO_REBUILD_ON)
+#define EVT_ID_AUTO_REBUILD_OFF		     _CLASS_ADPT(EVT_CODE_AUTO_REBUILD_OFF)
+#define EVT_ID_HD_MP_RATE_CHANGE		 _CLASS_ADPT(EVT_CODE_HD_MP_RATE_CHANGE)
+
+#endif /*  COM_EVENT_DEFINE_H */
--- /dev/null
+++ b/drivers/scsi/thor/include/generic/com_event_define_ext.h
@@ -0,0 +1,357 @@
+#ifndef COM_EVENT_DEFINE_EXT_H
+#define COM_EVENT_DEFINE_EXT_H
+
+//===================================================================================
+//===================================================================================
+//		All these events are new ones but which are listed in LSI product.
+//	Pay attention: All suggested display messages are from from LSI event list.
+//  We may make some little change later, especially for new events in Loki.
+//===================================================================================
+//===================================================================================
+
+//=======================================
+//=======================================
+//				Event Classes
+//=======================================
+//=======================================
+
+#define	EVT_CLASS_SAS				7		// SAS, mainly for SAS topology
+#define	EVT_CLASS_ENCL				8		// Enclosure
+#define	EVT_CLASS_BAT				9       // Battery
+#define	EVT_CLASS_FLASH				10      // Flash memory
+#define EVT_CLASS_CACHE             11      // Cache related
+#define EVT_CLASS_MISC              12      // For other miscellenous events
+
+//=============================================================
+//					Event Codes
+//
+//	!!!  When adding an EVT_ID, Please put its severity level
+//  !!!  and suggested mesage string as comments.  This is the
+//  !!!  only place to document how 'Params' in 'DriverEvent'
+//  !!!  structure is to be used.
+//  !!!  Please refer to the EventMessages.doc to get details.
+//=============================================================
+
+//
+// Event code for EVT_CLASS_SAS (sas)
+//
+
+#define EVT_CODE_SAS_LOOP_DETECTED				0  //SAS Topology error: Loop detected
+#define EVT_CODE_SAS_UNADDR_DEVICE				1  //SAS Topology error: Unaddressable device
+#define EVT_CODE_SAS_MULTIPORT_SAME_ADDR		2  //SAS Topology error: Multiple ports to the same SAS address
+#define EVT_CODE_SAS_EXPANDER_ERR				3  //SAS Topology error: Expander error
+#define EVT_CODE_SAS_SMP_TIMEOUT				4  //SAS Topology error: SMP timeout
+#define EVT_CODE_SAS_OUT_OF_ROUTE_ENTRIES		5  //SAS Topology error: Out of route entries
+#define EVT_CODE_SAS_INDEX_NOT_FOUND			6  //SAS Topology error: Index not found
+#define EVT_CODE_SAS_SMP_FUNC_FAILED			7  //SAS Topology error: SMP function failed
+#define EVT_CODE_SAS_SMP_CRC_ERR				8  //SAS Topology error: SMP CRC error
+#define EVT_CODE_SAS_MULTI_SUBTRACTIVE			9  //SAS Topology error: Multiple subtractive
+#define EVT_CODE_SAS_TABEL_TO_TABLE				10 //SAS Topology error: Table to Table
+#define EVT_CODE_SAS_MULTI_PATHS				11 //SAS Topology error: Multiple paths
+#define EVT_CODE_SAS_WIDE_PORT_LOST_LINK_ON_PHY	12 //SAS wide port %d lost link on PHY %d
+#define EVT_CODE_SAS_WIDE_PORT_REST_LINK_ON_PHY	13 //SAS wide port %d restored link on PHY %d
+#define EVT_CODE_SAS_PHY_EXCEED_ERR_RATE		14 //SAS port %d, PHY %d has exceeded the allowed error rate
+#define EVT_CODE_SAS_SATA_MIX_NOT_SUPPORTED		15 //SAS/SATA mixing not supported in enclosure: PD %d disabled
+
+//
+// Event code for EVT_CLASS_ENCL (enclosure)
+//
+
+#define	EVT_CODE_ENCL_SES_DISCOVERED			0   // Enclosure(SES) discovered on %d
+#define	EVT_CODE_ENCL_SAFTE_DISCOVERED			1   // Enclosure(SAFTE) discovered on %d
+#define	EVT_CODE_ENCL_COMMUNICATION_LOST		2   // Enclosure %d communication lost
+#define	EVT_CODE_ENCL_COMMUNICATION_RESTORED	3   // Enclosure %d communication restored
+#define	EVT_CODE_ENCL_FAN_FAILED				4   // Enclosure %d fan %d failed
+#define	EVT_CODE_ENCL_FAN_INSERTED				5   // Enclosure %d fan %d inserted
+#define	EVT_CODE_ENCL_FAN_REMOVED				6   // Enclosure %d fan %d removed
+#define	EVT_CODE_ENCL_PS_FAILED					7   // Enclosure %d power supply %d failed
+#define	EVT_CODE_ENCL_PS_INSERTED				8   // Enclosure %d power supply %d inserted
+#define	EVT_CODE_ENCL_PS_REMOVED				9   // Enclosure %d power supply %d removed
+#define	EVT_CODE_ENCL_SIM_FAILED				10  // Enclosure %d SIM %d failed
+#define	EVT_CODE_ENCL_SIM_INSERTED				11  // Enclosure %d SIM %d inserted
+#define	EVT_CODE_ENCL_SIM_REMOVED				12  // Enclosure %d SIM %d removed
+#define	EVT_CODE_ENCL_TEMP_SENSOR_BELOW_WARNING	13  // Enclosure %d temperature sensor %d below warning threshold
+#define	EVT_CODE_ENCL_TEMP_SENSOR_BELOW_ERR		14  // Enclosure %d temperature sensor %d below error threshold
+#define	EVT_CODE_ENCL_TEMP_SENSOR_ABOVE_WARNING	15  // Enclosure %d temperature sensor %d above warning threshold
+#define	EVT_CODE_ENCL_TEMP_SENSOR_ABOVE_ERR		16  // Enclosure %d temperature sensor %d above error threshold
+#define EVT_CODE_ENCL_SHUTDOWN					17  // Enclosure %d shutdown
+#define EVT_CODE_ENCL_NOT_SUPPORTED				18  // Enclosure %d not supported; too many enclosures connected to port
+#define	EVT_CODE_ENCL_FW_MISMATCH				19  // Enclosure %d firmware mismatch
+#define	EVT_CODE_ENCL_SENSOR_BAD				20  // Enclosure %d sensor %d bad
+#define	EVT_CODE_ENCL_PHY_BAD					21  // Enclosure %d phy %d bad
+#define	EVT_CODE_ENCL_IS_UNSTABLE				22  // Enclosure %d is unstable
+#define	EVT_CODE_ENCL_HW_ERR					23  // Enclosure %d hardware error
+#define	EVT_CODE_ENCL_NOT_RESPONDING			24  // Enclosure %d not responding
+#define	EVT_CODE_ENCL_HOTPLUG_DETECTED			25  // Enclosure(SES) hotplug on %d was detected, but is not supported
+#define	EVT_CODE_ENCL_PS_SWITCHED_OFF			26  // Enclosure %d Power supply %d switched off
+#define	EVT_CODE_ENCL_PS_SWITCHED_ON			27  // Enclosure %d Power supply %d switched on
+#define	EVT_CODE_ENCL_PS_CABLE_REMOVED			28  // Enclosure %d Power supply %d cable removed
+#define	EVT_CODE_ENCL_PS_CABLE_INSERTED			29  // Enclosure %d Power supply %d cable inserted
+#define	EVT_CODE_ENCL_FAN_RETURN_TO_NORMAL		30  // Enclosure %d Fan %d returned to normal
+#define	EVT_CODE_ENCL_TEMP_RETURN_TO_NORMAL		31  // Enclosure %d Temperature %d returned to normal
+#define	EVT_CODE_ENCL_FW_DWLD_IN_PRGS			32  // Enclosure %d Firmware download in progress
+#define	EVT_CODE_ENCL_FW_DWLD_FAILED			33  // Enclosure %d Firmware download failed
+#define	EVT_CODE_ENCL_TEMP_SENSOR_DIFF_DETECTED	34  // Enclosure %d Temperature sensor %d differential detected
+#define	EVT_CODE_ENCL_FAN_SPEED_CHANGED			35  // Enclosure %d fan %d speed changed
+
+
+//
+// Event code for EVT_CLASS_BAT
+//
+
+#define EVT_CODE_BAT_PRESENT					0	// Battery present
+#define EVT_CODE_BAT_NOT_PRESENT				1   // Battery not present
+#define EVT_CODE_BAT_NEW_BAT_DETECTED			2   // New battery detected
+#define EVT_CODE_BAT_REPLACED					3   // Battery has been replaced
+#define EVT_CODE_BAT_TEMP_IS_HIGH				4   // Battery temperature is high (%dC)
+#define EVT_CODE_BAT_VOLTAGE_LOW				5   // Battery voltage low (%f V)
+#define EVT_CODE_BAT_STARTED_CHARGING			6   // Battery started charging
+#define EVT_CODE_BAT_DISCHARGING				7   // Battery is discharging
+#define EVT_CODE_BAT_TEMP_IS_NORMAL				8	// Battery temperature is normal
+#define EVT_CODE_BAT_NEED_REPLACE				9	// Battery needs to be replacement, SOH bad
+#define EVT_CODE_BAT_RELEARN_STARTED			10  // Battery relearn started
+#define EVT_CODE_BAT_RELEARN_IN_PGRS			11  // Battery relearn in progress
+#define EVT_CODE_BAT_RELEARN_COMPLETED			12  // Battery relearn completed
+#define EVT_CODE_BAT_RELEARN_TIMED_OUT			13  // Battery relearn timed out
+#define EVT_CODE_BAT_RELEARN_PENDING			14  // Battery relearn pending: Battery is under charge
+#define EVT_CODE_BAT_RELEARN_POSTPONED			15  // Battery relearn postponed
+#define EVT_CODE_BAT_START_IN_4_DAYS			16  // Battery relearn will start in 4 days
+#define EVT_CODE_BAT_START_IN_2_DAYS			17	// Battery relearn will start in 2 days
+#define EVT_CODE_BAT_START_IN_1_DAY				18	// Battery relearn will start in 1 days
+#define EVT_CODE_BAT_START_IN_5_HOURS			19	// Battery relearn will start in 5 hours
+#define EVT_CODE_BAT_REMOVED					20  // Battery removed
+#define EVT_CODE_BAT_CHARGE_CMPLT				21  // Battery charged complete
+#define EVT_CODE_BAT_CHARGER_PROBLEM_DETECTED	22  // Battery/charger problems detected: SOH bad
+#define EVT_CODE_BAT_CAPACITY_BELOW_THRESHOLD	23  // Current capacity (%d) of the battery is below threshold (%d)
+#define EVT_CODE_BAT_CAPACITY_ABOVE_THRESHOLD	24  // Current capacity (%d) of the battery is above threshold (%d)
+
+
+
+//
+// Event code for EVT_CLASS_FLASH
+//
+
+#define EVT_CODE_FLASH_DWLDED_IMAGE_CORRUPTED	0	// Flash downloaded image corrupt
+#define EVT_CODE_FLASH_ERASE_ERR				1   // Flash erase error
+#define EVT_CODE_FLASH_ERASE_TIMEOUT			2   // Flash timeout during erase
+#define EVT_CODE_FLASH_FLASH_ERR				3	// Flash error
+#define EVT_CODE_FLASHING_IMAGE					4	// Flashing image: %d
+#define EVT_CODE_FLASHING_NEW_IMAGE_DONE		5   // Flash of new firmware images complete
+#define EVT_CODE_FLASH_PROGRAMMING_ERR			6   // Flash programming error
+#define EVT_CODE_FLASH_PROGRAMMING_TIMEOUT		7   // Flash timeout during programming
+#define EVT_CODE_FLASH_UNKNOWN_CHIP_TYPE		8   // Flash chip type unknown
+#define EVT_CODE_FLASH_UNKNOWN_CMD_SET			9   // Flash command set unknown
+#define EVT_CODE_FLASH_VERIFY_FAILURE			10  // Flash verify failure
+#define EVT_CODE_NVRAM_CORRUPT					11	// NVRAM is corrupt; reinitializing
+#define EVT_CODE_NVRAM_MISMACTH_OCCURED			12  // NVRAM mismatch occured
+
+
+//
+// Event code for EVT_CLASS_CACHE(Cache)
+//
+
+#define EVT_CODE_CACHE_NOT_RECV_FROM_TBBU		0	// Unable to recover cache data from TBBU
+#define EVT_CODE_CACHE_RECVD_FROM_TBBU			1   // Cache data recovered from TBBU successfully
+#define EVT_CODE_CACHE_CTRLER_CACHE_DISCARDED	2   // Controller cache discarded due to memory/battery problems
+#define EVT_CODE_CACHE_FAIL_RECV_DUETO_MISMATCH	3   // Unable to recover cache data due to configuration mismatch
+#define EVT_CODE_CACHE_DIRTY_DATA_DISCARDED		4	// Dirty cache data discarded by user
+#define EVT_CODE_CACHE_FLUSH_RATE_CHANGED		5   // Flush rate changed to %d seconds.
+
+
+//
+// Event code for EVT_CLASS_MISC
+//
+
+#define EVT_CODE_MISC_CONFIG_CLEARED				0	// Configuration cleared
+#define EVT_CODE_MISC_CHANGE_BACK_ACTIVITY_RATE		1	// Background activity rate changed to %d%%
+#define EVT_CODE_MISC_FATAL_FW_ERR					2   // Fatal firmware error: %d
+#define EVT_CODE_MISC_FACTORY_DEFAULTS_RESTORED		3   // Factory defaults restored
+#define EVT_CODE_MISC_GET_HIBER_CMD					4   // Hibernation command received from host
+#define EVT_CODE_MISC_MUTLI_BIT_ECC_ERR				5	// Multi-bit ECC error: ECAR=%x ELOG=%x, (%d)
+#define EVT_CODE_MISC_SINGLE_BIT_ECC_ERR			6   // Single-bit ECC error: ECAR=%x ELOG=%x, (%d)
+#define EVT_CODE_MISC_GET_SHUTDOWN_CMD				7	// Shutdown command received from host
+#define EVT_CODE_MISC_TIME_ESTABLISHED				8	// Time established as %d; (%d seconds since power on)
+#define EVT_CODE_MISC_USER_ENTERED_DEBUGGER			9   // User entered firmware debugger
+
+#define EVT_CODE_MISC_FORMAT_COMPLETE				10	// Format complete on %d
+#define EVT_CODE_MISC_FORMAT_STARTED				11	// Format started on %d
+#define EVT_CODE_MISC_REASSIGN_WRITE_OP				12	// Reassign write operation on %d is %d
+#define EVT_CODE_MISC_UNEXPECTED_SENSE				13	// Unexpected sense: %d, CDB%d, Sense: %d
+#define EVT_CODE_MISC_REPLACED_MISSING				14	// Replaced missing as %d on array %d row %d
+#define EVT_CODE_MISC_NOT_A_CERTIFIED_DRIVE			15  // %d is not a certificated derive
+
+/* May put into other group???*/
+#define EVT_CODE_MISC_PD_MISSING_FROM_CONFIG_AT_BOOT	16	// PDs missing from configuration on boot
+#define EVT_CODE_MISC_VD_MISSING_DRIVES					17  // VDs missing drives and will go offline at boot: %d
+#define EVT_CODE_MISC_VD_MISSING_AT_BOOT				18  // VDs missing at boot: %d
+#define EVT_CODE_MISC_PREVIOUS_CONFIG_MISSING_AT_BOOT	19  // Previous configuration completely missing at boot
+#define EVT_CODE_MISC_PD_TOO_SMALL_FOR_AUTOREBUILD		20  // PD too small to be used for auto-rebuild on %d.
+
+//=======================================
+//=======================================
+//				Event IDs
+//=======================================
+//=======================================
+
+//
+// Event id for EVT_CLASS_SAS
+//
+
+#define _CLASS_SAS(x)                (EVT_CLASS_SAS << 16 | (x))
+
+#define EVT_ID_SAS_LOOP_DETECTED					_CLASS_SAS(EVT_CODESAS_LOOP_DETECTED)
+#define EVT_ID_SAS_UNADDR_DEVICE					_CLASS_SAS(EVT_CODESAS_UNADDR_DEVICE)
+#define EVT_ID_SAS_MULTIPORT_SAME_ADDR				_CLASS_SAS(EVT_CODESAS_MULTIPORT_SAME_ADDR)
+#define EVT_ID_SAS_EXPANDER_ERR						_CLASS_SAS(EVT_CODESAS_EXPANDER_ERR)
+#define EVT_ID_SAS_SMP_TIMEOUT						_CLASS_SAS(EVT_CODESAS_SMP_TIMEOUT)
+#define EVT_ID_SAS_OUT_OF_ROUTE_ENTRIES				_CLASS_SAS(EVT_CODESAS_OUT_OF_ROUTE_ENTRIES)
+#define EVT_ID_SAS_INDEX_NOT_FOUND					_CLASS_SAS(EVT_CODESAS_INDEX_NOT_FOUND)
+#define EVT_ID_SAS_SMP_FUNC_FAILED					_CLASS_SAS(EVT_CODESAS_SMP_FUNC_FAILED)
+#define EVT_ID_SAS_SMP_CRC_ERR						_CLASS_SAS(EVT_CODESAS_SMP_CRC_ERR)
+#define EVT_ID_SAS_MULTI_SUBTRACTIVE				_CLASS_SAS(EVT_CODESAS_MULTI_SUBTRACTIVE)
+#define EVT_ID_SAS_TABEL_TO_TABLE					_CLASS_SAS(EVT_CODESAS_TABEL_TO_TABLE)
+#define EVT_ID_SAS_MULTI_PATHS						_CLASS_SAS(EVT_CODESAS_MULTI_PATHS)
+#define EVT_ID_SAS_WIDE_PORT_LOST_LINK_ON_PHY		_CLASS_SAS(EVT_CODESAS_WIDE_PORT_LOST_LINK_ON_PHY)
+#define EVT_ID_SAS_WIDE_PORT_REST_LINK_ON_PHY		_CLASS_SAS(EVT_CODESAS_WIDE_PORT_REST_LINK_ON_PHY)
+#define EVT_ID_SAS_PHY_EXCEED_ERR_RATE				_CLASS_SAS(EVT_CODESAS_PHY_EXCEED_ERR_RATE)
+#define EVT_ID_SAS_SATA_MIX_NOT_SUPPORTED			_CLASS_SAS(EVT_CODESAS_SATA_MIX_NOT_SUPPORTED)
+
+//
+// Event id for EVT_CLASS_ENCL (enclosure)
+//
+
+#define _CLASS_ENCL(x)                (EVT_CLASS_ENCL << 16 | (x))
+
+#define	EVT_ID_ENCL_SES_DISCOVERED					_CLASS_ENCL(EVT_CODE_ENCL_SES_DISCOVERED)
+#define	EVT_ID_ENCL_SAFTE_DISCOVERED				_CLASS_ENCL(EVT_CODE_ENCL_SAFTE_DISCOVERED)
+#define	EVT_ID_ENCL_COMMUNICATION_LOST				_CLASS_ENCL(EVT_CODE_ENCL_COMMUNICATION_LOST)
+#define	EVT_ID_ENCL_COMMUNICATION_RESTORED		    _CLASS_ENCL(EVT_CODE_ENCL_COMMUNICATION_RESTORED)
+#define	EVT_ID_ENCL_FAN_FAILED						_CLASS_ENCL(EVT_CODE_ENCL_FAN_FAILED)
+#define	EVT_ID_ENCL_FAN_INSERTED					_CLASS_ENCL(EVT_CODE_ENCL_FAN_INSERTED)
+#define	EVT_ID_ENCL_FAN_REMOVED						_CLASS_ENCL(EVT_CODE_ENCL_FAN_REMOVED)
+#define	EVT_ID_ENCL_PS_FAILED						_CLASS_ENCL(EVT_CODE_ENCL_PS_FAILED)
+#define	EVT_ID_ENCL_PS_INSERTED						_CLASS_ENCL(EVT_CODE_ENCL_PS_INSERTED)
+#define	EVT_ID_ENCL_PS_REMOVED						_CLASS_ENCL(EVT_CODE_ENCL_PS_REMOVED)
+#define	EVT_ID_ENCL_SIM_FAILED						_CLASS_ENCL(EVT_CODE_ENCL_SIM_FAILED)
+#define	EVT_ID_ENCL_SIM_INSERTED					_CLASS_ENCL(EVT_CODE_ENCL_SIM_INSERTED)
+#define	EVT_ID_ENCL_SIM_REMOVED						_CLASS_ENCL(EVT_CODE_ENCL_SIM_REMOVED)
+#define	EVT_ID_ENCL_TEMP_SENSOR_BELOW_WARNING		_CLASS_ENCL(EVT_CODE_ENCL_TEMP_SENSOR_BELOW_WARNING)
+#define	EVT_ID_ENCL_TEMP_SENSOR_BELOW_ERR			_CLASS_ENCL(EVT_CODE_ENCL_TEMP_SENSOR_BELOW_ERR)
+#define	EVT_ID_ENCL_TEMP_SENSOR_ABOVE_WARNING		_CLASS_ENCL(EVT_CODE_ENCL_TEMP_SENSOR_ABOVE_WARNING)
+#define	EVT_ID_ENCL_TEMP_SENSOR_ABOVE_ERR			_CLASS_ENCL(EVT_CODE_ENCL_TEMP_SENSOR_ABOVE_ERR)
+#define EVT_ID_ENCL_SHUTDOWN						_CLASS_ENCL(EVT_CODE_ENCL_SHUTDOWN)
+#define EVT_ID_ENCL_NOT_SUPPORTED				    _CLASS_ENCL(EVT_CODE_ENCL_NOT_SUPPORTED)
+#define	EVT_ID_ENCL_FW_MISMATCH						_CLASS_ENCL(EVT_CODE_ENCL_FW_MISMATCH)
+#define	EVT_ID_ENCL_SENSOR_BAD						_CLASS_ENCL(EVT_CODE_ENCL_SENSOR_BAD)
+#define	EVT_ID_ENCL_PHY_BAD							_CLASS_ENCL(EVT_CODE_ENCL_PHY_BAD)
+#define	EVT_ID_ENCL_IS_UNSTABLE						_CLASS_ENCL(EVT_CODE_ENCL_IS_UNSTABLE)
+#define	EVT_ID_ENCL_HW_ERR							_CLASS_ENCL(EVT_CODE_ENCL_HW_ERR)
+#define	EVT_ID_ENCL_NOT_RESPONDING					_CLASS_ENCL(EVT_CODE_ENCL_NOT_RESPONDING)
+#define	EVT_ID_ENCL_HOTPLUG_DETECTED				_CLASS_ENCL(EVT_CODE_ENCL_HOTPLUG_DETECTED)
+#define	EVT_ID_ENCL_PS_SWITCHED_OFF					_CLASS_ENCL(EVT_CODE_ENCL_PS_SWITCHED_OFF	)
+#define	EVT_ID_ENCL_PS_SWITCHED_ON					_CLASS_ENCL(EVT_CODE_ENCL_PS_SWITCHED_ON)
+#define	EVT_ID_ENCL_PS_CABLE_REMOVED				_CLASS_ENCL(EVT_CODE_ENCL_PS_CABLE_REMOVED)
+#define	EVT_ID_ENCL_PS_CABLE_INSERTED				_CLASS_ENCL(EVT_CODE_ENCL_PS_CABLE_INSERTED)
+#define	EVT_ID_ENCL_FAN_RETURN_TO_NORMAL			_CLASS_ENCL(EVT_CODE_ENCL_FAN_RETURN_TO_NORMAL)
+#define	EVT_ID_ENCL_TEMP_RETURN_TO_NORMAL			_CLASS_ENCL(EVT_CODE_ENCL_TEMP_RETURN_TO_NORMAL)
+#define	EVT_ID_ENCL_FW_DWLD_IN_PRGS					_CLASS_ENCL(EVT_CODE_ENCL_FW_DWLD_IN_PRGS	)
+#define	EVT_ID_ENCL_FW_DWLD_FAILED					_CLASS_ENCL(EVT_CODE_ENCL_FW_DWLD_FAILED)
+#define	EVT_ID_ENCL_TEMP_SENSOR_DIFF_DETECTED		_CLASS_ENCL(EVT_CODE_ENCL_TEMP_SENSOR_DIFF_DETECTED)
+#define	EVT_ID_ENCL_FAN_SPEED_CHANGED				_CLASS_ENCL(EVT_CODE_ENCL_FAN_SPEED_CHANGED)
+
+//
+// Event id for EVT_CLASS_BAT
+//
+
+#define _CLASS_BAT(x)                (EVT_CLASS_BAT << 16 | (x))
+
+#define EVT_ID_BAT_PRESENT						_CLASS_BAT(EVT_CODE_BAT_PRESENT)
+#define EVT_ID_BAT_NOT_PRESENT					_CLASS_BAT(EVT_CODE_BAT_NOT_PRESENT)
+#define EVT_ID_BAT_NEW_BAT_DETECTED				_CLASS_BAT(EVT_CODE_BAT_NEW_BAT_DETECTED)
+#define EVT_ID_BAT_REPLACED						_CLASS_BAT(EVT_CODE_BAT_REPLACED)
+#define EVT_ID_BAT_TEMP_IS_HIGH					_CLASS_BAT(EVT_CODE_BAT_TEMP_IS_HIGH)
+#define EVT_ID_BAT_VOLTAGE_LOW					_CLASS_BAT(EVT_CODE_BAT_VOLTAGE_LOW)
+#define EVT_ID_BAT_STARTED_CHARGING				_CLASS_BAT(EVT_CODE_BAT_STARTED_CHARGING)
+#define EVT_ID_BAT_DISCHARGING					_CLASS_BAT(EVT_CODE_BAT_DISCHARGING)
+#define EVT_ID_BAT_TEMP_IS_NORMAL				_CLASS_BAT(EVT_CODE_BAT_TEMP_IS_NORMAL)
+#define EVT_ID_BAT_NEED_REPLACE					_CLASS_BAT(EVT_CODE_BAT_NEED_REPLACE)
+#define EVT_ID_BAT_RELEARN_STARTED				_CLASS_BAT(EVT_CODE_BAT_RELEARN_STARTED)
+#define EVT_ID_BAT_RELEARN_IN_PGRS				_CLASS_BAT(EVT_CODE_BAT_RELEARN_IN_PGRS)
+#define EVT_ID_BAT_RELEARN_COMPLETED			_CLASS_BAT(EVT_CODE_BAT_RELEARN_COMPLETED)
+#define EVT_ID_BAT_RELEARN_TIMED_OUT			_CLASS_BAT(EVT_CODE_BAT_RELEARN_TIMED_OUT)
+#define EVT_ID_BAT_RELEARN_PENDING				_CLASS_BAT(EVT_CODE_BAT_RELEARN_PENDING)
+#define EVT_ID_BAT_RELEARN_POSTPONED			_CLASS_BAT(EVT_CODE_BAT_RELEARN_POSTPONED)
+#define EVT_ID_BAT_START_IN_4_DAYS				_CLASS_BAT(EVT_CODE_BAT_START_IN_4_DAYS)
+#define EVT_ID_BAT_START_IN_2_DAYS				_CLASS_BAT(EVT_CODE_BAT_START_IN_2_DAYS)
+#define EVT_ID_BAT_START_IN_1_DAY				_CLASS_BAT(EVT_CODE_BAT_START_IN_1_DAY)
+#define EVT_ID_BAT_START_IN_5_HOURS				_CLASS_BAT(EVT_CODE_BAT_START_IN_5_HOURS)
+#define EVT_ID_BAT_REMOVED						_CLASS_BAT(EVT_CODE_BAT_REMOVED)
+#define EVT_ID_BAT_CHARGE_CMPLT					_CLASS_BAT(EVT_CODE_BAT_CHARGE_CMPLT)
+#define EVT_ID_BAT_CHARGER_PROBLEM_DETECTED		_CLASS_BAT(EVT_CODE_BAT_CHARGER_PROBLEM_DETECTED)
+#define EVT_ID_BAT_CAPACITY_BELOW_THRESHOLD		_CLASS_BAT(EVT_CODE_BAT_CAPACITY_BELOW_THRESHOLD)
+#define EVT_ID_BAT_CAPACITY_ABOVE_THRESHOLD		_CLASS_BAT(EVT_CODE_BAT_CAPACITY_ABOVE_THRESHOLD)
+
+//
+// Event id for EVT_CLASS_FLASH
+//
+#define _CLASS_FLASH(x)                (EVT_CLASS_FLASH << 16 | (x))
+
+#define EVT_ID_FLASH_DWLDED_IMAGE_CORRUPTED		_CLASS_FLASH(EVT_CODE_FLASH_DWLDED_IMAGE_CORRUPTED)
+#define EVT_ID_FLASH_ERASE_ERR					_CLASS_FLASH(EVT_CODE_FLASH_ERASE_ERR)
+#define EVT_ID_FLASH_ERASE_TIMEOUT				_CLASS_FLASH(EVT_CODE_FLASH_ERASE_TIMEOUT)
+#define EVT_ID_FLASH_FLASH_ERR					_CLASS_FLASH(EVT_CODE_FLASH_FLASH_ERR)
+#define EVT_ID_FLASHING_IMAGE					_CLASS_FLASH(EVT_CODE_FLASHING_IMAGE)
+#define EVT_ID_FLASHING_NEW_IMAGE_DONE			_CLASS_FLASH(EVT_CODE_FLASHING_NEW_IMAGE_DONE)
+#define EVT_ID_FLASH_PROGRAMMING_ERR			_CLASS_FLASH(EVT_CODE_FLASH_PROGRAMMING_ERR)
+#define EVT_ID_FLASH_PROGRAMMING_TIMEOUT		_CLASS_FLASH(EVT_CODE_FLASH_PROGRAMMING_TIMEOUT)
+#define EVT_ID_FLASH_UNKNOWN_CHIP_TYPE			_CLASS_FLASH(EVT_CODE_FLASH_UNKNOWN_CHIP_TYPE)
+#define EVT_ID_FLASH_UNKNOWN_CMD_SET			_CLASS_FLASH(EVT_CODE_FLASH_UNKNOWN_CMD_SET)
+#define EVT_ID_FLASH_VERIFY_FAILURE				_CLASS_FLASH(EVT_CODE_FLASH_VERIFY_FAILURE)
+#define EVT_ID_NVRAM_CORRUPT					_CLASS_FLASH(EVT_CODE_NVRAM_CORRUPT)
+#define EVT_ID_NVRAM_MISMACTH_OCCURED			_CLASS_FLASH(EVT_CODE_NVRAM_MISMACTH_OCCURED)
+
+
+// Event code for EVT_CLASS_CACHE(Cache)
+//
+
+#define _CLASS_CACHE(x)                (EVT_CLASS_CACHE << 16 | (x))
+
+#define EVT_ID_CACHE_NOT_RECV_FROM_TBBU			_CLASS_CACHE(EVT_CODE_CACHE_NOT_RECV_FROM_TBBU)
+#define EVT_ID_CACHE_RECVD_FROM_TBBU			_CLASS_CACHE(EVT_CODE_CACHE_RECVD_FROM_TBBU)
+#define EVT_ID_CACHE_CTRLER_CACHE_DISCARDED		_CLASS_CACHE(EVT_CODE_CACHE_CTRLER_CACHE_DISCARDED)
+#define EVT_ID_CACHE_FAIL_RECV_DUETO_MISMATCH	_CLASS_CACHE(EVT_CODE_CACHE_FAIL_RECV_DUETO_MISMATCH)
+#define EVT_ID_CACHE_DIRTY_DATA_DISCARDED		_CLASS_CACHE(EVT_CODE_CACHE_DIRTY_DATA_DISCARDED)
+#define EVT_ID_CACHE_FLUSH_RATE_CHANGED			_CLASS_CACHE(EVT_CODE_CACHE_FLUSH_RATE_CHANGED)
+
+
+//
+// Event code for EVT_CLASS_MISC
+//
+
+#define _CLASS_MISC(x)                (EVT_CLASS_MISC << 16 | (x))
+
+#define EVT_ID_MISC_CONFIG_CLEARED					_CLASS_MISC(EVT_CODE_MISC_CONFIG_CLEARED)
+#define EVT_ID_MISC_CHANGE_BACK_ACTIVITY_RATE		_CLASS_MISC(EVT_CODE_MISC_CHANGE_BACK_ACTIVITY_RATE)
+#define EVT_ID_MISC_FATAL_FW_ERR					_CLASS_MISC(EVT_CODE_MISC_FATAL_FW_ERR)
+#define EVT_ID_MISC_FACTORY_DEFAULTS_RESTORED		_CLASS_MISC(EVT_CODE_MISC_FACTORY_DEFAULTS_RESTORED)
+#define EVT_ID_MISC_GET_HIBER_CMD					_CLASS_MISC(EVT_CODE_MISC_GET_HIBER_CMD)
+#define EVT_ID_MISC_MUTLI_BIT_ECC_ERR				_CLASS_MISC(EVT_CODE_MISC_MUTLI_BIT_ECC_ERR)
+#define EVT_ID_MISC_SINGLE_BIT_ECC_ERR				_CLASS_MISC(EVT_CODE_MISC_SINGLE_BIT_ECC_ERR)
+#define EVT_ID_MISC_GET_SHUTDOWN_CMD				_CLASS_MISC(EVT_CODE_MISC_GET_SHUTDOWN_CMD)
+#define EVT_ID_MISC_TIME_ESTABLISHED				_CLASS_MISC(EVT_CODE_MISC_TIME_ESTABLISHED)
+#define EVT_ID_MISC_USER_ENTERED_DEBUGGER			_CLASS_MISC(EVT_CODE_MISC_USER_ENTERED_DEBUGGER)
+#define EVT_ID_MISC_FORMAT_COMPLETE					_CLASS_MISC(EVT_CODE_MISC_FORMAT_COMPLETE)
+#define EVT_ID_MISC_FORMAT_STARTED					_CLASS_MISC(EVT_CODE_MISC_FORMAT_STARTED)
+#define EVT_ID_MISC_REASSIGN_WRITE_OP				_CLASS_MISC(EVT_CODE_MISC_REASSIGN_WRITE_OP)
+#define EVT_ID_MISC_UNEXPECTED_SENSE				_CLASS_MISC(EVT_CODE_MISC_UNEXPECTED_SENSE)
+#define EVT_ID_MISC_REPLACED_MISSING				_CLASS_MISC(EVT_CODE_MISC_REPLACED_MISSING)
+#define EVT_ID_MISC_NOT_A_CERTIFIED_DRIVE			_CLASS_MISC(EVT_CODE_MISC_NOT_A_CERTIFIED_DRIVE)
+
+/* May put into other group???*/
+#define EVT_ID_MISC_PD_MISSING_FROM_CONFIG_AT_BOOT	_CLASS_MISC(EVT_CODE_MISC_PD_MISSING_FROM_CONFIG_AT_BOOT)
+#define EVT_ID_MISC_VD_MISSING_DRIVES				_CLASS_MISC(EVT_CODE_MISC_VD_MISSING_DRIVES)
+#define EVT_ID_MISC_VD_MISSING_AT_BOOT				_CLASS_MISC(EVT_CODE_MISC_VD_MISSING_AT_BOOT)
+#define EVT_ID_MISC_PREVIOUS_CONFIG_MISSING_AT_BOOT _CLASS_MISC(EVT_CODE_MISC_PREVIOUS_CONFIG_MISSING_AT_BOOT)
+#define EVT_ID_MISC_PD_TOO_SMALL_FOR_AUTOREBUILD	_CLASS_MISC(EVT_CODE_MISC_PD_TOO_SMALL_FOR_AUTOREBUILD)
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/include/generic/com_struct.h
@@ -0,0 +1,684 @@
+#ifndef __MV_COM_STRUCT_H__
+#define __MV_COM_STRUCT_H__
+
+#include "com_define.h"
+
+#define GET_ALL                                 0xFF
+#define ID_UNKNOWN                              0x7F
+
+#define MAX_NUM_ADAPTERS                        2
+
+#ifndef _OS_BIOS
+/* The following MAX number are to support ALL products. */
+#define MAX_HD_SUPPORTED_API                    128
+#define MAX_EXPANDER_SUPPORTED_API              16
+#define MAX_PM_SUPPORTED_API                    8
+#define MAX_LD_SUPPORTED_API                    32
+#define MAX_BLOCK_SUPPORTED_API                 512
+#define MAX_BLOCK_PER_HD_SUPPORTED_API          16
+#endif /* _OS_BIOS */
+
+#define MAX_BGA_RATE                            0xFA
+#define MAX_MEDIAPATROL_RATE                    0xFF
+
+#ifndef MV_GUID_SIZE
+#define MV_GUID_SIZE                            8
+#endif /* MV_GUID_SIZE */
+
+#define LD_MAX_NAME_LENGTH                      16
+
+#define CACHE_WRITEBACK_ENABLE                  0
+#define CACHE_WRITETHRU_ENABLE                  1
+#define CACHE_ADAPTIVE_ENABLE                   2
+#define CACHE_WRITE_POLICY_FILTER               (CACHE_WRITEBACK_ENABLE | \
+						 CACHE_WRITETHRU_ENABLE | \
+						 CACHE_ADAPTIVE_ENABLE)
+#define CACHE_LOOKAHEAD_ENABLE                  MV_BIT(2)
+
+//The Flags of Eroor Handling
+#define EH_READ_VERIFY_REQ_ERROR		MV_BIT(0)
+#define EH_WRITE_REQ_ERROR			MV_BIT(1)
+#define EH_WRITE_RECOVERY_ERROR			MV_BIT(2)
+#define EH_MEDIA_ERROR				MV_BIT(3)
+#define EH_TIMEOUT_ERROR			MV_BIT(4)
+
+#define CONSISTENCYCHECK_ONLY                   0
+#define CONSISTENCYCHECK_FIX                    1
+
+#define INIT_QUICK                              0    //Just initialize first part size of LD
+#define INIT_FULLFOREGROUND                     1    //Initialize full LD size
+#define INIT_FULLBACKGROUND                     2    //Initialize full LD size background
+#define INIT_NONE                               3
+
+#define INIT_QUICK_WITHOUT_EVENT				0xf	 // Used for QUICK INIT but set cdb[5]to 0xf so driver won't send event.
+
+#define BGA_CONTROL_START                       0
+#define BGA_CONTROL_RESTART                     1
+#define BGA_CONTROL_PAUSE                       2
+#define BGA_CONTROL_RESUME                      3
+#define BGA_CONTROL_ABORT                       4
+#define BGA_CONTROL_COMPLETE                    5
+#define BGA_CONTROL_IN_PROCESS                  6
+#define BGA_CONTROL_TERMINATE_IMMEDIATE         7
+#define BGA_CONTROL_AUTO_PAUSE                  8
+
+#define LD_STATUS_FUNCTIONAL                    0
+#define LD_STATUS_DEGRADE                       1
+#define LD_STATUS_DELETED                       2
+#define LD_STATUS_MISSING                       3 /* LD missing in system. */
+#define LD_STATUS_OFFLINE                       4
+#define LD_STATUS_PARTIALLYOPTIMAL              5 /* r6 w/ 2 pd, 1 hd drops */
+#define LD_STATUS_INVALID                       0xFF
+
+#define LD_BGA_NONE                             0
+#define LD_BGA_REBUILD                          MV_BIT(0)
+#define LD_BGA_CONSISTENCY_FIX                  MV_BIT(1)
+#define LD_BGA_CONSISTENCY_CHECK                MV_BIT(2)
+#define LD_BGA_INIT_QUICK                       MV_BIT(3)
+#define LD_BGA_INIT_BACK                        MV_BIT(4)
+#define LD_BGA_MIGRATION                        MV_BIT(5)
+#define LD_BGA_INIT_FORE                        MV_BIT(6)
+
+#define LD_BGA_STATE_NONE                       0
+#define LD_BGA_STATE_RUNNING                    1
+#define LD_BGA_STATE_ABORTED                    2
+#define LD_BGA_STATE_PAUSED                     3
+#define LD_BGA_STATE_AUTOPAUSED                 4
+#define LD_BGA_STATE_DDF_PENDING                MV_BIT(7)
+
+#define LD_MODE_RAID0                           0x0
+#define LD_MODE_RAID1                           0x1
+#define LD_MODE_RAID5                           0x5
+#define LD_MODE_RAID6                           0x6
+#define LD_MODE_JBOD                            0x0f
+#define LD_MODE_RAID10                          0x10
+#define LD_MODE_RAID1E                          0x11
+#define LD_MODE_RAID50                          0x50
+#define LD_MODE_RAID60                          0x60
+#define LD_MODE_UNKNOWN							0xFF
+
+#define HD_WIPE_MDD                             0
+#define HD_WIPE_FORCE                           1
+
+#define ROUNDING_SCHEME_NONE                    0     /* no rounding */
+#define ROUNDING_SCHEME_1GB                     1     /* 1 GB rounding */
+#define ROUNDING_SCHEME_10GB                    2     /* 10 GB rounding */
+
+#define DEVICE_TYPE_NONE                        0
+#define DEVICE_TYPE_HD                          1		//  DT_DIRECT_ACCESS_BLOCK
+#define DEVICE_TYPE_PM                          2
+#define DEVICE_TYPE_EXPANDER                    3		// DT_EXPANDER
+#define DEVICE_TYPE_TAPE						4		// DT_SEQ_ACCESS
+#define DEVICE_TYPE_PRINTER						5		// DT_PRINTER
+#define DEVICE_TYPE_PROCESSOR					6		// DT_PROCESSOR
+#define DEVICE_TYPE_WRITE_ONCE					7 		// DT_WRITE_ONCE
+#define DEVICE_TYPE_CD_DVD						8		// DT_CD_DVD
+#define DEVICE_TYPE_OPTICAL_MEMORY				9 		// DT_OPTICAL_MEMORY
+#define DEVICE_TYPE_MEDIA_CHANGER				10		// DT_MEDIA_CHANGER
+#define DEVICE_TYPE_ENCLOSURE					11		// DT_ENCLOSURE
+#define DEVICE_TYPE_PORT                        0xFF	// DT_STORAGE_ARRAY_CTRL
+
+#define HD_STATUS_FREE                          MV_BIT(0)
+#define HD_STATUS_ASSIGNED                      MV_BIT(1)
+#define HD_STATUS_SPARE                         MV_BIT(2)
+#define HD_STATUS_OFFLINE                       MV_BIT(3)
+#define HD_STATUS_SMARTCHECKING                 MV_BIT(4)
+#define HD_STATUS_MP                            MV_BIT(5)
+
+#define HD_BGA_STATE_NONE                       LD_BGA_STATE_NONE
+#define HD_BGA_STATE_RUNNING                    LD_BGA_STATE_RUNNING
+#define HD_BGA_STATE_ABORTED                    LD_BGA_STATE_ABORTED
+#define HD_BGA_STATE_PAUSED                     LD_BGA_STATE_PAUSED
+#define HD_BGA_STATE_AUTOPAUSED                 LD_BGA_STATE_AUTOPAUSED
+
+#define HD_BGA_TYPE_NONE						0
+#define HD_BGA_TYPE_MP							1
+#define HD_BGA_TYPE_DATASCRUB					2
+
+#define GLOBAL_SPARE_DISK                       MV_BIT(2)
+
+#define PD_DDF_VALID                            MV_BIT(0)
+#define PD_DISK_VALID                           MV_BIT(1)
+#define PD_DDF_CLEAN                            MV_BIT(2)
+#define PD_NEED_UPDATE                          MV_BIT(3)
+#define PD_MBR_VALID                            MV_BIT(4)
+#define PD_NEED_FLUSH                           MV_BIT(5)
+#define PD_CLEAR_MBR                            MV_BIT(6)
+#if ERROR_HANDLING_SUPPORT
+#define PD_RCT_NEED_UPDATE             			MV_BIT(7)
+#endif
+
+#define PD_STATE_ONLINE                         MV_BIT(0)
+#define PD_STATE_FAILED                         MV_BIT(1)
+#define PD_STATE_REBUILDING                     MV_BIT(2)
+#define PD_STATE_TRANSITION                     MV_BIT(3)
+#define PD_STATE_SMART_ERROR                    MV_BIT(4)
+#define PD_STATE_READ_ERROR                     MV_BIT(5)
+#define PD_STATE_MISSING                        MV_BIT(6)
+
+#define HD_STATUS_SETONLINE                     0
+#define HD_STATUS_SETOFFLINE                    1
+#define HD_STATUS_INVALID                       0xFF
+
+// Definition used for old driver.
+#define HD_TYPE_SATA                            MV_BIT(0)
+#define HD_TYPE_PATA                            MV_BIT(1)
+#define HD_TYPE_SAS                             MV_BIT(2)
+#define HD_TYPE_ATAPI                           MV_BIT(3)
+#define HD_TYPE_TAPE                            MV_BIT(4)
+#define HD_TYPE_SES                             MV_BIT(5)
+
+// PD's Protocol/Connection type (used by new driver)
+#define DC_ATA		MV_BIT(0)
+#define DC_SCSI		MV_BIT(1)
+#define DC_SERIAL	MV_BIT(2)
+#define DC_PARALLEL	MV_BIT(3)
+#define DC_ATAPI	MV_BIT(4)		// used by core driver to prepare FIS
+
+// PD's Device type defined in SCSI-III specification (used by new driver)
+#define DT_DIRECT_ACCESS_BLOCK	0x00
+#define DT_SEQ_ACCESS			0x01
+#define DT_PRINTER				0x02
+#define DT_PROCESSOR			0x03
+#define DT_WRITE_ONCE			0x04
+#define DT_CD_DVD				0x05
+#define DT_OPTICAL_MEMORY		0x07
+#define DT_MEDIA_CHANGER		0x08
+#define DT_STORAGE_ARRAY_CTRL	0x0C
+#define DT_ENCLOSURE			0x0D
+// The following are defined by Marvell
+#define DT_EXPANDER				0x20
+#define DT_PM					0x21
+
+
+
+#define HD_FEATURE_NCQ                          MV_BIT(0)	/* Capability */
+#define HD_FEATURE_TCQ                          MV_BIT(1)
+#define HD_FEATURE_1_5G                         MV_BIT(2)
+#define HD_FEATURE_3G                           MV_BIT(3)
+#define HD_FEATURE_WRITE_CACHE                  MV_BIT(4)
+#define HD_FEATURE_48BITS                       MV_BIT(5)
+#define HD_FEATURE_SMART                        MV_BIT(6)
+
+#define HD_SPEED_1_5G							1		// current
+#define HD_SPEED_3G								2
+
+#define EXP_SSP                                 MV_BIT(0)
+#define EXP_STP                                 MV_BIT(1)
+#define EXP_SMP                                 MV_BIT(2)
+
+#define HD_DMA_NONE                             0
+#define HD_DMA_1                                1
+#define HD_DMA_2                                2
+#define HD_DMA_3                                3
+#define HD_DMA_4                                4
+#define HD_DMA_5                                5
+#define HD_DMA_6                                6
+#define HD_DMA_7                                7
+#define HD_DMA_8                                8
+#define HD_DMA_9                                9
+
+#define HD_PIO_NONE                             0
+#define HD_PIO_1                                1
+#define HD_PIO_2                                2
+#define HD_PIO_3                                3
+#define HD_PIO_4                                4
+#define HD_PIO_5                                5
+
+#define HD_XCQ_OFF                              0
+#define HD_NCQ_ON                               1
+#define HD_TCQ_ON                               2
+
+#define SECTOR_LENGTH                           512
+#define SECTOR_WRITE                            0
+#define SECTOR_READ                             1
+
+#define DBG_LD2HD                               0
+#define DBG_HD2LD                               1
+
+#define DRIVER_LENGTH                           1024*16
+#define FLASH_DOWNLOAD                          0xf0
+#define FLASH_UPLOAD                            0xf
+#define    FLASH_TYPE_CONFIG                    0
+#define    FLASH_TYPE_BIN                       1
+#define    FLASH_TYPE_BIOS                      2
+#define    FLASH_TYPE_FIRMWARE                  3
+
+#define BLOCK_INVALID                           0
+#define BLOCK_VALID                             MV_BIT(0)
+#define BLOCK_ASSIGNED                          MV_BIT(1)
+
+#ifdef _OS_BIOS
+#define FREE_BLOCK(Flags)       (Flags&(BLOCK_VALID) == Flags)
+#define ASSIGN_BLOCK(Flags)     (Flags&(BLOCK_VALID|BLOCK_ASSIGNED) == Flags)
+#define INVALID_BLOCK(Flags)    (Flags&(BLOCK_VALID|BLOCK_ASSIGNED) == 0)
+#endif /* _OS_BIOS */
+
+/* Target device type */
+#define TARGET_TYPE_LD                    0
+#define TARGET_TYPE_FREE_PD               1
+
+/*
+#define BLOCK_STATUS_NORMAL                     0
+#define BLOCK_STATUS_REBUILDING                 MV_BIT(0)
+#define BLOCK_STATUS_CONSISTENTCHECKING         MV_BIT(1)
+#define BLOCK_STATUS_INITIALIZING               MV_BIT(2)
+#define BLOCK_STATUS_MIGRATING                  MV_BIT(3)
+#define BLOCK_STATUS_OFFLINE                    MV_BIT(4)
+*/
+#define MAX_WIDEPORT_PHYS                       8
+
+#ifndef _OS_BIOS
+#pragma pack(8)
+#endif /* _OS_BIOS */
+
+typedef struct _Link_Endpoint
+{
+	MV_U16      DevID;
+	MV_U8       DevType;         /* Refer to DEVICE_TYPE_xxx, (additional
+					type like EDGE_EXPANDER and
+					FANOUT_EXPANDER might be added). */
+	MV_U8       PhyCnt;          /* Number of PHYs for this endpoint.
+					Greater than 1 if it is wide port. */
+	MV_U8       PhyID[MAX_WIDEPORT_PHYS];    /* Assuming wide port has
+						    max of 8 PHYs. */
+	MV_U8       SAS_Address[8];  /* Filled with 0 if not SAS device. */
+	MV_U8       Reserved1[8];
+} Link_Endpoint, * PLink_Endpoint;
+
+typedef struct _Link_Entity
+{
+	Link_Endpoint    Parent;
+	MV_U8            Reserved[8];
+	Link_Endpoint    Self;
+} Link_Entity,  *PLink_Entity;
+
+typedef struct _Version_Info
+{
+	MV_U32        VerMajor;
+	MV_U32        VerMinor;
+	MV_U32        VerOEM;
+	MV_U32        VerBuild;
+}Version_Info, *PVersion_Info;
+
+#define BASE_ADDRESS_MAX_NUM                    6
+
+#define SUPPORT_LD_MODE_RAID0                   MV_BIT(0)
+#define SUPPORT_LD_MODE_RAID1                   MV_BIT(1)
+#define SUPPORT_LD_MODE_RAID10                  MV_BIT(2)
+#define SUPPORT_LD_MODE_RAID1E                  MV_BIT(3)
+#define SUPPORT_LD_MODE_RAID5                   MV_BIT(4)
+#define SUPPORT_LD_MODE_RAID6                   MV_BIT(5)
+#define SUPPORT_LD_MODE_RAID50                  MV_BIT(6)
+#define SUPPORT_LD_MODE_JBOD                    MV_BIT(7)
+
+#define FEATURE_BGA_REBUILD_SUPPORT             MV_BIT(0)
+#define FEATURE_BGA_BKINIT_SUPPORT				MV_BIT(1)
+#define FEATURE_BGA_SYNC_SUPPORT				MV_BIT(2)
+#define FEATURE_BGA_MIGRATION_SUPPORT           MV_BIT(3)
+#define FEATURE_BGA_MEDIAPATROL_SUPPORT         MV_BIT(4)
+
+typedef struct _Adapter_Info
+{
+	Version_Info    DriverVersion;
+	Version_Info    BIOSVersion;
+	MV_U64          Reserved1[2];     /* Reserve for firmware */
+
+	MV_U32          SystemIOBusNumber;
+	MV_U32          SlotNumber;
+	MV_U32          InterruptLevel;
+	MV_U32          InterruptVector;
+
+	MV_U16          VenID;
+	MV_U16          SubVenID;
+	MV_U16          DevID;
+	MV_U16          SubDevID;
+
+	MV_U8           PortCount;        /* How many ports, like 4 ports,
+					     or 4S1P. */
+	MV_U8           PortSupportType;  /* Like SATA port, SAS port,
+					     PATA port, use MV_BIT */
+	MV_U8           Features;         /* Feature bits.  See FEATURE_XXX.
+					     If corresponding bit is set,
+					     that feature is supported. */
+	MV_BOOLEAN      AlarmSupport;
+	MV_U8           RevisionID;		/* Chip revision */
+	MV_U8           Reserved2[11];
+
+	MV_U8           MaxTotalBlocks;
+	MV_U8           MaxBlockPerPD;
+	MV_U8           MaxHD;
+	MV_U8           MaxExpander;
+	MV_U8           MaxPM;
+	MV_U8           MaxLogicalDrive;
+	MV_U16          LogicalDriverMode;
+
+	MV_U8           WWN[8];            /* For future VDS use. */
+} Adapter_Info, *PAdapter_Info;
+
+typedef struct _Adapter_Config {
+	MV_BOOLEAN      AlarmOn;
+	MV_BOOLEAN      AutoRebuildOn;
+	MV_U8           BGARate;
+	MV_BOOLEAN      PollSMARTStatus;
+	MV_U8           MediaPatrolRate;
+	MV_U8           Reserved[3];
+} Adapter_Config, *PAdapter_Config;
+
+typedef struct _HD_Info
+{
+	Link_Entity     Link;             /* Including self DevID & DevType */
+	MV_U8           AdapterID;
+	MV_U8           Status;           /* Refer to HD_STATUS_XXX */
+	MV_U8           HDType;           /* HD_Type_xxx, replaced by new driver with ConnectionType & DeviceType */
+	MV_U8           PIOMode;		  /* Max PIO mode */
+	MV_U8           MDMAMode;		  /* Max MDMA mode */
+	MV_U8           UDMAMode;		  /* Max UDMA mode */
+	MV_U8           ConnectionType;	  /* DC_XXX, ConnectionType & DeviceType in new driver to replace HDType above */
+	MV_U8           DeviceType;	      /* DT_XXX */
+
+	MV_U32          FeatureSupport;   /* Support 1.5G, 3G, TCQ, NCQ, and
+					     etc, MV_BIT related */
+	MV_U8           Model[40];
+	MV_U8           SerialNo[20];
+	MV_U8           FWVersion[8];
+	MV_U64          Size;             /* unit: 1KB */
+	MV_U8           WWN[8];          /* ATA/ATAPI-8 has such definitions
+					     for the identify buffer */
+	MV_U8           CurrentPIOMode;		/* Current PIO mode */
+	MV_U8           CurrentMDMAMode;	/* Current MDMA mode */
+	MV_U8           CurrentUDMAMode;	/* Current UDMA mode */
+	MV_U8			Reserved3[5];
+//	MV_U32			FeatureEnable;
+
+	MV_U8           Reserved4[80];
+}HD_Info, *PHD_Info;
+
+typedef struct _HD_MBR_Info
+{
+	MV_U8           HDCount;
+	MV_U8           Reserved[7];
+	MV_U16          HDIDs[MAX_HD_SUPPORTED_API];
+	MV_BOOLEAN      hasMBR[MAX_HD_SUPPORTED_API];
+} HD_MBR_Info, *PHD_MBR_Info;
+
+
+typedef struct _HD_FreeSpaceInfo
+{
+	MV_U16          ID;               /* ID should be unique*/
+	MV_U8           AdapterID;
+	MV_U8           Reserved[4];
+	MV_BOOLEAN      isFixed;
+
+	MV_U64          Size;             /* unit: 1KB */
+}HD_FreeSpaceInfo, *PHD_FreeSpaceInfo;
+
+typedef struct _HD_Block_Info
+{
+	MV_U16          ID;               /* ID in the HD_Info*/
+	MV_U8           Type;             /* Refer to DEVICE_TYPE_xxx */
+	MV_U8           Reserved1[5];
+
+	/* Free is 0xff */
+	MV_U16          BlockIDs[MAX_BLOCK_PER_HD_SUPPORTED_API];
+}HD_Block_Info, *PHD_Block_Info;
+
+typedef struct _Exp_Info
+{
+	Link_Entity       Link;            /* Including self DevID & DevType */
+	MV_U8             AdapterID;
+	MV_BOOLEAN        Configuring;
+	MV_BOOLEAN        RouteTableConfigurable;
+	MV_U8             PhyCount;
+	MV_U16            ExpChangeCount;
+	MV_U16            MaxRouteIndexes;
+	MV_U8             VendorID[8+1];
+	MV_U8             ProductID[16+1];
+	MV_U8             ProductRev[4+1];
+	MV_U8             ComponentVendorID[8+1];
+	MV_U16            ComponentID;
+	MV_U8             ComponentRevisionID;
+	MV_U8             Reserved1[17];
+}Exp_Info, * PExp_Info;
+
+typedef  struct _PM_Info{
+	Link_Entity       Link;           /* Including self DevID & DevType */
+	MV_U8             AdapterID;
+	MV_U8             ProductRevision;
+	MV_U8             PMSpecRevision; /* 10 means 1.0, 11 means 1.1 */
+	MV_U8             NumberOfPorts;
+	MV_U16            VendorId;
+	MV_U16            DeviceId;
+	MV_U8             Reserved1[8];
+}PM_Info, *PPM_Info;
+
+typedef struct _HD_CONFIG
+{
+	MV_BOOLEAN        WriteCacheOn;   /* 1: enable write cache */
+	MV_BOOLEAN        SMARTOn;        /* 1: enable S.M.A.R.T */
+	MV_BOOLEAN        Online;         /* 1: to set HD online */
+	MV_U8             DriveSpeed;	  // For SATA & SAS.  HD_SPEED_1_5G, HD_SPEED_3G etc
+	MV_U8             Reserved[2];
+	MV_U16            HDID;
+}HD_Config, *PHD_Config;
+
+typedef struct  _HD_STATUS
+{
+	MV_BOOLEAN        SmartThresholdExceeded;
+	MV_U8             Reserved[1];
+	MV_U16            HDID;
+}HD_Status, *PHD_Status;
+
+typedef struct  _SPARE_STATUS
+{
+	MV_U16            HDID;
+	MV_U16            LDID;
+	MV_U8             Status;         /* HD_STATUS_SPARE */
+	MV_U8             Reserved[3];
+}Spare_Status, *PSpare_Status;
+
+typedef struct  _BSL{
+	MV_U64            LBA;            /* Bad sector LBA for the HD. */
+
+	MV_U32            Count;          /* How many serial bad sectors */
+	MV_BOOLEAN        Flag;           /* Fake bad sector or not. */
+	MV_U8             Reserved[3];
+}BSL,*PBSL;
+
+typedef struct _BLOCK_INFO
+{
+	MV_U16            ID;
+	MV_U16            HDID;           /* ID in the HD_Info */
+	MV_U16            Flags;          /* Refer to BLOCK_XXX definition */
+	MV_U16            LDID;           /* Belong to which LD */
+
+	MV_U8             Status;         /* Refer to BLOCK_STATUS_XXX*/
+	MV_U8             Reserved[3];
+	MV_U32            ReservedSpaceForMigration;	/* Space reserved for migration */
+
+	MV_U64            StartLBA;       /* unit: 512 bytes */
+	MV_U64            Size;           /* unit: 512 bytes, including ReservedSpaceForMigration */
+}Block_Info, *PBlock_Info;
+
+typedef struct _LD_Info
+{
+	MV_U16            ID;
+	MV_U8             Status;         /* Refer to LD_STATUS_xxx */
+	MV_U8             BGAStatus;      /* Refer to LD_BGA_STATE_xxx */
+	MV_U16            StripeBlockSize;/* unit: 512 bytes */
+	MV_U8             RaidMode;
+	MV_U8             HDCount;
+
+	MV_U8             CacheMode;      /* Default is CacheMode_Default,
+					     see above */
+	MV_U8             LD_GUID[MV_GUID_SIZE];
+	MV_U8             SectorCoefficient; /* (sector size) 1=>512 (default)
+						, 2=>1024, 4=>2048, 8=>4096 */
+	MV_U8             AdapterID;
+	MV_U8             Reserved[5];
+
+	MV_U64            Size;           /* LD size, unit: 512 bytes */
+
+	MV_U8             Name[LD_MAX_NAME_LENGTH];
+
+	MV_U16            BlockIDs[MAX_HD_SUPPORTED_API];        /* 32 */
+/*
+ * According to BLOCK ID, to get the related HD ID, then WMRU can
+ * draw the related graph like above.
+ */
+	MV_U8             SubLDCount;     /* for raid 10, 50,60 */
+	MV_U8             NumParityDisk;  /* For RAID 6. */
+	MV_U8             Reserved1[6];
+}LD_Info, *PLD_Info;
+
+typedef struct _Create_LD_Param
+{
+	MV_U8             RaidMode;
+	MV_U8             HDCount;
+	MV_U8             RoundingScheme; /* please refer to the definitions
+					     of  ROUNDING_SCHEME_XXX. */
+	MV_U8             SubLDCount;     /* for raid 10,50,60 */
+	MV_U16            StripeBlockSize;/*In sectors unit: 1KB */
+	MV_U8             NumParityDisk;  /* For RAID 6. */
+	MV_U8             CachePolicy;    /* please refer to the definitions
+					     of CACHEMODE_XXXX. */
+
+	MV_U8             InitializationOption;/* please refer to the
+						 definitions of INIT_XXXX. */
+	MV_U8             SectorCoefficient; /* (sector size) 1=>512
+						(default), 2=>1024, 4=>2048,
+						8=>4096 */
+	MV_U16            LDID;               /* ID of the LD to be migrated
+						 or expanded */
+	MV_U8             Reserved2[4];
+
+	MV_U16            HDIDs[MAX_HD_SUPPORTED_API];    /* 32 */
+	MV_U8             Name[LD_MAX_NAME_LENGTH];
+
+	MV_U64            Size;           /* size of LD in sectors */
+} Create_LD_Param, *PCreate_LD_Param;
+
+typedef struct _LD_STATUS
+{
+	MV_U8            Status;          /* Refer to LD_STATUS_xxx */
+	MV_U8            Bga;             /* Refer to LD_BGA_xxx */
+	MV_U16           BgaPercentage;   /* xx% */
+	MV_U8            BgaState;        /* Refer to LD_BGA_STATE_xxx */
+	MV_U8            Reserved[1];
+	MV_U16           LDID;
+} LD_Status, *PLD_Status;
+
+typedef struct    _LD_Config
+{
+	MV_U8            CacheMode;        /* See definition 4.4.1
+					      CacheMode_xxx */
+	MV_U8            Reserved1;
+	MV_BOOLEAN       AutoRebuildOn;    /* 1- AutoRebuild On */
+	MV_U8            Status;
+	MV_U16           LDID;
+	MV_U8            Reserved2[2];
+
+	MV_U8            Name[LD_MAX_NAME_LENGTH];
+}LD_Config, * PLD_Config;
+
+// Giving TargetID and LUN, returns it Type and DeviceID.  If returned Type or DeviceID is 0xFF, not found.
+typedef struct    _TargetLunType
+{
+	MV_U8            AdapterID;
+	MV_U8            TargetID;
+	MV_U8            Lun;
+	MV_U8            Type;		// TARGET_TYPE_LD or TARGET_TYPE_FREE_PD
+	MV_U16           DeviceID;	// LD ID or PD ID depends on Type
+	MV_U8            Reserved[30];
+}TargetLunType, * PTargetLunType;
+
+typedef struct _HD_MPSTATUS
+{
+	MV_U16            HDID;
+	MV_U16            LoopCount;      /* loop count */
+	MV_U16            ErrorCount;     /* error detected during media patrol */
+	MV_U16            Percentage;     /* xx% */
+	MV_U8             Status;         /* Refer to HD_BGA_STATE_xxx */
+	MV_U8             Type;
+	MV_U8             Reserved[50];
+}HD_MPStatus, *PHD_MPStatus;
+
+typedef struct _HD_BGA_STATUS
+{
+	MV_U16            HDID;
+	MV_U16            Percentage;     /* xx% */
+	MV_U8             Bga;             /* Refer to HD_BGA_TYPE_xxx */
+	MV_U8             Status;          /* Refer to HD_STATUS_xxx */
+	MV_U8             BgaStatus;         /* Refer to HD_BGA_STATE_xxx */
+	MV_U8             Reserved[1];
+}HD_BGA_Status, *PHD_BGA_Status;
+
+typedef struct _RCT_Record{
+	MV_LBA lba;
+	MV_U32 sec;	//sector count
+	MV_U8   flag;
+	MV_U8   rev[3];
+}RCT_Record, *PRCT_Record;
+
+typedef struct _DBG_DATA
+{
+	MV_U64            LBA;
+	MV_U64            Size;
+	MV_U8             Data[SECTOR_LENGTH];
+}DBG_Data, *PDBG_Data;
+
+typedef struct _DBG_HD
+{
+	MV_U64            LBA;
+	MV_U16            HDID;
+	MV_BOOLEAN        isUsed;
+	MV_U8             Reserved[5];
+}DBG_HD;
+
+typedef struct _DBG_MAP
+{
+	MV_U64            LBA;
+	MV_U16            LDID;
+	MV_BOOLEAN        isUsed;
+	MV_U8             Reserved[5];
+	DBG_HD            HDs[MAX_HD_SUPPORTED_API];
+}DBG_Map, *PDBG_Map;
+
+#ifdef CACHE_MODULE_SUPPORT
+typedef struct _LD_CACHE_STATUS
+{
+	MV_BOOLEAN       IsOnline;
+	MV_U8            CachePolicy;
+	MV_U16           StripeUnitSize;
+	MV_U32           StripeSize;
+	MV_U8            Sector_Coefficient;
+	MV_U8            RAID_Level;
+	MV_U8            Reserved[6];
+	MV_LBA	         MAX_LBA;
+#ifdef _DDR_BBU_ENABLE
+	MV_U8				LD_GUID[MV_GUID_SIZE];
+#endif
+}
+LD_CACHE_STATUS, *PLD_CACHE_STATUS;
+#endif /* CACHE_MODULE_SUPPORT */
+
+#define MAX_PASS_THRU_DATA_BUFFER_SIZE (SECTOR_LENGTH+128)
+
+typedef struct {
+	// We put Data_Buffer[] at the very beginning of this structure because SCSI commander did so.
+	MV_U8			Data_Buffer[MAX_PASS_THRU_DATA_BUFFER_SIZE];  // set by driver if read, by application if write
+	MV_U8			Reserved1[128];
+    MV_U32			Data_Length;	// set by driver if read, by application if write
+	MV_U16			DevId;		//	PD ID (used by application only)
+	MV_U8			CDB_Type;	// define a CDB type for each CDB category (used by application only)
+	MV_U8			Reserved2;
+	MV_U32			lba;
+	MV_U8			Reserved3[64];
+} PassThrough_Config, * PPassThorugh_Config;
+
+
+#ifndef _OS_BIOS
+#pragma pack()
+#endif /* _OS_BIOS */
+
+#endif /*  __MV_COM_STRUCT_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/icommon/com_api.h
@@ -0,0 +1,167 @@
+#ifndef  __MV_COM_API_H__
+#define  __MV_COM_API_H__
+
+#define MAX_CDB_SIZE                           16
+
+/* CDB definitions */
+#define APICDB0_ADAPTER                        0xF0
+#define APICDB0_LD                             0xF1
+#define APICDB0_BLOCK                          0xF2
+#define APICDB0_PD                             0xF3
+#define APICDB0_EVENT                          0xF4
+#define APICDB0_DBG                            0xF5
+#define APICDB0_FLASH                          0xF6
+
+/* for Adapter */
+#define APICDB1_ADAPTER_GETCOUNT               0
+#define APICDB1_ADAPTER_GETINFO                (APICDB1_ADAPTER_GETCOUNT + 1)
+#define APICDB1_ADAPTER_GETCONFIG              (APICDB1_ADAPTER_GETCOUNT + 2)
+#define APICDB1_ADAPTER_SETCONFIG              (APICDB1_ADAPTER_GETCOUNT + 3)
+#define APICDB1_ADAPTER_POWER_STATE_CHANGE     (APICDB1_ADAPTER_GETCOUNT + 4)
+#define APICDB1_ADAPTER_MAX                    (APICDB1_ADAPTER_GETCOUNT + 5)
+
+/* for LD */
+#define APICDB1_LD_CREATE                      0
+#define APICDB1_LD_GETMAXSIZE                  (APICDB1_LD_CREATE + 1)
+#define APICDB1_LD_GETINFO                     (APICDB1_LD_CREATE + 2)
+#define APICDB1_LD_GETTARGETLDINFO             (APICDB1_LD_CREATE + 3)
+#define APICDB1_LD_DELETE                      (APICDB1_LD_CREATE + 4)
+#define APICDB1_LD_GETSTATUS                   (APICDB1_LD_CREATE + 5)
+#define APICDB1_LD_GETCONFIG                   (APICDB1_LD_CREATE + 6)
+#define APICDB1_LD_SETCONFIG                   (APICDB1_LD_CREATE + 7)
+#define APICDB1_LD_STARTREBUILD                (APICDB1_LD_CREATE + 8)
+#define APICDB1_LD_STARTCONSISTENCYCHECK       (APICDB1_LD_CREATE + 9)
+#define APICDB1_LD_STARTINIT                   (APICDB1_LD_CREATE + 10)
+#define APICDB1_LD_STARTMIGRATION              (APICDB1_LD_CREATE + 11)
+#define APICDB1_LD_BGACONTROL                  (APICDB1_LD_CREATE + 12)
+#define APICDB1_LD_WIPEMDD                     (APICDB1_LD_CREATE + 13)
+#define APICDB1_LD_GETSPARESTATUS              (APICDB1_LD_CREATE + 14)
+#define APICDB1_LD_SETGLOBALSPARE              (APICDB1_LD_CREATE + 15)
+#define APICDB1_LD_SETLDSPARE                  (APICDB1_LD_CREATE + 16)
+#define APICDB1_LD_REMOVESPARE                 (APICDB1_LD_CREATE + 17)
+#define APICDB1_LD_HD_SETSTATUS                (APICDB1_LD_CREATE + 18)
+#define APICDB1_LD_SHUTDOWN                    (APICDB1_LD_CREATE + 19)
+#define APICDB1_LD_HD_FREE_SPACE_INFO          (APICDB1_LD_CREATE + 20)
+#define APICDB1_LD_HD_GETMBRINFO               (APICDB1_LD_CREATE + 21)
+#define APICDB1_LD_SIZEOF_MIGRATE_TARGET       (APICDB1_LD_CREATE + 22)
+#define APICDB1_LD_TARGET_LUN_TYPE			   (APICDB1_LD_CREATE + 23)
+#define APICDB1_LD_HD_MPCHECK                  (APICDB1_LD_CREATE + 24)
+#define APICDB1_LD_HD_GETMPSTATUS              (APICDB1_LD_CREATE + 25)
+#define APICDB1_LD_HD_GET_RCT_COUNT            (APICDB1_LD_CREATE + 26)
+#define APICDB1_LD_HD_RCT_REPORT               (APICDB1_LD_CREATE + 27)
+#define APICDB1_LD_HD_START_DATASCRUB          (APICDB1_LD_CREATE + 28)	// temp
+#define APICDB1_LD_HD_BGACONTROL	           (APICDB1_LD_CREATE + 29)	// temp
+#define APICDB1_LD_HD_GETBGASTATUS             (APICDB1_LD_CREATE + 30)	// temp
+// Added reserved for future expansion so that MRU/CLI can be backward compatible with drier.
+#define APICDB1_LD_RESERVED1				   (APICDB1_LD_CREATE + 31)
+#define APICDB1_LD_RESERVED2				   (APICDB1_LD_CREATE + 32)
+#define APICDB1_LD_RESERVED3				   (APICDB1_LD_CREATE + 33)
+#define APICDB1_LD_RESERVED4				   (APICDB1_LD_CREATE + 34)
+#define APICDB1_LD_RESERVED5				   (APICDB1_LD_CREATE + 35)
+
+#define APICDB1_LD_MAX                         (APICDB1_LD_CREATE + 36)
+
+/* for PD */
+#define APICDB1_PD_GETHD_INFO                  0
+#define APICDB1_PD_GETEXPANDER_INFO            (APICDB1_PD_GETHD_INFO + 1)
+#define APICDB1_PD_GETPM_INFO                  (APICDB1_PD_GETHD_INFO + 2)
+#define APICDB1_PD_GETSETTING                  (APICDB1_PD_GETHD_INFO + 3)
+#define APICDB1_PD_SETSETTING                  (APICDB1_PD_GETHD_INFO + 4)
+#define APICDB1_PD_BSL_DUMP                    (APICDB1_PD_GETHD_INFO + 5)
+#define APICDB1_PD_RESERVED1				   (APICDB1_PD_GETHD_INFO + 6)	// not used
+#define APICDB1_PD_RESERVED2				   (APICDB1_PD_GETHD_INFO + 7)	// not used
+#define APICDB1_PD_GETSTATUS                   (APICDB1_PD_GETHD_INFO + 8)
+#define APICDB1_PD_GETHD_INFO_EXT              (APICDB1_PD_GETHD_INFO + 9)	// APICDB1_PD_GETHD_INFO extension
+#define APICDB1_PD_MAX                         (APICDB1_PD_GETHD_INFO + 10)
+
+/* Sub command for APICDB1_PD_SETSETTING */
+#define APICDB4_PD_SET_WRITE_CACHE_OFF         0
+#define APICDB4_PD_SET_WRITE_CACHE_ON          1
+#define APICDB4_PD_SET_SMART_OFF               2
+#define APICDB4_PD_SET_SMART_ON                3
+#define APICDB4_PD_SMART_RETURN_STATUS         4
+#define APICDB4_PD_SET_SPEED_3G				   5
+#define APICDB4_PD_SET_SPEED_1_5G			   6
+
+/* for Block */
+#define APICDB1_BLOCK_GETINFO                  0
+#define APICDB1_BLOCK_HD_BLOCKIDS              (APICDB1_BLOCK_GETINFO + 1)
+#define APICDB1_BLOCK_MAX                      (APICDB1_BLOCK_GETINFO + 2)
+
+/* for event */
+#define APICDB1_EVENT_GETEVENT                 0
+#define APICDB1_EVENT_MAX                      (APICDB1_EVENT_GETEVENT + 1)
+
+/* for DBG */
+#define APICDB1_DBG_PDWR                       0
+#define APICDB1_DBG_MAP                        (APICDB1_DBG_PDWR + 1)
+#define APICDB1_DBG_LDWR					   (APICDB1_DBG_PDWR + 2)
+#define APICDB1_DBG_ADD_RCT_ENTRY              (APICDB1_DBG_PDWR + 3)
+#define APICDB1_DBG_REMOVE_RCT_ENTRY           (APICDB1_DBG_PDWR + 4)
+#define APICDB1_DBG_REMOVE_ALL_RCT             (APICDB1_DBG_PDWR + 5)
+#define APICDB1_DBG_MAX                        (APICDB1_DBG_PDWR + 6)
+
+/* for FLASH */
+#define APICDB1_FLASH_BIN                      0
+
+#if defined(SUPPORT_CSMI)
+/* for SDI(HP CSMI) */
+#   define APICDB0_CSMI_CORE                      0xF7
+#   define APICDB0_CSMI_RAID                      0xF8
+#   define APICDB1_CSMI_GETINFO                   0
+#   define APICDB1_CSMI_HD_BLOCKIDS               (APICDB1_BLOCK_GETINFO + 1)
+#   define APICDB1_CSMI_MAX                       (APICDB1_BLOCK_GETINFO + 2)
+
+#   define CSMI_DRIVER_NAME                       "mv64xx"
+#   define CSMI_DRIVER_DESC                       "64xx:SAS Controller"
+#endif
+
+/* for passthru commands
+	Cdb[0]: APICDB0_PASS_THRU_CMD_SCSI or APICDB0_PASS_THRU_CMD_ATA
+	Cdb[1]: APICDB1 (Data flow)
+	Cdb[2]: TargetID MSB
+	Cdb[3]: TargetID LSB
+	Cdb[4]-Cdb[15]: SCSI/ATA command is embedded here
+		SCSI command: SCSI command Cdb bytes is in the same order as the spec
+		ATA Command:
+			Features = pReq->Cdb[0];
+			Sector_Count = pReq->Cdb[1];
+			LBA_Low = pReq->Cdb[2];
+			LBA_Mid = pReq->Cdb[3];
+			LBA_High = pReq->Cdb[4];
+			Device = pReq->Cdb[5];
+			Command = pReq->Cdb[6];
+
+			if necessary:
+			Feature_Exp = pReq->Cdb[7];
+			Sector_Count_Exp = pReq->Cdb[8];
+			LBA_Low_Exp = pReq->Cdb[9];
+			LBA_Mid_Exp = pReq->Cdb[10];
+			LBA_High_Exp = pReq->Cdb[11];
+*/
+#define APICDB0_PASS_THRU_CMD_SCSI			      0xFA
+#define APICDB0_PASS_THRU_CMD_ATA				  0xFB
+
+#define APICDB1_SCSI_NON_DATA					  0x00
+#define APICDB1_SCSI_PIO_IN						  0x01 // goes with Read Long
+#define APICDB1_SCSI_PIO_OUT					  0x02 // goes with Write Long
+
+#define APICDB1_ATA_NON_DATA					  0x00
+#define APICDB1_ATA_PIO_IN						  0x01
+#define APICDB1_ATA_PIO_OUT						  0x02
+
+#ifdef _OS_LINUX
+#ifdef NEW_IO_CONTORL_PATH
+		#define API_IOCTL_DEFAULT_FUN				0x1981
+#else
+		#define API_IOCTL_DEFAULT_FUN				0x00
+#endif
+#define	API_IOCTL_GET_VIRTURL_ID				(API_IOCTL_DEFAULT_FUN + 1)
+#define	API_IOCTL_GET_HBA_COUNT				(API_IOCTL_DEFAULT_FUN + 2)
+#define	API_IOCTL_LOOKUP_DEV					(API_IOCTL_DEFAULT_FUN + 3)
+#define API_IOCTL_CHECK_VIRT_DEV                        (API_IOCTL_DEFAULT_FUN + 4)
+#define API_IOCTL_MAX                       				(API_IOCTL_DEFAULT_FUN + 5)
+#endif
+
+
+#endif /*  __MV_COM_API_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/icommon/com_event_struct.h
@@ -0,0 +1,40 @@
+#ifndef COM_EVENT_DRIVER_H
+#define COM_EVENT_DRIVER_H
+
+#include "com_define.h"
+
+#define MAX_EVENTS                      20
+#define MAX_EVENT_PARAMS                4
+#define MAX_EVENTS_RETURNED             6
+
+#ifndef _OS_BIOS
+#pragma pack(8)
+#endif /*  _OS_BIOS */
+
+typedef struct _DriverEvent
+{
+	MV_U32  TimeStamp;
+	MV_U32  SequenceNo; /* (contiguous in a single adapter) */
+	MV_U32  EventID;    /* 1st 16 bits - Event class */
+                            /* last 16 bits - Event code of this particular
+			       Event class */
+	MV_U8   Severity;
+	MV_U8   AdapterID;
+	MV_U16  DeviceID;   /* Device ID relate to the event
+			       class (HD ID, LD ID etc) */
+	MV_U32  Params[MAX_EVENT_PARAMS]; /* Additional information if
+					     ABSOLUTELY necessary. */
+} DriverEvent, * PDriverEvent;
+
+typedef struct _EventRequest
+{
+	MV_U8        Count; /* [OUT] # of actual events returned */
+	MV_U8        Reserved[3];
+	DriverEvent  Events[MAX_EVENTS_RETURNED];
+} EventRequest, * PEventRequest;
+
+#ifndef _OS_BIOS
+#pragma pack()
+#endif /*  _OS_BIOS */
+
+#endif /*  COM_EVENT_DRIVER_H */
--- /dev/null
+++ b/drivers/scsi/thor/include/icommon/com_flash.h
@@ -0,0 +1,26 @@
+#ifndef __MV_COM_FLASH_H__
+#define __MV_COM_FLASH_H__
+
+#include "com_define.h"
+
+#define DRIVER_LENGTH                      1024*16
+
+#ifndef _OS_BIOS
+#pragma pack(8)
+#endif /* _OS_BIOS */
+
+typedef struct _Flash_DriverData
+{
+	MV_U16            Size;
+	MV_U8             PageNumber;
+	MV_BOOLEAN        isLastPage;
+	MV_U16            Reserved[2];
+	MV_U8             Data[DRIVER_LENGTH];
+}
+Flash_DriveData, *PFlash_DriveData;
+
+#ifndef _OS_BIOS
+#pragma pack()
+#endif /* _OS_BIOS */
+
+#endif /* __MV_COM_FLASH_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/include/icommon/com_ioctl.h
@@ -0,0 +1,59 @@
+#ifndef __MV_COM_IOCTL_H__
+#define __MV_COM_IOCTL_H__
+
+#if defined (_OS_WINDOWS)
+#include <ntddscsi.h>
+#elif defined(_OS_LINUX)
+
+#endif /* _OS_WINDOWS */
+
+/* private IOCTL commands */
+#define MV_IOCTL_CHECK_DRIVER                                \
+	    CTL_CODE(FILE_DEVICE_CONTROLLER,                 \
+                     0x900, METHOD_BUFFERED,                 \
+                     FILE_READ_ACCESS | FILE_WRITE_ACCESS)
+
+/*
+ * MV_IOCTL_LEAVING_S0 is a notification when the system is going
+ * to leaving S0. This gives the driver a chance to do some house
+ * keeping work before system really going to sleep.
+ *
+ * The MV_IOCTL_LEAVING_S0 will be translated to APICDB0_ADAPTER/
+ * APICDB1_ADAPTER_POWER_STATE_CHANGE and passed down along the
+ * module stack. A module shall handle this request if necessary.
+ *
+ * Upon this request, usually the Cache module shall flush all
+ * cached data. And the RAID module shall auto-pause all background
+ * activities.
+ */
+#define MV_IOCTL_LEAVING_S0                                \
+	    CTL_CODE(FILE_DEVICE_CONTROLLER,                 \
+                     0x901, METHOD_BUFFERED,                 \
+                     FILE_READ_ACCESS | FILE_WRITE_ACCESS)
+
+/* IOCTL signature */
+#define MV_IOCTL_DRIVER_SIGNATURE                "mv61xxsg"
+#define MV_IOCTL_DRIVER_SIGNATURE_LENGTH         8
+
+/* IOCTL command status */
+#define IOCTL_STATUS_SUCCESS                     0
+#define IOCTL_STATUS_INVALID_REQUEST             1
+#define IOCTL_STATUS_ERROR                       2
+
+#ifndef _OS_BIOS
+#pragma pack(8)
+#endif  /* _OS_BIOS */
+
+typedef struct _MV_IOCTL_BUFFER
+{
+#ifdef _OS_WINDOWS
+	SRB_IO_CONTROL Srb_Ctrl;
+#endif /* _OS_WINDOWS */
+	MV_U8          Data_Buffer[32];
+} MV_IOCTL_BUFFER, *PMV_IOCTL_BUFFER;
+
+#ifndef _OS_BIOS
+#pragma pack()
+#endif /* _OS_BIOS */
+
+#endif /* __MV_COM_IOCTL_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/lib/common/com_dbg.c
@@ -0,0 +1,33 @@
+#include "com_define.h"
+#include "com_type.h"
+#include "com_dbg.h"
+
+MV_BOOLEAN mvLogRegisterModule(MV_U8 moduleId, MV_U32 filterMask, char* name)
+{
+	return MV_TRUE;
+}
+
+void mvLogMsg(MV_U8 moduleId, MV_U32 type, char* format, ...)
+{
+	va_list args;
+	static char szMessageBuffer[1024];
+
+	va_start(args, format);
+	vsprintf(szMessageBuffer, format, args);
+	va_end(args);
+	MV_DPRINT((szMessageBuffer));
+}
+#if (defined(_OS_WINDOWS) && (_WIN32_WINNT >= 0x0600) && defined(MV_DEBUG) )
+ULONG _cdecl MV_PRINT(char* format, ...)
+{
+	va_list args;
+	static char szMessageBuffer[1024];
+	ULONG result;
+
+	va_start(args, format);
+	vsprintf(szMessageBuffer, format, args);
+	result = DbgPrintEx(DPFLTR_DEFAULT_ID, DPFLTR_ERROR_LEVEL, szMessageBuffer);
+	va_end(args);
+	return result;
+}
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/lib/common/com_nvram.c
@@ -0,0 +1,164 @@
+/* ;This file is discarded after POST. */
+/*******************************************************************************
+*
+*                   Copyright 2006,MARVELL SEMICONDUCTOR ISRAEL, LTD.
+* THIS CODE CONTAINS CONFIDENTIAL INFORMATION OF MARVELL.
+* NO RIGHTS ARE GRANTED HEREIN UNDER ANY PATENT, MASK WORK RIGHT OR COPYRIGHT
+* OF MARVELL OR ANY THIRD PARTY. MARVELL RESERVES THE RIGHT AT ITS SOLE
+* DISCRETION TO REQUEST THAT THIS CODE BE IMMEDIATELY RETURNED TO MARVELL.
+* THIS CODE IS PROVIDED "AS IS". MARVELL MAKES NO WARRANTIES, EXPRESSED,
+* IMPLIED OR OTHERWISE, REGARDING ITS ACCURACY, COMPLETENESS OR PERFORMANCE.
+*
+* MARVELL COMPRISES MARVELL TECHNOLOGY GROUP LTD. (MTGL) AND ITS SUBSIDIARIES,
+* MARVELL INTERNATIONAL LTD. (MIL), MARVELL TECHNOLOGY, INC. (MTI), MARVELL
+* SEMICONDUCTOR, INC. (MSI), MARVELL ASIA PTE LTD. (MAPL), MARVELL JAPAN K.K.
+* (MJKK), MARVELL SEMICONDUCTOR ISRAEL. (MSIL),  MARVELL TAIWAN, LTD. AND
+* SYSKONNECT GMBH.
+*
+********************************************************************************
+* com_nvram.c - File for implementation of the Driver Intermediate Application Layer
+*
+* DESCRIPTION:
+*       None.
+*
+* DEPENDENCIES:
+*   mv_include.h
+*
+* FILE REVISION NUMBER:
+*       $Revision: 1.1.1.1 $
+*******************************************************************************/
+#ifdef ODIN_DRIVER
+#include "mv_include.h"
+#include "core_header.h"
+#include "core_helper.h"
+#include "core_spi.h"
+#include "com_nvram.h"
+
+MV_BOOLEAN mvui_init_param( MV_PVOID This, pHBA_Info_Page pHBA_Info_Param)
+{
+//	MV_U32 					nsize = FLASH_PARAM_SIZE;
+	MV_U32 					param_flash_addr=PARAM_OFFSET,i = 0;
+//	MV_U16 					my_ds=0;
+	PCore_Driver_Extension	pCore;
+	AdapterInfo				AI;
+
+	if (!This)
+		return MV_FALSE;
+
+	pCore = (PCore_Driver_Extension)This;
+	AI.bar[2] = pCore->Base_Address[2];
+
+	if (-1 == OdinSPI_Init(&AI))
+		return MV_FALSE;
+
+	/* step 1 read param from flash offset = 0x3FFF00 */
+	OdinSPI_ReadBuf( &AI, param_flash_addr, (MV_PU8)pHBA_Info_Param, FLASH_PARAM_SIZE);
+
+	/* step 2 check the signature first */
+	if(pHBA_Info_Param->Signature[0] == 'M'&& \
+	    pHBA_Info_Param->Signature[1] == 'R'&& \
+	    pHBA_Info_Param->Signature[2] == 'V'&& \
+	    pHBA_Info_Param->Signature[3] == 'L' && \
+	    (!mvVerifyChecksum((MV_PU8)pHBA_Info_Param,FLASH_PARAM_SIZE)))
+	{
+		if(pHBA_Info_Param->HBA_Flag == 0xFFFFFFFFL)
+		{
+			pHBA_Info_Param->HBA_Flag = 0;
+			pHBA_Info_Param->HBA_Flag |= HBA_FLAG_INT13_ENABLE;
+			pHBA_Info_Param->HBA_Flag &= ~HBA_FLAG_SILENT_MODE_ENABLE;
+		}
+
+		for(i=0;i<8;i++)
+		{
+			if(pHBA_Info_Param->PHY_Rate[i]>0x1)
+				/* phy host link rate */
+				pHBA_Info_Param->PHY_Rate[i] = 0x1;
+
+			// validate phy tuning
+			//pHBA_Info_Param->PHY_Tuning[i].Reserved[0] = 0;
+			//pHBA_Info_Param->PHY_Tuning[i].Reserved[1] = 0;
+		}
+	}
+	else
+	{
+		MV_FillMemory((MV_PVOID)pHBA_Info_Param, FLASH_PARAM_SIZE, 0xFF);
+		pHBA_Info_Param->Signature[0] = 'M';
+		pHBA_Info_Param->Signature[1] = 'R';
+		pHBA_Info_Param->Signature[2] = 'V';
+	    pHBA_Info_Param->Signature[3] = 'L';
+
+		// Set BIOS Version
+		pHBA_Info_Param->Minor = NVRAM_DATA_MAJOR_VERSION;
+		pHBA_Info_Param->Major = NVRAM_DATA_MINOR_VERSION;
+
+		// Set SAS address
+		for(i=0;i<MAX_PHYSICAL_PORT_NUMBER;i++)
+		{
+			pHBA_Info_Param->SAS_Address[i].b[0]=  0x50;
+			pHBA_Info_Param->SAS_Address[i].b[1]=  0x05;
+			pHBA_Info_Param->SAS_Address[i].b[2]=  0x04;
+			pHBA_Info_Param->SAS_Address[i].b[3]=  0x30;
+			pHBA_Info_Param->SAS_Address[i].b[4]=  0x11;
+			pHBA_Info_Param->SAS_Address[i].b[5]=  0xab;
+			pHBA_Info_Param->SAS_Address[i].b[6]=  0x00;
+			pHBA_Info_Param->SAS_Address[i].b[7]=  0x00;
+			/*+(MV_U8)i; - All ports' WWN has to be same */
+		}
+
+		/* init phy link rate */
+		for(i=0;i<8;i++)
+		{
+			/* phy host link rate */
+			pHBA_Info_Param->PHY_Rate[i] = 0x1;//Default is 3.0G;
+		}
+
+		MV_PRINT("pHBA_Info_Param->HBA_Flag = 0x%x \n",pHBA_Info_Param->HBA_Flag);
+
+		/* init setting flags */
+		pHBA_Info_Param->HBA_Flag = 0;
+		pHBA_Info_Param->HBA_Flag |= HBA_FLAG_INT13_ENABLE;
+		pHBA_Info_Param->HBA_Flag &= ~HBA_FLAG_SILENT_MODE_ENABLE;
+		/* write to flash and save it now */
+		if(OdinSPI_SectErase( &AI, param_flash_addr) != -1)
+			MV_PRINT("FLASH ERASE SUCCESS\n");
+		else
+			MV_PRINT("FLASH ERASE FAILED\n");
+
+		pHBA_Info_Param->Check_Sum = 0;
+		pHBA_Info_Param->Check_Sum=mvCaculateChecksum((MV_PU8)pHBA_Info_Param,sizeof(HBA_Info_Page));
+		/* init the parameter in ram */
+		OdinSPI_WriteBuf( &AI, param_flash_addr, (MV_PU8)pHBA_Info_Param, FLASH_PARAM_SIZE);
+	}
+	return MV_TRUE;
+}
+
+MV_U8	mvCaculateChecksum(MV_PU8	Address, MV_U32 Size)
+{
+		MV_U8 checkSum;
+		MV_U32 temp=0;
+        checkSum = 0;
+        for (temp = 0 ; temp < Size ; temp ++)
+        {
+                checkSum += Address[temp];
+        }
+
+        checkSum = (~checkSum) + 1;
+
+
+
+		return	checkSum;
+}
+
+MV_U8	mvVerifyChecksum(MV_PU8	Address, MV_U32 Size)
+{
+		MV_U8	checkSum=0;
+		MV_U32 	temp=0;
+        for (temp = 0 ; temp < Size ; temp ++)
+        {
+            checkSum += Address[temp];
+        }
+
+		return	checkSum;
+}
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/lib/common/com_scsi.c
@@ -0,0 +1,84 @@
+#include "com_type.h"
+#include "com_define.h"
+#include "com_dbg.h"
+#include "com_scsi.h"
+#include "com_util.h"
+
+/* NODRV device is used to send controller commands.
+ * Now we are using "Storage array controller device" as Microsoft recommended.
+ * Peripheral Device Type: 03h Processor ( can be 3h or 0ch )
+ * Peripheral Qualifier: 0h
+ * Response Data Format: 2h ( must be 2 )
+ * Version: 4h ( must be 4, 5 or 6 )
+ * Only need support minimum 36 bytes inquiry data.
+ * Must return EVPD 0x0, 0x83, 0x80 */
+#ifndef SUPPORT_VIRTUAL_DEVICE
+	/* Standard Inquiry Data for Virtual Device */
+	MV_U8 BASEATTR MV_INQUIRY_VIRTUALD_DATA[] = {
+				0x03,0x00,0x03,0x03,0xFA,0x00,0x00,0x30,
+				'M', 'a', 'r', 'v', 'e', 'l', 'l', ' ',
+				0x52,0x41,0x49,0x44,0x20,0x43,0x6F,0x6E,  /* "Raid Con" */
+				0x73,0x6F,0x6C,0x65,0x20,0x20,0x20,0x20,  /* "sole    " */
+				0x31,0x2E,0x30,0x30,0x20,0x20,0x20,0x20,  /* "1.00    " */
+				0x53,0x58,0x2F,0x52,0x53,0x41,0x46,0x2D,  /* "SX/RSAF-" */
+				0x54,0x45,0x31,0x2E,0x30,0x30,0x20,0x20,  /* "TE1.00  " */
+				0x0C,0x20,0x20,0x20,0x20,0x20,0x20,0x20
+			};
+
+	/* EVPD Inquiry Page for Virtual Device */
+	//#define MV_INQUIRY_VPD_PAGE0_VIRTUALD_DATA	MV_INQUIRY_VPD_PAGE0_DEVICE_DATA
+	MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE80_VIRTUALD_DATA[] = {
+		0x03, 0x80, 0x00, 0x08, 'C', 'o', 'n', 's', 'o', 'l', 'e', ' '};
+	//#define MV_INQUIRY_VPD_PAGE83_VIRTUALD_DATA	MV_INQUIRY_VPD_PAGE83_DEVICE_DATA
+#else
+	/* If VIRTUAL_DEVICE_TYPE==0x10, Device ID is SCSI\BridgeMARVELL_Virtual_Device__
+	 * If VIRTUAL_DEVICE_TYPE==0x0C, Device ID is SCSI\ArrayMARVELL_Virtual_Device__
+	 * If VIRTUAL_DEVICE_TYPE==0x03, Device ID is SCSI\ProcessorMARVELL_Virtual_Device__ */
+	#define VIRTUAL_DEVICE_TYPE	0x0C
+	/* Standard Inquiry Data for Virtual Device */
+	MV_U8 BASEATTR MV_INQUIRY_VIRTUALD_DATA[] = {
+					VIRTUAL_DEVICE_TYPE,0x00,0x02,0x02,0x20,0x00,0x00,0x00,//?Version should be 0x4 instead of 0x2.
+					'M', 'A', 'R', 'V', 'E', 'L', 'L', ' ',
+					'V', 'i', 'r', 't', 'u', 'a', 'l', ' ',
+					'D', 'e', 'v', 'i', 'c', 'e', ' ', ' ',
+					0x31,0x2E,0x30,0x30
+					};
+
+	/* EVPD Inquiry Page for Virtual Device */
+	MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE0_VIRTUALD_DATA[] = {
+		VIRTUAL_DEVICE_TYPE, 0x00, 0x00, 0x03, 0x00, 0x80, 0x83};
+
+	MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE80_VIRTUALD_DATA[] = {
+		VIRTUAL_DEVICE_TYPE, 0x80, 0x00, 0x08, 'V', ' ', 'D', 'e', 'v', 'i', 'c', 'e'};
+
+	//MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE83_VIRTUALD_DATA[] = {
+	//	VIRTUAL_DEVICE_TYPE, 0x83, 0x00, 0x0C, 0x01, 0x02, 0x00, 0x08,
+	//	0x00, 0x50, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00};
+	MV_U8 BASEATTR MV_INQUIRY_VPD_PAGE83_VIRTUALD_DATA[] = {
+		VIRTUAL_DEVICE_TYPE, 0x83, 0x00, 0x14, 0x02, 0x01, 0x00, 0x10,
+		'M',  'A',  'R',  'V',  'E',  'L',  'L',  ' ',	/* T10 Vendor Identification */
+		0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01	/* Vendor Specific Identifier */
+	};
+#endif	/* SUPPORT_VIRTUAL_DEVICE */
+
+MV_VOID MV_SetSenseData(
+	IN PMV_Sense_Data pSense,
+	IN MV_U8 SenseKey,
+	IN MV_U8 AdditionalSenseCode,
+	IN MV_U8 ASCQ
+	)
+{
+	/* The caller should make sure it's a valid sense buffer. */
+	MV_DASSERT( pSense!=NULL );
+
+	if ( pSense!=NULL ) {
+		MV_ZeroMemory(pSense, sizeof(MV_Sense_Data));
+
+		pSense->Valid = 0;
+		pSense->ErrorCode = MV_SCSI_RESPONSE_CODE;
+		pSense->SenseKey = SenseKey;
+		pSense->AdditionalSenseCode = AdditionalSenseCode;
+		pSense->AdditionalSenseCodeQualifier = ASCQ;
+		pSense->AdditionalSenseLength = sizeof(MV_Sense_Data) - 8;
+	}
+}
--- /dev/null
+++ b/drivers/scsi/thor/lib/common/com_sgd.c
@@ -0,0 +1,935 @@
+#include "com_type.h"
+#include "com_define.h"
+#ifdef USE_NEW_SGTABLE
+#include "com_sgd.h"
+#include "com_u64.h"
+#include "com_dbg.h"
+#include "com_util.h"
+#include "hba_exp.h"
+
+#if defined(_64BPLATFORM) || defined(_64_SYS_)
+#define USES_64B_POINTER
+#endif
+
+int sg_iter_walk(
+	IN sgd_t* sgd,
+	IN MV_U32 offset,
+	IN MV_U32 count,
+	IN sgd_visitor_t visitor,
+	IN MV_PVOID context
+	)
+{
+	sgd_t	sg[2];
+	int		sg_cnt = 0;
+	MV_U32	sz;
+
+	sgd_getsz(sgd,sz);
+	while( sz <= offset )
+	{
+		offset -= sz;
+		MV_ASSERT( !sgd_eot(sgd) );
+
+		sg_cnt++;
+		sgd_inc(sgd);
+		sgd_getsz(sgd,sz);
+	}
+
+	while(1)
+	{
+		if( sgd->flags & (SGD_REFTBL|SGD_REFSGD) )
+		{
+			MV_U32 copy_count = sz - offset;
+			MV_U32 offRef;
+			sgd_tbl_t* refSgdt;
+			sgd_t* refSgd;
+
+			sgd_get_reftbl(sgd,refSgdt);
+			if( sgd->flags & SGD_REFTBL )
+				refSgd = refSgdt->Entry_Ptr;
+			else
+				refSgd = (sgd_t*) refSgdt;
+
+			sgd_get_refoff(sgd,offRef);
+
+			if( copy_count > count )
+				copy_count = count;
+
+			if( !sg_iter_walk(
+				refSgd,
+				offRef + offset,
+				copy_count,
+				visitor,
+				context ) )
+				return 0;
+			count -= copy_count;
+		}
+		else if( sgd->flags & SGD_NEXT_TBL )
+		{
+			MV_ASSERT( MV_FALSE );	// TODO
+		}
+		else
+		{
+			sgd_copy( sg, sgd );
+
+			if( offset )
+			{
+				sg[0].baseAddr = U64_ADD_U32(sg[0].baseAddr,offset);
+
+				if( sgd->flags & SGD_VP )
+				{
+					((sgd_vp_t*)sg)->u.vaddr = ((MV_U8*) ((sgd_vp_t*)sg)->u.vaddr) +
+						offset;
+				}
+
+				if (sgd->flags & SGD_PCTX) {
+					((sgd_pctx_t *)sg)->rsvd += offset;
+				}
+
+				sg[0].size -= offset;
+			}
+
+			if( sg[0].size > count )
+				sg[0].size = count;
+
+			if( !visitor( sg, context ) )
+				return 0;
+
+			count -= sg[0].size;
+		}
+
+		sg_cnt++;
+
+		if( sgd_eot(sgd)
+			|| count==0 )
+		{
+			MV_ASSERT( count == 0 );
+			break;
+		}
+		offset = 0;
+		sgd_inc(sgd);
+		sgd_getsz(sgd,sz);
+	}
+
+	return sg_cnt;
+}
+
+int sgd_table_walk(
+	sgd_tbl_t*		sgdt,
+	sgd_visitor_t	visitor,
+	MV_PVOID		ctx
+	)
+{
+	return sg_iter_walk(
+		sgdt->Entry_Ptr,
+		0,
+		sgdt->Byte_Count,
+		visitor,
+		ctx );
+}
+
+void  sgd_iter_init(
+	sgd_iter_t*	iter,
+	sgd_t*		sgd,
+	MV_U32		offset,
+	MV_U32		count
+	)
+{
+	MV_U32	sz;
+
+	sgd_getsz(sgd,sz);
+	while( sz <= offset )
+	{
+		offset -= sz;
+		MV_ASSERT( !sgd_eot(sgd) );
+		sgd_inc(sgd);
+		sgd_getsz(sgd,sz);
+	}
+
+	iter->sgd = sgd;
+	iter->offset = offset;
+	iter->remainCnt = count;
+}
+
+int sgd_iter_get_next(
+	sgd_iter_t*	iter,
+	sgd_t*		sgd
+	)
+{
+	MV_U32	sz;
+
+	if( iter->remainCnt == 0 )
+		return 0;
+
+	sgd_getsz(iter->sgd,sz);
+	while( iter->offset >= sz )
+	{
+		if( sgd_eot(iter->sgd) )
+		{
+			iter->remainCnt = 0;
+			return 0;
+		}
+
+		iter->offset -= sz;
+		sgd_inc(iter->sgd);
+		sgd_getsz(iter->sgd,sz);
+	}
+again:
+	if( iter->sgd->flags & (SGD_REFTBL|SGD_REFSGD) )
+	{
+		sgd_iter_t	sub_iter;
+		sgd_t*		refSgd;
+		sgd_tbl_t*	refSgdt;
+		MV_U32		sub_cnt = sz - iter->offset;
+		MV_U32		offRef;
+
+		if( sub_cnt > iter->remainCnt )
+			sub_cnt = iter->remainCnt;
+
+		sgd_get_reftbl(iter->sgd,refSgdt);
+
+		if( iter->sgd->flags & SGD_REFTBL )
+			refSgd = refSgdt->Entry_Ptr;
+		else
+			refSgd = (sgd_t*) refSgdt;
+
+		sgd_get_refoff(iter->sgd,offRef);
+
+		sgd_iter_init(
+			&sub_iter,
+			refSgd,
+			offRef + iter->offset,
+			sub_cnt );
+
+		if( !sgd_iter_get_next( &sub_iter, sgd ) )
+		{
+			if( sgd_eot(iter->sgd) )
+			{
+				iter->remainCnt = 0;
+				return 0;
+			}
+			sgd_inc(iter->sgd);
+			iter->offset = 0;
+			goto again;
+		}
+		else if( sgd->flags & SGD_NEXT_TBL )
+		{
+			MV_ASSERT( MV_FALSE );	// TODO
+		}
+		else
+		{
+			sgd_getsz(sgd,sz);
+			if( sz > iter->remainCnt )
+				sgd_setsz(sgd,iter->remainCnt);
+
+			iter->offset += sz;
+			iter->remainCnt -= sz;
+		}
+
+		return 1;
+	}
+	else
+	{
+		sgd_copy( sgd, iter->sgd );
+
+		sgd->baseAddr = U64_ADD_U32(sgd->baseAddr,iter->offset);
+
+		if( sgd->flags & SGD_VP )
+		{
+			((sgd_vp_t*)sgd)->u.vaddr = ((MV_U8*) ((sgd_vp_t*)sgd)->u.vaddr) +
+				iter->offset;
+		}
+
+		if (sgd->flags & SGD_PCTX) {
+			((sgd_pctx_t *)sgd)->rsvd += iter->offset;
+		}
+
+		sz -= iter->offset;
+		sgd_setsz( sgd, sz );
+	}
+
+	if( sz > iter->remainCnt )
+	{
+		sgd_setsz( sgd, iter->remainCnt );
+		sz = iter->remainCnt;
+	}
+
+	iter->remainCnt -= sz;
+
+	if( sgd_eot(iter->sgd)
+		|| iter->remainCnt == 0 )
+	{
+		iter->remainCnt = 0;
+		return 1;
+	}
+
+	iter->offset = 0;
+	sgd_inc(iter->sgd);
+
+	return 1;
+}
+
+void sgd_dump(sgd_t* sg, char* prefix)
+{
+	MV_U32	sz;
+
+	sgd_getsz(sg,sz);
+
+	if( prefix )
+	{
+#ifndef _OS_LINUX
+		MV_PRINT(prefix);
+#endif
+	}
+
+	if( sg->flags & SGD_VIRTUAL )
+	{
+		MV_PVOID vaddr, xctx;
+
+		sgd_get_vaddr(sg,vaddr);
+		sgd_get_xctx(sg,xctx);
+
+		MV_PRINT( "\tV %p T %p %08x F %08x\n"
+			, vaddr
+			, xctx
+			, sz
+			, sg->flags );
+	}
+	else if( sg->flags & (SGD_REFTBL|SGD_REFSGD) )
+	{
+		MV_PVOID ref;
+		MV_U32	refOff;
+
+		sgd_get_ref(sg,ref);
+		sgd_get_refoff(sg,refOff);
+
+		MV_PRINT( "\tR %p O %08x %08x F %08x\n"
+			, ref
+			, refOff
+			, sz
+			, sg->flags );
+	}
+	else if( sg->flags & SGD_NEXT_TBL )
+	{
+		MV_PVOID nexttbl;
+
+		sgd_get_nexttbl(sg, nexttbl);
+
+		MV_PRINT( "\tN %p F %08x\n"
+			, nexttbl, sg->flags );
+
+	}
+	else if( sg->flags & SGD_VP )
+	{
+		sgd_vp_t* vp = (sgd_vp_t*) sg;
+		MV_PRINT( "\tX %08x_%08x %p F %08x\n"
+			, vp->baseAddr.parts.high
+			, vp->baseAddr.parts.low
+			, vp->u.vaddr
+			, sg->flags );
+	}
+	else if( sg->flags & SGD_VWOXCTX )
+	{
+		sgd_v_t* vp = (sgd_v_t*) sg;
+
+		MV_PRINT( "\tV %p T %p %08x F %08x\n"
+			, vp->u.vaddr
+			, (MV_PVOID)0
+			, sz
+			, sg->flags );
+	}
+	else if( sg->flags & SGD_PCTX )
+	{
+		sgd_pctx_t* p = (sgd_pctx_t*) sg;
+		MV_PRINT( "\tP %08x_%08x %08x F %08x X %p\n"
+			, p->baseAddr.parts.high, p->baseAddr.parts.low, p->size, p->flags
+			, p->u.xctx );
+	}
+	else
+	{
+		MV_PRINT( "\tP %08x_%08x %08x F %08x\n"
+		, sg->baseAddr.parts.high, sg->baseAddr.parts.low, sz, sg->flags );
+	}
+}
+
+void sgdl_dump(sgd_t* sg, char* prefix )
+{
+	while(1)
+	{
+		sgd_dump(sg,prefix);
+
+		if( sg->flags & SGD_REFTBL )
+		{
+			sgd_tbl_t* tbl;
+			sgd_get_reftbl(sg,tbl);
+			sgdl_dump( tbl->Entry_Ptr, "R " );
+		}
+		else if( sg->flags & SGD_REFSGD )
+		{
+			sgd_t* refsgd;
+			sgd_get_refsgd(sg,refsgd);
+			sgdl_dump( refsgd, "R " );
+		}
+
+		if( sgd_eot(sg) )
+			break;
+		sgd_inc(sg);
+	}
+}
+
+
+void sgdt_dump(sgd_tbl_t *SgTbl, char* prefix)
+{
+	sgd_t* sg = SgTbl->Entry_Ptr;
+
+	MV_PRINT( "%s %p %u of %u 0x%x bytes\n"
+		, prefix ? prefix : " "
+		, SgTbl
+		, SgTbl->Valid_Entry_Count
+		, SgTbl->Max_Entry_Count
+		, SgTbl->Byte_Count
+		);
+
+	if( !SgTbl->Valid_Entry_Count )
+		return;
+
+#if 0
+	sgdl_dump(sg, NULL);
+#else
+	while(1)
+	{
+
+		sgd_dump(sg,NULL);
+		if( sgd_eot(sg) )
+			break;
+		sgd_inc(sg);
+	}
+#endif
+}
+
+void sgdt_clear_eot(
+	sgd_tbl_t*	sgdt
+	)
+{
+	if( sgdt->Valid_Entry_Count )
+	{
+		sgd_t* sgd;
+		sgdt_get_lastsgd(sgdt,sgd);
+
+		sgd_clear_eot(sgd);
+	}
+}
+
+void sgdt_append(
+	sgd_tbl_t*	sgdt,
+	MV_U32		address,
+	MV_U32		addressHigh,
+	MV_U32		size
+	)
+{
+	sgd_t* pSGEntry = &sgdt->Entry_Ptr[sgdt->Valid_Entry_Count];
+
+	MV_ASSERT( sgdt->Valid_Entry_Count+1<=sgdt->Max_Entry_Count );
+
+	sgdt_clear_eot(sgdt);
+
+	sgdt->Valid_Entry_Count += 1;
+	sgdt->Byte_Count += size;
+
+	pSGEntry->flags = 0;
+	pSGEntry->baseAddr.parts.low = address;
+	pSGEntry->baseAddr.parts.high = addressHigh;
+	pSGEntry->size = size;
+
+	sgd_mark_eot(pSGEntry);
+}
+
+void sgdt_append_pctx(
+	sgd_tbl_t*	sgdt,
+	MV_U32		address,
+	MV_U32		addressHigh,
+	MV_U32		size,
+	MV_PVOID	xctx
+	)
+{
+	sgd_pctx_t* pSGEntry = (sgd_pctx_t*) &sgdt->Entry_Ptr[sgdt->Valid_Entry_Count];
+
+	MV_ASSERT( sgdt->Valid_Entry_Count+2<=sgdt->Max_Entry_Count );
+
+	sgdt_clear_eot(sgdt);
+
+	sgdt->Valid_Entry_Count += 2;
+	sgdt->Byte_Count += size;
+
+	pSGEntry->flags = SGD_PCTX | SGD_WIDE | SGD_EOT;
+	pSGEntry->baseAddr.parts.low = address;
+	pSGEntry->baseAddr.parts.high = addressHigh;
+	pSGEntry->size = size;
+	pSGEntry->u.xctx = xctx;
+	pSGEntry->flagsEx = SGD_X64;
+	pSGEntry->rsvd = 0;
+}
+
+void sgdt_append_sgd(
+	sgd_tbl_t*	sgdt,
+	sgd_t*		sgd
+	)
+{
+	sgd_t*	pSGEntry = &sgdt->Entry_Ptr[sgdt->Valid_Entry_Count];
+	MV_U8	cnt = 1;
+	MV_U32	sgdsz;
+
+	sgd_getsz(sgd,sgdsz);
+
+	if( sgd->flags & SGD_WIDE )
+		cnt++;
+
+	MV_ASSERT( sgdt->Valid_Entry_Count+cnt<=sgdt->Max_Entry_Count );
+
+	sgdt_clear_eot(sgdt);
+	sgdt->Valid_Entry_Count += cnt;
+	sgdt->Byte_Count += sgdsz;
+
+	MV_CopyMemory( pSGEntry, sgd, sizeof(sgd_t) * cnt );
+
+	sgd_mark_eot(pSGEntry);
+}
+
+static int sgdt_append_virtual_wo_xctx(
+	sgd_tbl_t* sgdt,
+	MV_PVOID virtual_address,
+	MV_U32 size
+	)
+{
+	sgd_t* sg = &sgdt->Entry_Ptr[sgdt->Valid_Entry_Count];
+	sgd_v_t* vsg = (sgd_v_t*) sg;
+
+	MV_ASSERT( sgdt->Valid_Entry_Count+1<=sgdt->Max_Entry_Count );
+
+	if( sgdt->Valid_Entry_Count + 1 > sgdt->Max_Entry_Count )
+		return -1;	// not enough space
+
+	sgdt_clear_eot(sgdt);
+
+	vsg->flags = SGD_EOT | SGD_VWOXCTX;
+	vsg->size = size;
+	vsg->u.vaddr = virtual_address;
+
+	sgdt->Valid_Entry_Count++;
+	sgdt->Byte_Count += size;
+
+	return 0;
+}
+
+int sgdt_append_virtual(
+	sgd_tbl_t* sgdt,
+	MV_PVOID virtual_address,
+	MV_PVOID translation_ctx,
+	MV_U32 size
+	)
+{
+	sgd_t* sg;
+#ifdef USES_64B_POINTER
+	sgd_v64_t* vsg;
+#else
+	sgd_v32_t* vsg;
+#endif
+
+	if( translation_ctx == 0 )
+		return sgdt_append_virtual_wo_xctx(sgdt,virtual_address,size);
+
+	sg = &sgdt->Entry_Ptr[sgdt->Valid_Entry_Count];
+
+#ifdef USES_64B_POINTER
+	vsg = (sgd_v64_t*) sg;
+
+	MV_ASSERT( sgdt->Valid_Entry_Count+2<=sgdt->Max_Entry_Count );
+
+	if( sgdt->Valid_Entry_Count + 2 > sgdt->Max_Entry_Count )
+		return -1;	// not enough space
+
+	sgdt_clear_eot(sgdt);
+
+	vsg->u1.vaddr = virtual_address;
+	vsg->u2.xctx = translation_ctx;
+	vsg->flags = SGD_WIDE | SGD_VIRTUAL | SGD_EOT;
+	vsg->flagsEx = SGD_X64;
+	sgdt->Valid_Entry_Count++;
+#else	// USES_64B_POINTER
+	vsg = (sgd_v32_t*) sg;
+
+	MV_ASSERT( sgdt->Valid_Entry_Count+1<=sgdt->Max_Entry_Count );
+
+	if( sgdt->Valid_Entry_Count + 1 > sgdt->Max_Entry_Count )
+		return -1;	// not enough space
+
+	sgdt_clear_eot(sgdt);
+
+	vsg->vaddr = virtual_address;
+	vsg->xctx = translation_ctx;
+	vsg->flags = SGD_VIRTUAL | SGD_EOT;
+#endif	// !USES_64B_POINTER
+
+	vsg->size = size;
+
+	sgdt->Valid_Entry_Count++;
+	sgdt->Byte_Count += size;
+
+	return 0;
+}
+
+int sgdt_append_vp(
+	sgd_tbl_t*	sgdt,
+	MV_PVOID	virtual_address,
+	MV_U32		size,
+	MV_U32		address,
+	MV_U32		addressHigh
+	)
+{
+	sgd_vp_t* sg = (sgd_vp_t*) &sgdt->Entry_Ptr[sgdt->Valid_Entry_Count];
+
+	MV_ASSERT( sgdt->Valid_Entry_Count+2<=sgdt->Max_Entry_Count );
+
+	if( sgdt->Valid_Entry_Count + 2 > sgdt->Max_Entry_Count )
+		return -1;	// not enough space
+
+	sgdt_clear_eot(sgdt);
+
+	sg->baseAddr.parts.low = address;
+	sg->baseAddr.parts.high = addressHigh;
+	sg->flags = SGD_VP | SGD_WIDE | SGD_EOT;
+	sg->size = size;
+
+	sg->u.vaddr = virtual_address;
+	sg->flagsEx = SGD_X64;
+
+	sgdt->Valid_Entry_Count += 2;
+	sgdt->Byte_Count += size;
+
+	return 0;
+}
+
+int sgdt_append_ref(
+	sgd_tbl_t*	sgdt,
+	MV_PVOID	ref,
+	MV_U32		offset,
+	MV_U32		size,
+	MV_BOOLEAN	refTbl
+	)
+{
+	sgd_t* sg;
+
+	if( sgdt->Valid_Entry_Count )
+	{
+		sgdt_get_lastsgd(sgdt,sg);
+
+		if( sg->flags&(SGD_REFTBL|SGD_REFSGD) )
+		{
+			MV_PVOID lastRef;
+			MV_U32 lastOffset;
+
+			sgd_get_ref(sg, lastRef);
+			sgd_get_refoff(sg, lastOffset);
+
+			if( lastRef == ref
+				&& lastOffset + sg->size == offset )
+			{
+				// contiguous items!
+				sg->size += size;
+				sgdt->Byte_Count += size;
+				return 0;
+			}
+		}
+	}
+
+	sg = &sgdt->Entry_Ptr[sgdt->Valid_Entry_Count];
+
+	{
+
+#ifdef USES_64B_POINTER
+	sgd_ref64_t* rsg = (sgd_ref64_t*) sg;
+	MV_ASSERT( sgdt->Valid_Entry_Count+2<=sgdt->Max_Entry_Count );
+	if( sgdt->Valid_Entry_Count + 2 > sgdt->Max_Entry_Count )
+		return -1;	// not enough space
+	sgdt_clear_eot(sgdt);
+	rsg->u.ref = ref;
+	sgdt->Valid_Entry_Count++;
+	rsg->flags = SGD_WIDE | SGD_EOT | (refTbl ? SGD_REFTBL : SGD_REFSGD);
+	rsg->flagsEx = SGD_X64;
+#else
+
+	sgd_ref32_t* rsg = (sgd_ref32_t*) sg;
+	MV_ASSERT( sgdt->Valid_Entry_Count+1<=sgdt->Max_Entry_Count );
+	if( sgdt->Valid_Entry_Count + 1 > sgdt->Max_Entry_Count )
+		return -1;	// not enough space
+	sgdt_clear_eot(sgdt);
+	rsg->ref = ref;
+	rsg->flags = SGD_EOT | (refTbl ? SGD_REFTBL : SGD_REFSGD);
+#endif
+
+	rsg->offset = offset;
+	rsg->size = size;
+
+	sgdt->Valid_Entry_Count++;
+	sgdt->Byte_Count += size;
+
+	}
+
+	return 0;
+}
+
+
+void
+sgdt_copy_partial(
+	sgd_tbl_t* sgdt,
+	sgd_t**	ppsgd,
+	MV_PU32	poff,
+	MV_U32	size
+	)
+{
+	MV_U32	sgdsz;
+	MV_U32	tmpSize;
+	sgd_t	sgd[2];
+
+	while( size )
+	{
+		sgd_getsz( *ppsgd, sgdsz );
+		MV_ASSERT( sgdsz > *poff );
+
+		tmpSize = MV_MIN( size, sgdsz - *poff );
+
+		if( sgdt )
+		{
+			sgd_copy( sgd, *ppsgd );
+
+			sgd_setsz( sgd, tmpSize );
+
+			if( *poff )
+			{
+				if( sgd->flags & (SGD_REFTBL|SGD_REFSGD) )
+				{
+					MV_U32 refoff;
+					sgd_get_refoff( sgd, refoff );
+					sgd_set_refoff( sgd, refoff+(*poff) );
+				}
+				else
+				{
+					sgd->baseAddr = U64_ADD_U32( sgd->baseAddr, (*poff) );
+					if( sgd->flags & SGD_VP )
+					{
+						sgd_vp_t* vp = (sgd_vp_t*) sgd;
+						vp->u.vaddr = ((MV_U8*)vp->u.vaddr) + (*poff);
+					}
+
+					if (sgd->flags & SGD_PCTX) {
+						sgd_pctx_t *pctx =
+							(sgd_pctx_t *)sgd;
+						pctx->rsvd += (*poff);
+					}
+				}
+			}
+
+			sgdt_append_sgd( sgdt, sgd );
+
+		}
+
+		if( size == sgdsz - *poff
+			|| tmpSize == sgdsz - *poff )
+		{
+			sgd_inc( *ppsgd );
+			(*poff) = 0;
+		}
+		else
+			(*poff) += tmpSize;
+
+		size -= tmpSize;
+	}
+}
+
+#ifdef SIMULATOR
+
+int SgVisitor(sgd_t* sg, MV_PVOID ctx)
+{
+	MV_U32* p = (MV_U32*)ctx;
+
+	sgd_dump( sg, NULL );
+
+	(*p)++;
+
+	return 1;
+}
+
+void sgd_test()
+{
+	sgd_tbl_t SgTbl1 = {0,};
+	sgd_tbl_t SgTbl2 = {0,};
+	sgd_tbl_t SgTbl3 = {0,};
+	sgd_t Entries1[32];
+	sgd_t Entries2[32];
+	sgd_t Entries3[32];
+
+	SgTbl1.Max_Entry_Count = sizeof(Entries1)/sizeof(Entries1[0]);
+	SgTbl1.Entry_Ptr = Entries1;
+
+	SgTbl2.Max_Entry_Count = sizeof(Entries2)/sizeof(Entries2[0]);
+	SgTbl2.Entry_Ptr = Entries2;
+
+	SgTbl3.Max_Entry_Count = sizeof(Entries3)/sizeof(Entries3[0]);
+	SgTbl3.Entry_Ptr = Entries3;
+
+	int i;
+
+	for( i = 0; i < 32; i++ )
+	{
+		sgdt_append( &SgTbl2, 0x80000000+i*0x1000, 0x90000000, 0x1000 );
+	}
+
+	sgdt_dump( &SgTbl2, " " );
+
+	sgdt_append_reftbl( &SgTbl1, &SgTbl2, 0x3800, 0x1000*10 );
+	sgdt_append_virtual( &SgTbl1, (MV_PVOID)0x40000, (MV_PVOID)0x60000, 0x1000*10 );
+	sgdt_append_vp( &SgTbl1, (MV_PVOID)0x80000, 0x1000*10, 0x4000, 0 );
+
+	sgdt_dump( &SgTbl1, " " );
+
+	MV_PRINT( "Walking through the table:\n" );
+
+	MV_U32 index = 0;
+	sgd_table_walk( &SgTbl1, SgVisitor, &index );
+
+	sgd_iter_t iter;
+	sgd_t sg[2];
+
+	sgd_iter_init( &iter, SgTbl1.Entry_Ptr, 0, SgTbl1.Byte_Count );
+
+	MV_PRINT( "Walking through the table in another way:\n" );
+	i = 0;
+	while( sgd_iter_get_next( &iter, sg ) )
+	{
+		sgd_dump( sg, NULL );
+	}
+
+	sgdt_dump( &SgTbl1, " " );
+	sgd_t* sgd = SgTbl1.Entry_Ptr;
+	MV_U32 off = 0x1000;
+	sgdt_copy_partial( &SgTbl3, &sgd, &off, 0x1000 );
+	sgdt_copy_partial( &SgTbl3, &sgd, &off, 0x9000 );
+	sgdt_copy_partial( &SgTbl3, &sgd, &off, 0x9000 );
+	sgdt_copy_partial( &SgTbl3, &sgd, &off, 0x1000 );
+	sgdt_copy_partial( &SgTbl3, &sgd, &off, 0x1000 );
+	sgdt_dump( &SgTbl3, " " );
+}
+#endif
+
+typedef struct _PRDTableWalkCtx
+{
+	MV_PVOID		pCore;
+	sgd_t*			pSg;
+	int				avail;
+	int				itemCnt;
+} PRDTableWalkCtx;
+
+static int PRDTablePrepareVisitor(sgd_t* sg, MV_PVOID _ctx)
+{
+	PRDTableWalkCtx* ctx = (PRDTableWalkCtx*) _ctx;
+
+	if( !ctx->avail )
+		return 0;
+
+	if( sg->flags & (SGD_VIRTUAL|SGD_VWOXCTX) )
+	{
+		MV_U32 totalSize, thisSize;
+		MV_PVOID vaddr;
+		MV_PVOID xctx;
+		MV_U64 paddr;
+#ifdef _OS_LINUX
+		MV_ASSERT( 0 );
+#endif /* _OS_LINUX */
+		sgd_getsz( sg, totalSize );
+
+		if( sg->flags & SGD_VIRTUAL )
+		{
+			sgd_get_vaddr( sg, vaddr );
+			sgd_get_xctx( sg, xctx );
+		}
+		else
+		{
+			vaddr = ((sgd_v_t*)sg)->u.vaddr;
+			xctx = 0;
+		}
+
+		while( 1 )
+		{
+			thisSize = totalSize;
+
+			if( !HBA_ModuleGetPhysicalAddress(
+					ctx->pCore,
+					vaddr,
+					xctx,
+					&paddr,
+					&thisSize ) )
+				return 0;
+
+			ctx->avail--;
+			ctx->itemCnt++;
+
+			ctx->pSg->flags = 0;
+#ifdef ODIN_DRIVER
+			ctx->pSg->size = MV_CPU_TO_LE32(thisSize);
+#else
+			ctx->pSg->size = MV_CPU_TO_LE32(thisSize - 1);
+#endif
+			ctx->pSg->baseAddr.parts.low = MV_CPU_TO_LE32(paddr.parts.low);
+			ctx->pSg->baseAddr.parts.high = MV_CPU_TO_LE32(paddr.parts.high);
+			ctx->pSg++;
+
+			totalSize -= thisSize;
+			if( totalSize == 0 )
+				break;
+
+			if( !ctx->avail )
+				return 0;
+
+			vaddr = (MV_PVOID)((MV_PU8) vaddr + thisSize);
+		}
+	}
+	else
+	{
+		// including SGD_VP/SGD_PCTX
+		ctx->avail--;
+		ctx->itemCnt++;
+
+		ctx->pSg->flags = 0;
+#ifdef ODIN_DRIVER
+		ctx->pSg->size = MV_CPU_TO_LE32(sg->size);
+#else
+		ctx->pSg->size = MV_CPU_TO_LE32(sg->size - 1);
+#endif
+		ctx->pSg->baseAddr.parts.low = MV_CPU_TO_LE32(sg->baseAddr.parts.low);
+		ctx->pSg->baseAddr.parts.high = MV_CPU_TO_LE32(sg->baseAddr.parts.high);
+		ctx->pSg++;
+	}
+
+	return 1;
+}
+
+int sgdt_prepare_hwprd(
+	MV_PVOID		pCore,
+	sgd_tbl_t*		pSource,
+	sgd_t*			pSg,
+	int				availSgEntry
+	)
+{
+	PRDTableWalkCtx ctx;
+
+	ctx.pCore = pCore;
+	ctx.pSg = pSg;
+	ctx.avail = availSgEntry;
+	ctx.itemCnt = 0;
+
+	if( !sgd_table_walk( pSource, PRDTablePrepareVisitor, &ctx) )
+		return 0;
+
+	return ctx.itemCnt;
+}
+
+#endif // USE_NEW_SGTABLE
--- /dev/null
+++ b/drivers/scsi/thor/lib/common/com_tag.c
@@ -0,0 +1,71 @@
+#include "com_define.h"
+#include "com_tag.h"
+#include "com_dbg.h"
+
+MV_VOID Tag_Init( PTag_Stack pTagStack, MV_U16 size )
+{
+	MV_U16 i;
+
+	MV_DASSERT( size == pTagStack->Size );
+
+	pTagStack->Top = size;
+	pTagStack->TagStackType = FILO_TAG;
+	pTagStack->PtrOut = 0;
+	for ( i=0; i<size; i++ )
+	{
+		pTagStack->Stack[i] = size-1-i;
+	}
+}
+
+MV_VOID Tag_Init_FIFO( PTag_Stack pTagStack, MV_U16 size )
+{
+	MV_U16 i;
+
+	MV_DASSERT( size == pTagStack->Size );
+
+	pTagStack->Top = size;
+	pTagStack->TagStackType = FIFO_TAG;
+	pTagStack->PtrOut = 0;
+	for ( i=0; i<size; i++ )
+	{
+			pTagStack->Stack[i] = i;
+	}
+}
+
+MV_U16 Tag_GetOne(PTag_Stack pTagStack)
+{
+	MV_U16 nTag;
+
+	MV_DASSERT( pTagStack->Top>0 );
+	if(pTagStack->TagStackType==FIFO_TAG)
+	{
+		nTag = pTagStack->Stack[pTagStack->PtrOut++];
+		if(pTagStack->PtrOut>=pTagStack->Size)
+			pTagStack->PtrOut=0;
+		pTagStack->Top--;
+		return nTag;
+	}
+	else
+		return pTagStack->Stack[--pTagStack->Top];
+}
+
+MV_VOID Tag_ReleaseOne(PTag_Stack pTagStack, MV_U16 tag)
+{
+	MV_DASSERT( pTagStack->Top<pTagStack->Size );
+	if(pTagStack->TagStackType==FIFO_TAG)
+	{
+		pTagStack->Stack[(pTagStack->PtrOut+pTagStack->Top)%pTagStack->Size] = tag;
+		pTagStack->Top++;
+	}
+	else
+		pTagStack->Stack[pTagStack->Top++] = tag;
+}
+
+MV_BOOLEAN Tag_IsEmpty(PTag_Stack pTagStack)
+{
+	if ( pTagStack->Top==0 )
+	{
+		return MV_TRUE;
+	}
+	return MV_FALSE;
+}
--- /dev/null
+++ b/drivers/scsi/thor/lib/common/com_u64.c
@@ -0,0 +1,148 @@
+#include "com_define.h"
+
+MV_U64 U64_ADD_U32(MV_U64 v64, MV_U32 v32)
+{
+#ifdef _64_BIT_COMPILER
+	v64.value += v32;
+#else
+	v64.parts.low += v32;
+	v64.parts.high = 0;
+#endif
+	return v64;
+}
+
+MV_U64 U64_SUBTRACT_U32(MV_U64 v64, MV_U32 v32)
+{
+#ifdef _64_BIT_COMPILER
+	v64.value -= v32;
+#else
+	v64.parts.low -= v32;
+	v64.parts.high = 0;
+#endif
+	return v64;
+}
+
+MV_U64 U64_MULTIPLY_U32(MV_U64 v64, MV_U32 v32)
+{
+#ifdef _64_BIT_COMPILER
+	v64.value *= v32;
+#else
+	v64.parts.low *= v32;
+	v64.parts.high = 0;
+#endif
+	return v64;
+}
+
+MV_U32 U64_MOD_U32(MV_U64 v64, MV_U32 v32)
+{
+#ifdef _OS_LINUX
+	return do_div(v64.value, v32);
+#else
+	return (MV_U32) (v64.value % v32);
+#endif /* _OS_LINUX */
+}
+
+MV_U64 U64_DIVIDE_U32(MV_U64 v64, MV_U32 v32)
+{
+#ifdef _OS_LINUX
+	do_div(v64.value, v32);
+#else
+#ifdef _64_BIT_COMPILER
+	v64.value /= v32;
+#else
+	v64.parts.high = 0;
+	v64.parts.low /= v32;
+#endif /* _64_BIT_COMPILER */
+
+#endif /* _OS_LINUX */
+	return v64;
+}
+
+MV_I32 U64_COMPARE_U32(MV_U64 v64, MV_U32 v32)
+{
+	if (v64.parts.high > 0)
+		return 1;
+	if (v64.parts.low > v32)
+		return 1;
+#ifdef _64_BIT_COMPILER
+	else if (v64.value == v32)
+#else
+	else if (v64.parts.low == v32)
+#endif
+		return 0;
+	else
+		return -1;
+}
+
+MV_U64 U64_ADD_U64(MV_U64 v1, MV_U64 v2)
+{
+#ifdef _64_BIT_COMPILER
+	v1.value += v2.value;
+#else
+	v1.parts.low += v2.parts.low;
+	v1.parts.high = 0;
+#endif
+	return v1;
+}
+
+MV_U64 U64_SUBTRACT_U64(MV_U64 v1, MV_U64 v2)
+{
+#ifdef _64_BIT_COMPILER
+	v1.value -= v2.value;
+#else
+	v1.parts.low -= v2.parts.low;
+	v1.parts.high = 0;
+#endif
+	return v1;
+}
+
+MV_U32 U64_DIVIDE_U64(MV_U64 v1, MV_U64 v2)
+{
+#ifdef _OS_LINUX
+	MV_U32 ret = 0;
+	while (v1.value > v2.value) {
+		v1.value -= v2.value;
+		ret++;
+	}
+	return ret;
+#else
+#ifdef _64_BIT_COMPILER
+	v1.value /= v2.value;
+#else
+	v1.parts.high = 0;
+	v1.parts.low /= v2.parts.low;
+#endif
+	return v1.parts.low;
+#endif
+}
+
+MV_I32 U64_COMPARE_U64(MV_U64 v1, MV_U64 v2)
+{
+#ifdef _64_BIT_COMPILER
+	if (v1.value > v2.value)
+		return 1;
+	else if (v1.value == v2.value)
+		return 0;
+	else
+		return -1;
+#else
+	if (v1.value > v2.value)
+		return 1;
+	else if (v1.value == v2.value)
+		return 0;
+	else
+		return -1;
+
+#endif
+
+}
+
+#ifdef _OS_BIOS
+MV_U64 ZeroU64(MV_U64 v1)
+{
+	v1.parts.low = 0;
+	v1.parts.high = 0;
+
+	return	v1;
+}
+#endif /*  _OS_BIOS */
--- /dev/null
+++ b/drivers/scsi/thor/lib/common/com_util.c
@@ -0,0 +1,736 @@
+#include "com_define.h"
+#include "com_dbg.h"
+#include "com_scsi.h"
+#include "com_util.h"
+#include "com_u64.h"
+
+#if ERROR_HANDLING_SUPPORT
+void MV_ZeroMvRequest(PMV_Request pReq)
+{
+	PMV_SG_Entry pSGEntry;
+	MV_U8 maxEntryCount;
+	MV_PVOID pSenseBuffer;
+	MV_PVOID pbheh_ctx;
+#ifdef __RES_MGMT__
+	List_Head list;
+#endif
+	MV_DASSERT(pReq);
+	pSGEntry = pReq->SG_Table.Entry_Ptr;
+	maxEntryCount = pReq->SG_Table.Max_Entry_Count;
+	pSenseBuffer = pReq->Sense_Info_Buffer;
+	pbheh_ctx = pReq->bh_eh_ctx;
+#ifdef __RES_MGMT__
+	list = pReq->pool_entry;
+#endif
+	MV_ZeroMemory(pReq, MV_REQUEST_SIZE);
+#ifdef __RES_MGMT__
+	pReq->pool_entry = list;
+#endif
+	pReq->SG_Table.Entry_Ptr = pSGEntry;
+	pReq->SG_Table.Max_Entry_Count = maxEntryCount;
+	pReq->Sense_Info_Buffer = pSenseBuffer;
+	pReq->bh_eh_ctx = pbheh_ctx;
+}
+#else
+void MV_ZeroMvRequest(PMV_Request pReq)
+{
+	PMV_SG_Entry pSGEntry;
+	MV_U8 maxEntryCount;
+	MV_PVOID pSenseBuffer;
+#ifdef __RES_MGMT__
+	List_Head list;
+#endif
+
+	MV_DASSERT(pReq);
+	pSGEntry = pReq->SG_Table.Entry_Ptr;
+	maxEntryCount = pReq->SG_Table.Max_Entry_Count;
+	pSenseBuffer = pReq->Sense_Info_Buffer;
+#ifdef __RES_MGMT__
+	list = pReq->pool_entry;
+#endif
+	MV_ZeroMemory(pReq, MV_REQUEST_SIZE);
+#ifdef __RES_MGMT__
+	pReq->pool_entry = list;
+#endif
+	pReq->SG_Table.Entry_Ptr = pSGEntry;
+	pReq->SG_Table.Max_Entry_Count = maxEntryCount;
+	pReq->Sense_Info_Buffer = pSenseBuffer;
+}
+#endif
+
+void MV_CopySGTable(PMV_SG_Table pTargetSGTable, PMV_SG_Table pSourceSGTable)
+{
+	pTargetSGTable->Valid_Entry_Count = pSourceSGTable->Valid_Entry_Count;
+	pTargetSGTable->Flag = pSourceSGTable->Flag;
+	pTargetSGTable->Byte_Count = pSourceSGTable->Byte_Count;
+	MV_CopyMemory(pTargetSGTable->Entry_Ptr, pSourceSGTable->Entry_Ptr,
+					sizeof(MV_SG_Entry)*pTargetSGTable->Valid_Entry_Count);
+
+}
+
+#ifdef _OS_BIOS
+MV_U64 CPU_TO_BIG_ENDIAN_64(MV_U64 x)
+
+{
+	MV_U64 x1;
+	ZeroU64(x1);
+	x1.parts.low=CPU_TO_BIG_ENDIAN_32(x.parts.high);
+	x1.parts.high=CPU_TO_BIG_ENDIAN_32(x.parts.low);
+	return x1;
+}
+#endif	/* #ifdef _OS_BIOS */
+
+MV_BOOLEAN MV_Equals(
+	IN MV_PU8		des,
+	IN MV_PU8		src,
+	IN MV_U8		len
+)
+{
+	MV_U8 i;
+
+	for (i=0; i<len; i++){
+		if (*des != *src){
+			return MV_FALSE;
+		}
+		des++;
+		src++;
+	}
+	return MV_TRUE;
+}
+/*
+ * SG Table operation
+ */
+void SGTable_Init(
+	OUT PMV_SG_Table pSGTable,
+	IN MV_U8 flag
+	)
+{
+/*	pSGTable->Max_Entry_Count = MAX_SG_ENTRY;  set during module init */
+	pSGTable->Valid_Entry_Count = 0;
+	pSGTable->Flag = flag;
+	pSGTable->Byte_Count = 0;
+}
+
+#ifndef USE_NEW_SGTABLE
+void SGTable_Append(
+	OUT PMV_SG_Table pSGTable,
+	MV_U32 address,
+	MV_U32 addressHigh,
+	MV_U32 size
+	)
+{
+	PMV_SG_Entry pSGEntry;
+
+	pSGEntry = &pSGTable->Entry_Ptr[pSGTable->Valid_Entry_Count];
+
+	MV_ASSERT(pSGTable->Valid_Entry_Count < pSGTable->Max_Entry_Count);
+	/*
+	 * Workaround hardware issue:
+	 * If the transfer size is odd, some request cannot be finished.
+	 * Hopefully the workaround won't damage the system.
+	 */
+#ifdef PRD_SIZE_WORD_ALIGN
+	if ( size%2 ) size++;
+#endif
+
+	pSGTable->Valid_Entry_Count += 1;
+	pSGTable->Byte_Count += size;
+
+	pSGEntry->Base_Address = address;
+	pSGEntry->Base_Address_High = addressHigh;
+	pSGEntry->Size = size;
+	pSGEntry->Reserved0 = 0;
+}
+#endif
+
+MV_BOOLEAN SGTable_Available(
+	IN PMV_SG_Table pSGTable
+	)
+{
+	return (pSGTable->Valid_Entry_Count < pSGTable->Max_Entry_Count);
+}
+
+void MV_InitializeTargetIDTable(
+	IN PMV_Target_ID_Map pMapTable
+	)
+{
+	MV_FillMemory((MV_PVOID)pMapTable, sizeof(MV_Target_ID_Map)*MV_MAX_TARGET_NUMBER, 0xFF);
+}
+
+MV_U16 MV_MapTargetID(
+	IN PMV_Target_ID_Map	pMapTable,
+	IN MV_U16				deviceId,
+	IN MV_U8				deviceType
+	)
+{
+	MV_U16 i;
+	for (i=0; i<MV_MAX_TARGET_NUMBER; i++) {
+		if (pMapTable[i].Type==0xFF) {	/* not mapped yet */
+			pMapTable[i].Device_Id = deviceId;
+			pMapTable[i].Type = deviceType;
+			break;
+		}
+	}
+	return i;
+}
+
+MV_U16 MV_MapToSpecificTargetID(
+	IN PMV_Target_ID_Map	pMapTable,
+	IN MV_U16				specificId,
+	IN MV_U16				deviceId,
+	IN MV_U8				deviceType
+	)
+{
+	/* first check if the device can be mapped to the specific ID */
+	if (pMapTable[specificId].Type==0xFF) {	/* not used yet */
+		pMapTable[specificId].Device_Id = deviceId;
+		pMapTable[specificId].Type = deviceType;
+		return specificId;
+	}
+	/* cannot mapped to the specific ID */
+	/* just map the device to first available ID */
+	return MV_MapTargetID(pMapTable, deviceId, deviceType);
+}
+
+MV_U16 MV_RemoveTargetID(
+	IN PMV_Target_ID_Map	pMapTable,
+	IN MV_U16				deviceId,
+	IN MV_U8				deviceType
+	)
+{
+	MV_U16 i;
+	for (i=0; i<MV_MAX_TARGET_NUMBER; i++) {
+		if ( (pMapTable[i].Type==deviceType) && (pMapTable[i].Device_Id==deviceId) ) {
+			pMapTable[i].Type = 0xFF;
+			pMapTable[i].Device_Id = 0xFFFF;
+			break;
+		}
+	}
+	return i;
+}
+
+MV_U16 MV_GetMappedID(
+	IN PMV_Target_ID_Map	pMapTable,
+	IN MV_U16				deviceId,
+	IN MV_U8				deviceType
+	)
+{
+	MV_U16 mappedID;
+	for (mappedID=0; mappedID<MV_MAX_TARGET_NUMBER; mappedID++) {
+		if ( (pMapTable[mappedID].Type==deviceType) && (pMapTable[mappedID].Device_Id==deviceId) )
+			break;
+	}
+	if (mappedID >= MV_MAX_TARGET_NUMBER)
+		mappedID = 0xFFFF;
+	else {
+		MV_DASSERT(mappedID < MV_MAX_TARGET_NUMBER);
+#ifdef SUPPORT_SCSI_PASSTHROUGH
+		if (mappedID == VIRTUAL_DEVICE_ID) {
+			/* Device is on LUN 1 */
+			mappedID |= 0x0100;
+		}
+#endif /* SUPPORT_SCSI_PASSTHROUGH */
+	}
+	return mappedID;
+}
+
+void MV_DecodeReadWriteCDB(
+	IN MV_PU8 Cdb,
+	OUT MV_LBA *pLBA,
+	OUT MV_U32 *pSectorCount)
+{
+	MV_LBA tmpLBA;
+	MV_U32 tmpSectorCount;
+
+	if ((!SCSI_IS_READ(Cdb[0])) &&
+	    (!SCSI_IS_WRITE(Cdb[0])) &&
+	    (!SCSI_IS_VERIFY(Cdb[0])))
+		return;
+
+	/* This is READ/WRITE command */
+	switch (Cdb[0]) {
+	case SCSI_CMD_READ_6:
+	case SCSI_CMD_WRITE_6:
+		tmpLBA.value = (MV_U32)((((MV_U32)(Cdb[1] & 0x1F))<<16) |
+					((MV_U32)Cdb[2]<<8) |
+					((MV_U32)Cdb[3]));
+		tmpSectorCount = (MV_U32)Cdb[4];
+		break;
+	case SCSI_CMD_READ_10:
+	case SCSI_CMD_WRITE_10:
+	case SCSI_CMD_VERIFY_10:
+		tmpLBA.value = (MV_U32)(((MV_U32)Cdb[2]<<24) |
+					((MV_U32)Cdb[3]<<16) |
+					((MV_U32)Cdb[4]<<8) |
+					((MV_U32)Cdb[5]));
+		tmpSectorCount = ((MV_U32)Cdb[7]<<8) | (MV_U32)Cdb[8];
+		break;
+	case SCSI_CMD_READ_12:
+	case SCSI_CMD_WRITE_12:
+		tmpLBA.value = (MV_U32)(((MV_U32)Cdb[2]<<24) |
+					((MV_U32)Cdb[3]<<16) |
+					((MV_U32)Cdb[4]<<8) |
+					((MV_U32)Cdb[5]));
+		tmpSectorCount = (MV_U32)(((MV_U32)Cdb[6]<<24) |
+					  ((MV_U32)Cdb[7]<<16) |
+					  ((MV_U32)Cdb[8]<<8) |
+					  ((MV_U32)Cdb[9]));
+		break;
+	case SCSI_CMD_READ_16:
+	case SCSI_CMD_WRITE_16:
+	case SCSI_CMD_VERIFY_16:
+		tmpLBA.parts.high = (MV_U32)(((MV_U32)Cdb[2]<<24) |
+				       ((MV_U32)Cdb[3]<<16) |
+				       ((MV_U32)Cdb[4]<<8) |
+				       ((MV_U32)Cdb[5]));
+		tmpLBA.parts.low = (MV_U32)(((MV_U32)Cdb[6]<<24) |
+				      ((MV_U32)Cdb[7]<<16) |
+				      ((MV_U32)Cdb[8]<<8) |
+				      ((MV_U32)Cdb[9]));
+
+		tmpSectorCount = (MV_U32)(((MV_U32)Cdb[10]<<24) |
+					  ((MV_U32)Cdb[11]<<16) |
+					  ((MV_U32)Cdb[12]<<8) |
+					  ((MV_U32)Cdb[13]));
+		break;
+	default:
+		MV_DPRINT(("Unsupported READ/WRITE command [%x]\n", Cdb[0]));
+		U64_SET_VALUE(tmpLBA, 0);
+		tmpSectorCount = 0;
+	}
+	*pLBA = tmpLBA;
+	*pSectorCount = tmpSectorCount;
+}
+
+#ifndef _OS_BIOS
+void MV_DumpRequest(PMV_Request pReq, MV_BOOLEAN detail)
+{
+	MV_DPRINT(("Device[%d] Cdb[%2x,%2x,%2x,%2x, %2x,%2x,%2x,%2x, %2x,%2x,%2x,%2x].\n",
+		pReq->Device_Id,
+		pReq->Cdb[0],
+		pReq->Cdb[1],
+		pReq->Cdb[2],
+		pReq->Cdb[3],
+		pReq->Cdb[4],
+		pReq->Cdb[5],
+		pReq->Cdb[6],
+		pReq->Cdb[7],
+		pReq->Cdb[8],
+		pReq->Cdb[9],
+		pReq->Cdb[10],
+		pReq->Cdb[11]
+		));
+
+	if ( detail )
+	{
+		MV_DPRINT(("Scsi_Status=0x%x\n", pReq->Scsi_Status));
+		MV_DPRINT(("Tag=0x%x\n", pReq->Tag));
+		MV_DPRINT(("Data_Transfer_Length=%d\n", pReq->Data_Transfer_Length));
+		MV_DPRINT(("Sense_Info_Buffer_Length=%d\n", pReq->Sense_Info_Buffer_Length));
+		MV_DPRINT(("Org_Req : %p\n", pReq->Org_Req));
+	}
+}
+
+#if defined(SUPPORT_RAID6) && defined(RAID_DRIVER)
+void MV_DumpXORRequest(PMV_XOR_Request pXORReq, MV_BOOLEAN detail)
+{
+	MV_U32 i,j;
+
+	MV_PRINT("MV_XOR_Request: Type=0x%x, Source count=%d, Target count=%d\n",
+		pXORReq->Request_Type,
+		pXORReq->Source_SG_Table_Count,
+		pXORReq->Target_SG_Table_Count
+		);
+
+	if ( detail )
+	{
+		MV_PRINT("Source SG table...\n");
+		for ( i=0; i<pXORReq->Source_SG_Table_Count; i++ )
+			MV_DumpSGTable(&pXORReq->Source_SG_Table_List[i]);
+
+		MV_PRINT("Target SG table...\n");
+		for ( i=0; i<pXORReq->Target_SG_Table_Count; i++ )
+			MV_DumpSGTable(&pXORReq->Target_SG_Table_List[i]);
+
+		MV_PRINT("Coefficient...\n");
+		for ( i=0; i<pXORReq->Target_SG_Table_Count; i++ ) {
+			for ( j=0; j<pXORReq->Source_SG_Table_Count; j++ ) {
+				MV_PRINT("[%d,%d]=0x%x\n", i, j, pXORReq->Coef[i][j]);
+			}
+		}
+	}
+}
+#endif /* SUPPORT_RAID6 */
+
+void MV_DumpSGTable(PMV_SG_Table pSGTable)
+{
+#ifdef USE_NEW_SGTABLE
+	sgdt_dump(pSGTable, " ");
+#else
+	PMV_SG_Entry pSGEntry;
+	MV_U32 i;
+	MV_PRINT("SG Table: size(0x%x)\n", pSGTable->Byte_Count);
+	for ( i=0; i<pSGTable->Valid_Entry_Count; i++ )
+	{
+		pSGEntry = &pSGTable->Entry_Ptr[i];
+		MV_PRINT("%d: addr(0x%x-0x%x), size(0x%x).\n",
+			i, pSGEntry->Base_Address_High, pSGEntry->Base_Address, pSGEntry->Size);
+	}
+#endif
+}
+
+const char* MV_DumpSenseKey(MV_U8 sense)
+{
+	switch ( sense )
+	{
+		case SCSI_SK_NO_SENSE:
+			return "SCSI_SK_NO_SENSE";
+		case SCSI_SK_RECOVERED_ERROR:
+			return "SCSI_SK_RECOVERED_ERROR";
+		case SCSI_SK_NOT_READY:
+			return "SCSI_SK_NOT_READY";
+		case SCSI_SK_MEDIUM_ERROR:
+			return "SCSI_SK_MEDIUM_ERROR";
+		case SCSI_SK_HARDWARE_ERROR:
+			return "SCSI_SK_HARDWARE_ERROR";
+		case SCSI_SK_ILLEGAL_REQUEST:
+			return "SCSI_SK_ILLEGAL_REQUEST";
+		case SCSI_SK_UNIT_ATTENTION:
+			return "SCSI_SK_UNIT_ATTENTION";
+		case SCSI_SK_DATA_PROTECT:
+			return "SCSI_SK_DATA_PROTECT";
+		case SCSI_SK_BLANK_CHECK:
+			return "SCSI_SK_BLANK_CHECK";
+		case SCSI_SK_VENDOR_SPECIFIC:
+			return "SCSI_SK_VENDOR_SPECIFIC";
+		case SCSI_SK_COPY_ABORTED:
+			return "SCSI_SK_COPY_ABORTED";
+		case SCSI_SK_ABORTED_COMMAND:
+			return "SCSI_SK_ABORTED_COMMAND";
+		case SCSI_SK_VOLUME_OVERFLOW:
+			return "SCSI_SK_VOLUME_OVERFLOW";
+		case SCSI_SK_MISCOMPARE:
+			return "SCSI_SK_MISCOMPARE";
+		default:
+			MV_DPRINT(("Unknown sense key 0x%x.\n", sense));
+			return "Unknown sense key";
+	}
+}
+#endif	/* #ifndef _OS_BIOS */
+
+static MV_U32  BASEATTR crc_tab[] = {
+        0x00000000L, 0x77073096L, 0xee0e612cL, 0x990951baL, 0x076dc419L,
+        0x706af48fL, 0xe963a535L, 0x9e6495a3L, 0x0edb8832L, 0x79dcb8a4L,
+        0xe0d5e91eL, 0x97d2d988L, 0x09b64c2bL, 0x7eb17cbdL, 0xe7b82d07L,
+        0x90bf1d91L, 0x1db71064L, 0x6ab020f2L, 0xf3b97148L, 0x84be41deL,
+        0x1adad47dL, 0x6ddde4ebL, 0xf4d4b551L, 0x83d385c7L, 0x136c9856L,
+        0x646ba8c0L, 0xfd62f97aL, 0x8a65c9ecL, 0x14015c4fL, 0x63066cd9L,
+        0xfa0f3d63L, 0x8d080df5L, 0x3b6e20c8L, 0x4c69105eL, 0xd56041e4L,
+        0xa2677172L, 0x3c03e4d1L, 0x4b04d447L, 0xd20d85fdL, 0xa50ab56bL,
+        0x35b5a8faL, 0x42b2986cL, 0xdbbbc9d6L, 0xacbcf940L, 0x32d86ce3L,
+        0x45df5c75L, 0xdcd60dcfL, 0xabd13d59L, 0x26d930acL, 0x51de003aL,
+        0xc8d75180L, 0xbfd06116L, 0x21b4f4b5L, 0x56b3c423L, 0xcfba9599L,
+        0xb8bda50fL, 0x2802b89eL, 0x5f058808L, 0xc60cd9b2L, 0xb10be924L,
+        0x2f6f7c87L, 0x58684c11L, 0xc1611dabL, 0xb6662d3dL, 0x76dc4190L,
+        0x01db7106L, 0x98d220bcL, 0xefd5102aL, 0x71b18589L, 0x06b6b51fL,
+        0x9fbfe4a5L, 0xe8b8d433L, 0x7807c9a2L, 0x0f00f934L, 0x9609a88eL,
+        0xe10e9818L, 0x7f6a0dbbL, 0x086d3d2dL, 0x91646c97L, 0xe6635c01L,
+        0x6b6b51f4L, 0x1c6c6162L, 0x856530d8L, 0xf262004eL, 0x6c0695edL,
+        0x1b01a57bL, 0x8208f4c1L, 0xf50fc457L, 0x65b0d9c6L, 0x12b7e950L,
+        0x8bbeb8eaL, 0xfcb9887cL, 0x62dd1ddfL, 0x15da2d49L, 0x8cd37cf3L,
+        0xfbd44c65L, 0x4db26158L, 0x3ab551ceL, 0xa3bc0074L, 0xd4bb30e2L,
+        0x4adfa541L, 0x3dd895d7L, 0xa4d1c46dL, 0xd3d6f4fbL, 0x4369e96aL,
+        0x346ed9fcL, 0xad678846L, 0xda60b8d0L, 0x44042d73L, 0x33031de5L,
+        0xaa0a4c5fL, 0xdd0d7cc9L, 0x5005713cL, 0x270241aaL, 0xbe0b1010L,
+        0xc90c2086L, 0x5768b525L, 0x206f85b3L, 0xb966d409L, 0xce61e49fL,
+        0x5edef90eL, 0x29d9c998L, 0xb0d09822L, 0xc7d7a8b4L, 0x59b33d17L,
+        0x2eb40d81L, 0xb7bd5c3bL, 0xc0ba6cadL, 0xedb88320L, 0x9abfb3b6L,
+        0x03b6e20cL, 0x74b1d29aL, 0xead54739L, 0x9dd277afL, 0x04db2615L,
+        0x73dc1683L, 0xe3630b12L, 0x94643b84L, 0x0d6d6a3eL, 0x7a6a5aa8L,
+        0xe40ecf0bL, 0x9309ff9dL, 0x0a00ae27L, 0x7d079eb1L, 0xf00f9344L,
+        0x8708a3d2L, 0x1e01f268L, 0x6906c2feL, 0xf762575dL, 0x806567cbL,
+        0x196c3671L, 0x6e6b06e7L, 0xfed41b76L, 0x89d32be0L, 0x10da7a5aL,
+        0x67dd4accL, 0xf9b9df6fL, 0x8ebeeff9L, 0x17b7be43L, 0x60b08ed5L,
+        0xd6d6a3e8L, 0xa1d1937eL, 0x38d8c2c4L, 0x4fdff252L, 0xd1bb67f1L,
+        0xa6bc5767L, 0x3fb506ddL, 0x48b2364bL, 0xd80d2bdaL, 0xaf0a1b4cL,
+        0x36034af6L, 0x41047a60L, 0xdf60efc3L, 0xa867df55L, 0x316e8eefL,
+        0x4669be79L, 0xcb61b38cL, 0xbc66831aL, 0x256fd2a0L, 0x5268e236L,
+        0xcc0c7795L, 0xbb0b4703L, 0x220216b9L, 0x5505262fL, 0xc5ba3bbeL,
+        0xb2bd0b28L, 0x2bb45a92L, 0x5cb36a04L, 0xc2d7ffa7L, 0xb5d0cf31L,
+        0x2cd99e8bL, 0x5bdeae1dL, 0x9b64c2b0L, 0xec63f226L, 0x756aa39cL,
+        0x026d930aL, 0x9c0906a9L, 0xeb0e363fL, 0x72076785L, 0x05005713L,
+        0x95bf4a82L, 0xe2b87a14L, 0x7bb12baeL, 0x0cb61b38L, 0x92d28e9bL,
+        0xe5d5be0dL, 0x7cdcefb7L, 0x0bdbdf21L, 0x86d3d2d4L, 0xf1d4e242L,
+        0x68ddb3f8L, 0x1fda836eL, 0x81be16cdL, 0xf6b9265bL, 0x6fb077e1L,
+        0x18b74777L, 0x88085ae6L, 0xff0f6a70L, 0x66063bcaL, 0x11010b5cL,
+        0x8f659effL, 0xf862ae69L, 0x616bffd3L, 0x166ccf45L, 0xa00ae278L,
+        0xd70dd2eeL, 0x4e048354L, 0x3903b3c2L, 0xa7672661L, 0xd06016f7L,
+        0x4969474dL, 0x3e6e77dbL, 0xaed16a4aL, 0xd9d65adcL, 0x40df0b66L,
+        0x37d83bf0L, 0xa9bcae53L, 0xdebb9ec5L, 0x47b2cf7fL, 0x30b5ffe9L,
+        0xbdbdf21cL, 0xcabac28aL, 0x53b39330L, 0x24b4a3a6L, 0xbad03605L,
+        0xcdd70693L, 0x54de5729L, 0x23d967bfL, 0xb3667a2eL, 0xc4614ab8L,
+        0x5d681b02L, 0x2a6f2b94L, 0xb40bbe37L, 0xc30c8ea1L, 0x5a05df1bL,
+        0x2d02ef8dL
+};
+
+/* Calculate CRC and generate PD_Reference number */
+MV_U32 MV_CRC(
+	IN	MV_PU8		pData,
+	IN	MV_U16		len
+)
+{
+    MV_U16 i;
+    MV_U32 crc = MV_MAX_U32;
+
+    for (i = 0;  i < len;  i ++) {
+		crc = crc_tab[(crc ^ pData[i]) & 0xff] ^ (crc >> 8);
+    }
+    return MV_CPU_TO_BE32(crc);
+}
+
+#ifdef MV_DEBUG
+void MV_CHECK_OS_SG_TABLE(
+	IN PMV_SG_Table pSGTable
+	)
+{
+#ifndef USE_NEW_SGTABLE
+	/* Check whether there are duplicated entries pointed to the same physical address. */
+	MV_U32 i,j;
+	static MV_BOOLEAN assertSGTable = MV_TRUE;
+
+	if ( assertSGTable ) {
+		for ( i=0; i<pSGTable->Valid_Entry_Count; i++ ) {
+			for ( j=i+1; j<pSGTable->Valid_Entry_Count; j++ ) {
+				MV_DASSERT( pSGTable->Entry_Ptr[i].Base_Address!=pSGTable->Entry_Ptr[j].Base_Address );
+			}
+		}
+	}
+#endif
+}
+#endif /* MV_DEBUG */
+
+#ifdef _OS_LINUX
+
+#if 1//def MV_DEBUG
+/*
+ * Insert a new entry between two known consecutive entries.
+ *
+ * This is only for internal list manipulation where we know
+ * the prev/next entries already!
+ */
+
+void list_add_debug(struct list_head *new_list,
+			      struct list_head *prev,
+			      struct list_head *next)
+{
+	if (unlikely(next->prev != prev)) {
+		MV_DPRINT(( "list_add corruption. next->prev should be "
+			"prev (0x%p), but was 0x%p. (next=0x%p).\n",
+			prev, next->prev, next));
+
+		MV_DPRINT(( "prev=0x%p, prev->next=0x%p,  prev->prev=0x%p"
+			"next=0x%p, next->next=0x%p, next->prev=0x%p.\n",
+			prev, prev->next, prev->prev, next, next->next, next->prev));
+#ifdef MV_LINUX_KGDB
+		MV_DASSERT(0);
+#endif
+
+	}
+	if (unlikely(prev->next != next)) {
+		MV_DPRINT(( "list_add corruption. prev->next should be "
+			"next (0x%p), but was 0x%p. (prev=0x%p).\n",
+			next, prev->next, prev));
+
+		MV_DPRINT(( "prev=0x%p, prev->next=0x%p,  prev->prev=0x%p,"
+			"next=0x%p, next->next=0x%p, next->prev=0x%p.\n",
+			prev, prev->next, prev->prev, next, next->next, next->prev));
+
+#ifdef MV_LINUX_KGDB
+		MV_DASSERT(0);
+#endif
+
+	}
+	next->prev = new_list;
+	new_list->next = next;
+	new_list->prev = prev;
+	prev->next = new_list;
+}
+
+/**
+ * list_del - deletes entry from list.
+ * @entry: the element to delete from the list.
+ * Note: list_empty on entry does not return true after this, the entry is
+ * in an undefined state.
+ */
+void List_Del(struct list_head *entry)
+{
+	if (unlikely(entry->prev->next != entry)) {
+		MV_DPRINT(("list_del corruption. prev->next should be %p, "
+				"but was %p\n", entry, entry->prev->next));
+		MV_DPRINT(( "entry->next=%p, entry->next->next=%p,  entry->next->prev=%p.\n"
+				"entry->prev=%p, entry->prev->next=%p, entry->prev->prev=%p.\n",
+			entry->prev, entry->next->next, entry->next->prev, entry->prev, entry->prev->next, entry->prev->prev));
+
+#ifdef MV_LINUX_KGDB
+		MV_DASSERT(0);
+#endif
+	}
+	if (unlikely(entry->next->prev != entry)) {
+		MV_DPRINT(( "list_del corruption. next->prev should be %p, "
+				"but was %p\n", entry, entry->next->prev));
+
+		MV_DPRINT(("entry->next=%p, entry->next->next=%p,  entry->next->prev=%p.\n"
+				"entry->prev=%p, entry->prev->next=%p, entry->prev->prev=%p.\n",
+			entry->next, entry->next->next, entry->next->prev, entry->prev, entry->prev->next, entry->prev->prev));
+
+#ifdef MV_LINUX_KGDB
+		MV_DASSERT(0);
+#endif
+	}
+
+	if(likely(entry->prev->next == entry) && likely(entry->next->prev == entry))
+		__list_del(entry->prev, entry->next);
+
+	entry->next = NULL;
+	entry->prev = NULL;
+}
+
+#else
+
+void List_Del(List_Head *entry)
+{
+#ifdef SUPPORT_REQUEST_TIMER
+          BUG_ON(!entry);
+          BUG_ON(!entry->prev);
+          BUG_ON(!entry->next);
+#endif
+//	list_del(entry);	//Not use kernel function, which set next,prev not NULL but LIST_POISON1 under DEBUG mode
+	__list_del(entry->prev, entry->next);
+	entry->next = NULL;
+	entry->prev = NULL;
+
+//	if((entry->prev) || (entry->next))
+//	__List_Del(entry->prev, entry->next);
+//	entry->next = NULL;
+//	entry->prev = NULL;
+}
+
+
+#endif
+
+
+void List_Add(List_Head *new_one, List_Head *head)
+{
+          BUG_ON(!head);
+          BUG_ON(!head->prev);
+          BUG_ON(!head->next);
+
+          BUG_ON(!new_one);
+
+#ifdef MV_DEBUG
+	list_add_debug(new_one, head, head->next);
+#else
+	list_add(new_one, head);
+#endif
+        BUG_ON(!new_one->next);
+         BUG_ON(!new_one->prev);
+
+}
+
+void List_AddTail(List_Head *new_one, List_Head *head)
+{
+          BUG_ON(!head);
+          BUG_ON(!head->prev);
+          BUG_ON(!head->next);
+
+          BUG_ON(!new_one);
+
+#ifdef MV_DEBUG
+	list_add_debug(new_one, head->prev, head);
+#else
+	list_add_tail(new_one, head);
+#endif
+        BUG_ON(!new_one->next);
+         BUG_ON(!new_one->prev);
+
+}
+
+
+
+struct list_head *List_GetFirst(struct list_head *head)
+{
+        struct list_head * one = NULL;
+          BUG_ON(!head);
+          BUG_ON(!head->prev);
+          BUG_ON(!head->next);
+
+	if (unlikely(head->prev->next != head)) {
+		MV_DPRINT(( "List_GetFirst corruption. prev->next should be 0x%p, "
+				"but was 0x%p\n", head, head->prev->next));
+
+		MV_DPRINT(( "head=0x%p, head->next=0x%p,  head->prev=0x%p,"
+			"head->prev->next=0x%p, head->prev->prev=0x%p.\n",
+			head, head->next, head->prev,  head->prev->next, head->prev->prev));
+
+#ifdef MV_LINUX_KGDB
+		MV_DASSERT(0);
+#endif
+	}
+	if (unlikely(head->next->prev != head)) {
+		MV_DPRINT(( "List_GetFirst corruption. next->prev should be 0x%p, "
+				"but was 0x%p\n", head, head->next->prev));
+
+		MV_DPRINT(( "head=%p, head->next=%p,  head->prev=%p,"
+			"head->next->next=%p, head->next->prev=%p.\n",
+			head, head->next, head->prev, head->next->next, head->next->prev));
+
+#ifdef MV_LINUX_KGDB
+		MV_DASSERT(0);
+#endif
+	}
+
+	if (list_empty(head))
+		return NULL;
+
+        one = head->next;
+        BUG_ON(!one->next);
+        BUG_ON(!one->prev);
+	List_Del(one);
+
+	if (unlikely(head->prev->next != head)) {
+		MV_DPRINT(( "List_GetFirst after del corruption. prev->next should be 0x%p, "
+				"but was 0x%p\n", head, head->prev->next));
+#ifdef MV_LINUX_KGDB
+		MV_DASSERT(0);
+#endif
+	}
+	if (unlikely(head->next->prev != head)) {
+		MV_DPRINT(("List_GetFirst after del corruption. next->prev should be 0x%p, "
+				"but was 0x%p\n", head, head->next->prev));
+#ifdef MV_LINUX_KGDB
+		MV_DASSERT(0);
+#endif
+	}
+
+        return one;
+}
+
+struct list_head *List_GetLast(struct list_head *head)
+{
+        struct list_head * one = NULL;
+            BUG_ON(!head);
+          BUG_ON(!head->prev);
+          BUG_ON(!head->next);
+        if (list_empty(head))
+		return NULL;
+
+        one = head->prev;
+         BUG_ON(!one->next);
+        BUG_ON(!one->prev);
+        List_Del(one);
+        return one;
+}
+
+
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/linux/hba_exp.c
@@ -0,0 +1,305 @@
+#include "hba_header.h"
+#include "linux_main.h"
+
+#include "hba_exp.h"
+
+/*
+ *
+ * Other exposed functions
+ *
+ */
+int __mv_is_mod_all_started(struct mv_adp_desc *adp_desc);
+/*
+ * The extension is the calling module extension.
+ *   It can be any module extension.
+ */
+void HBA_ModuleStarted(struct mv_mod_desc *mod_desc)
+{
+	struct hba_extension *hba;
+	struct mv_mod_desc *desc;
+	MV_DBG(DMSG_KERN, "start HBA_ModuleStarted addr %p.\n",mod_desc);
+
+	desc = mod_desc;
+	while (desc->parent)
+		desc = desc->parent; /* hba to be the uppermost */
+	hba = (struct hba_extension *) desc->extension;
+
+	if (__mv_is_mod_all_started(desc->hba_desc)) {
+		MV_DBG(DMSG_HBA, "all modules have been started.\n");
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+		atomic_set(&hba->hba_sync, 0);
+#else
+		complete(&hba->cmpl);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+		/* We are totally ready for requests handling. */
+		hba->State = DRIVER_STATUS_STARTED;
+
+		/* Module 0 is the last module */
+		hba->desc->ops->module_notification(hba,
+						    EVENT_MODULE_ALL_STARTED,
+						    NULL);
+	} else {
+		/*
+		 * hba's start has already been called, so we should not
+		 * call it here. (hba is the highest module, it has no parent.)
+		 */
+		if (mod_desc->parent && mod_desc->parent->parent)
+		{
+			MV_DBG(DMSG_HBA, "start module %d.....\n",mod_desc->parent->module_id);
+			mod_desc->parent->ops->module_start(mod_desc->parent->extension);
+		}
+	}
+
+}
+
+#ifdef __BIG_ENDIAN
+void hba_swap_buf_le16(u16 *buf, unsigned int words)
+{
+
+	unsigned int i;
+
+	for (i=0; i < words; i++)
+                buf[i] = le16_to_cpu(buf[i]);
+
+}
+#else
+inline void hba_swap_buf_le16(u16 *buf, unsigned int words) {}
+#endif /* __BIG_ENDIAN */
+
+void hba_map_sg_to_buffer(void *preq)
+{
+	struct scsi_cmnd *scmd =NULL;
+	struct scatterlist *sg =NULL;
+	PMV_Request        req =NULL;
+
+	req  = (PMV_Request) preq;
+
+	if (REQ_TYPE_OS != req->Req_Type)
+		return;
+
+	scmd = (struct scsi_cmnd *) req->Org_Req;
+	sg = (struct scatterlist *) mv_rq_bf(scmd);
+
+
+	if (mv_use_sg(scmd)) {
+		if (mv_use_sg(scmd) > 1)
+			MV_DBG(DMSG_SCSI,
+			       "_MV_ more than 1 sg entry in an inst cmd.\n");
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+		req->Data_Buffer = kmalloc(sg->length, GFP_ATOMIC);
+		if (req->Data_Buffer) {
+			memset(req->Data_Buffer, 0, sg->length);
+		}
+#else
+		req->Data_Buffer = kzalloc(sg->length, GFP_ATOMIC);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14) */
+
+		req->Data_Transfer_Length = sg->length;
+	} else {
+	        req->Data_Buffer = mv_rq_bf(scmd);
+	}
+}
+
+void hba_unmap_sg_to_buffer(void *preq)
+{
+	void *buf;
+	struct scsi_cmnd *scmd = NULL;
+	struct scatterlist *sg = NULL;
+	PMV_Request        req = NULL;
+	unsigned long flags = 0;
+
+	req  = (PMV_Request) preq;
+
+	if (REQ_TYPE_OS != req->Req_Type)
+		return;
+
+	scmd = (struct scsi_cmnd *) req->Org_Req;
+	sg   = (struct scatterlist *)mv_rq_bf(scmd);
+
+	if (mv_use_sg(scmd)) {
+		//MV_WARNON(!irqs_disabled());
+		local_irq_save(flags);
+		buf = map_sg_page(sg) + sg->offset;
+		memcpy(buf, req->Data_Buffer, sg->length);
+		kunmap_atomic(buf, KM_IRQ0);
+		kfree(req->Data_Buffer);
+		local_irq_restore(flags);
+	}
+}
+
+MV_PVOID HBA_GetModuleExtension(MV_PVOID ext, MV_U32 mod_id)
+{
+	struct mv_mod_desc *mod_desc=(struct mv_mod_desc *)__ext_to_gen(ext)->desc;
+	struct mv_adp_desc *hba_desc = mod_desc->hba_desc;
+	MV_ASSERT(mod_id<MAX_MODULE_NUMBER);
+	BUG_ON(NULL == mod_desc);
+	list_for_each_entry(mod_desc, &hba_desc->online_module_list,
+			    mod_entry)
+	{
+		BUG_ON(NULL == mod_desc);
+		if (mod_desc->status != MV_MOD_GONE)
+		{
+			if ((mod_desc->module_id == mod_id) && (mod_desc->extension))
+			{
+				return mod_desc->extension;
+			}
+		}
+	}
+
+	MV_ASSERT(MV_FALSE);
+	return	NULL;
+}
+#ifdef THOR_DRIVER
+void HBA_TimerRoutine(unsigned long DeviceExtension)
+{
+#ifndef SUPPORT_TIMER
+	PHBA_Extension pHBA   = (PHBA_Extension)HBA_GetModuleExtension((MV_PVOID)DeviceExtension, MODULE_HBA);
+	PTimer_Module  pTimer = &pHBA->Timer_Module;
+	unsigned long  flags;
+
+	MV_DASSERT(pTimer->routine != NULL);
+	spin_lock_irqsave(&pHBA->desc->hba_desc->global_lock, flags);
+	pTimer->routine(pTimer->context);
+	spin_unlock_irqrestore(&pHBA->desc->hba_desc->global_lock, flags);
+#endif /* SUPPORT_TIMER */
+}
+
+
+void HBA_RequestTimer(
+	MV_PVOID extension,
+	MV_U32 millisecond,
+	MV_VOID (*routine) (MV_PVOID)
+	)
+{
+	PHBA_Extension pHBA   = (PHBA_Extension)HBA_GetModuleExtension(extension,MODULE_HBA);
+	PTimer_Module pTimer = &pHBA->Timer_Module;
+	u64 jif_offset;
+
+	pTimer->routine = routine;
+	pTimer->context = extension;
+	del_timer(&pHBA->timer);
+	pHBA->timer.function = HBA_TimerRoutine;
+	pHBA->timer.data = (unsigned long)extension;
+	jif_offset = (u64)(millisecond * HZ);
+	do_div(jif_offset, 1000);
+	pHBA->timer.expires = jiffies + 1 + jif_offset;
+	add_timer(&pHBA->timer);
+}
+
+void hba_spin_lock_irq(spinlock_t* plock)
+{
+	WARN_ON(irqs_disabled());
+	spin_lock_irq(plock);
+}
+
+void hba_spin_unlock_irq(spinlock_t* plock)
+{
+	spin_unlock_irq(plock);
+}
+
+
+#endif
+
+#define LO_ADDR(x) ((MV_U32) ((MV_PTR_INTEGER) (x)))
+#define HI_ADDR(x) ((MV_U32) (sizeof(void *)>4?((MV_PTR_INTEGER) (x))>>32:0))
+
+MV_BOOLEAN __is_scsi_cmd_simulated(MV_U8 cmd_type)
+{
+	switch (cmd_type)
+	{
+	case SCSI_CMD_INQUIRY:
+	case SCSI_CMD_READ_CAPACITY_10:
+	case SCSI_CMD_READ_CAPACITY_16:
+	case SCSI_CMD_SYNCHRONIZE_CACHE_10:
+	case SCSI_CMD_TEST_UNIT_READY:
+	case SCSI_CMD_REQUEST_SENSE:
+	case SCSI_CMD_RESERVE_6:
+	case SCSI_CMD_RELEASE_6:
+	case SCSI_CMD_REPORT_LUN:
+	case SCSI_CMD_MODE_SENSE_6:
+	case SCSI_CMD_MODE_SENSE_10:
+	case SCSI_CMD_MODE_SELECT_6:
+	case SCSI_CMD_MODE_SELECT_10:
+#ifdef SUPPORT_ATA_SMART
+	case SCSI_CMD_LOG_SENSE:
+	case SCSI_CMD_READ_DEFECT_DATA_10:
+#endif
+#ifdef CORE_SUPPORT_API
+	case APICDB0_PD:
+#   ifdef SUPPORT_PASS_THROUGH_DIRECT
+	case APICDB0_PASS_THRU_CMD:
+#   endif /* SUPPORT_PASS_THROUGH_DIRECT */
+#   ifdef SUPPORT_CSMI
+	case APICDB0_CSMI_CORE:
+#   endif /* SUPPORT_CSMI */
+#endif /* CORE_SUPPORT_API */
+		return MV_TRUE;
+	default:
+		return MV_FALSE;
+	}
+}
+
+MV_U32 hba_parse_ata_protocol(struct scsi_cmnd *scmd)
+{
+	MV_U8 protocol, t_length, t_dir;
+	MV_U32 cmd_flag =0;
+
+	protocol = (scmd->cmnd[1]>> 1) & 0x0F;
+	if(protocol== HARD_RESET || protocol==SRST){
+		MV_PRINT("Unsupported ATA Protocol = 0x%x\n", protocol);
+		return cmd_flag;
+	}
+
+	t_length = scmd->cmnd[2] & 0x03;
+	t_dir = (scmd->cmnd[2] >> 3) & 0x01;
+
+	if (t_length == 0){
+		cmd_flag = CMD_FLAG_NON_DATA;
+	}else {
+		if (t_dir == 0)
+			cmd_flag = CMD_FLAG_DATA_OUT;
+		else
+			cmd_flag = CMD_FLAG_DATA_IN;
+	}
+	switch (protocol) {
+	case NON_DATA:
+		cmd_flag |= CMD_FLAG_NON_DATA;
+		break;
+	case PIO_DATA_IN:
+		cmd_flag |= CMD_FLAG_PIO;
+		if (!(cmd_flag & CMD_FLAG_DATA_IN))
+			cmd_flag |= CMD_FLAG_DATA_IN;
+		break;
+	case PIO_DATA_OUT:
+		cmd_flag |= CMD_FLAG_PIO;
+		if (!(cmd_flag & CMD_FLAG_DATA_OUT))
+			cmd_flag |= CMD_FLAG_DATA_OUT;
+		break;
+	case DMA:
+		cmd_flag |= CMD_FLAG_DMA;
+		break;
+	case DMA_QUEUED:
+		cmd_flag |= (CMD_FLAG_DMA | CMD_FLAG_TCQ);
+		break;
+	case DEVICE_DIAGNOSTIC:
+	case DEVICE_RESET:
+		/* Do nothing*/
+		break;
+	case UDMA_DATA_IN:
+		cmd_flag |= CMD_FLAG_DMA;
+		break;
+	case UDMA_DATA_OUT:
+		cmd_flag |= CMD_FLAG_DMA;
+		break;
+	case FPDMA:
+		cmd_flag |= (CMD_FLAG_DMA | CMD_FLAG_NCQ);
+		break;
+	case RTN_INFO:
+		break;
+	default:
+		MV_PRINT("Unsupported ATA Protocol = 0x%x\n", protocol);
+		break;
+	}
+	return cmd_flag;
+}
--- /dev/null
+++ b/drivers/scsi/thor/linux/hba_exp.h
@@ -0,0 +1,196 @@
+#ifndef __HBA_EXPOSE_H__
+#define __HBA_EXPOSE_H__
+
+#ifdef SUPPORT_EVENT
+#include "com_event_struct.h"
+#include "com_event_define.h"
+#include "com_event_define_ext.h"
+#endif /* SUPPORT_EVENT */
+
+#include "hba_header.h"
+#include "com_mod_mgmt.h"
+
+struct _MV_SCP {
+	MV_U16           mapped;
+	MV_U16           map_atomic;
+	BUS_ADDRESS bus_address;
+};
+
+#define MV_SCp(cmd) ((struct _MV_SCP *)(&((struct scsi_cmnd *)cmd)->SCp))
+
+typedef struct _Assigned_Uncached_Memory
+{
+	MV_PVOID			Virtual_Address;
+	MV_PHYSICAL_ADDR	Physical_Address;
+	MV_U32				Byte_Size;
+	MV_U32				Reserved0;
+} Assigned_Uncached_Memory, *PAssigned_Uncached_Memory;
+
+typedef struct _Controller_Infor
+{
+        MV_PVOID Base_Address;
+
+        MV_U16 Vendor_Id;
+        MV_U16 Device_Id;
+        MV_U8 Revision_Id;
+        MV_U8 Reserved[3];
+} Controller_Infor, *PController_Infor;
+
+void HBA_ModuleStarted(struct mv_mod_desc *mod_desc);
+
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+#define IS_ATA_PASS_THROUGH_COMMAND(pReq) \
+	((pReq->Cdb[0] == SCSI_CMD_MARVELL_SPECIFIC) && \
+		(pReq->Cdb[1] == CDB_CORE_MODULE) && (\
+			pReq->Cdb[2] == CDB_CORE_ATA_SLEEP || \
+			pReq->Cdb[2] == CDB_CORE_ATA_IDLE || \
+			pReq->Cdb[2] == CDB_CORE_ATA_STANDBY || \
+			pReq->Cdb[2] == CDB_CORE_ATA_IDLE_IMMEDIATE || \
+			pReq->Cdb[2] == CDB_CORE_ATA_CHECK_POWER_MODE || \
+			pReq->Cdb[2] == CDB_CORE_ATA_STANDBY_IMMEDIATE || \
+			pReq->Cdb[2] == CDB_CORE_RESET_DEVICE))
+#endif
+
+#define IS_ATA_12_CMD(scmd) \
+	((scmd->cmnd[0]==ATA_12)&& \
+	 (scmd->cmnd[9] ==0x08||scmd->cmnd[9] ==0xE0|| \
+	 scmd->cmnd[9] ==0xE1||scmd->cmnd[9] ==0xE2|| \
+	 scmd->cmnd[9] ==0xE3|| scmd->cmnd[9] ==0xE5||\
+	 scmd->cmnd[9] ==0xE6||scmd->cmnd[9] ==0xEC|| \
+	 scmd->cmnd[9] ==0xA1||scmd->cmnd[9] ==0x92|| \
+	 scmd->cmnd[9] ==0xB0))
+
+
+/*ATA Protocols*/
+enum _ATA_PROTOCOL {
+	HARD_RESET 	= 0x00,
+	SRST  			= 0x01,
+	BUS_IDLE 		= 0x02,
+	NON_DATA 		= 0x03,
+	PIO_DATA_IN 	= 0x04,
+	PIO_DATA_OUT 	= 0x05,
+	DMA			= 0x06,
+	DMA_QUEUED	= 0x07,
+	DEVICE_DIAGNOSTIC	= 0x08,
+	DEVICE_RESET		= 0x09,
+	UDMA_DATA_IN		= 0x0A,
+	UDMA_DATA_OUT	= 0x0B,
+	FPDMA				= 0x0C,
+	RTN_INFO			= 0x0F,
+};
+
+#ifdef SUPPORT_EVENT
+/* wrapper for DriverEvent, needed to implement queue */
+typedef struct _Driver_Event_Entry
+{
+	struct list_head Queue_Pointer;
+	DriverEvent Event;
+} Driver_Event_Entry, *PDriver_Event_Entry;
+#endif /* SUPPORT_EVENT */
+
+struct gen_module_desc {
+#ifdef __MM_SE__
+/* Must the first */
+	struct mv_mod_desc *desc;
+#else
+	MV_PVOID	reserved;
+#endif /* __MM_SE__ */
+};
+
+#define __ext_to_gen(_ext)       ((struct gen_module_desc *) (_ext))
+
+#define HBA_GetNextModuleSendFunction(_ext, _child_ext_p, _func_pp)          \
+           {                                                                 \
+		   *(_func_pp) = __ext_to_gen(_ext)->desc->child->ops->module_sendrequest; \
+		   *(_child_ext_p) = __ext_to_gen(_ext)->desc->child->extension;           \
+	   }
+
+#define HBA_GetUpperModuleNotificationFunction(_ext, _parent_ext_p, _func_pp) \
+           {                                                                  \
+		   *(_func_pp) = __ext_to_gen(_ext)->desc->parent->ops->module_notification; \
+		   *(_parent_ext_p) = __ext_to_gen(_ext)->desc->parent->extension;           \
+	   }
+
+
+#define hba_notify_upper_md(ext, eid, param)                 			 	\
+   {                                                                     	\
+	__ext_to_gen(ext)->desc->parent->ops->module_notification(       		\
+                                            __ext_to_gen(ext)->desc->parent->extension, 	\
+										eid,         				 	\
+										param);					 	\
+   }
+
+
+
+#define HBA_GetControllerInfor(_ext, _pinfo)                              \
+           {    \
+		   (_pinfo)->Base_Address = __ext_to_gen(_ext)->desc->hba_desc->Base_Address; \
+		   (_pinfo)->Vendor_Id    = __ext_to_gen(_ext)->desc->hba_desc->vendor;    \
+		   (_pinfo)->Device_Id    = __ext_to_gen(_ext)->desc->hba_desc->device;    \
+		   (_pinfo)->Revision_Id  = __ext_to_gen(_ext)->desc->hba_desc->Revision_Id;  \
+	   }
+
+
+void hba_swap_buf_le16(u16 *buf, unsigned int words);
+
+/* map bus addr in sg entry into cpu addr (access via. Data_Buffer) */
+void hba_map_sg_to_buffer(void *preq);
+void hba_unmap_sg_to_buffer(void *preq);
+
+void hba_log_msg(unsigned char *buf, unsigned int len);
+
+int __hba_dump_log(unsigned char *buf);
+
+static inline MV_BOOLEAN
+HBA_ModuleGetPhysicalAddress(MV_PVOID Module,
+			     MV_PVOID Virtual,
+			     MV_PVOID TranslationContext,
+			     MV_PU64 PhysicalAddress,
+			     MV_PU32 Length)
+{
+	panic("not supposed to be called.\n");
+	return MV_FALSE;
+};
+
+void __hba_dump_req_info(unsigned int module, PMV_Request req);
+int HBA_GetResource(struct mv_mod_desc *mod_desc,
+		    enum Resource_Type type,
+		    MV_U32  size,
+		    Assigned_Uncached_Memory *dma_res);
+MV_PVOID HBA_GetModuleExtension(MV_PVOID ext, MV_U32 mod_id);
+void HBA_ModuleNotification(MV_PVOID This,
+			     enum Module_Event event,
+			     struct mod_notif_param *event_param);
+
+#ifdef THOR_DRIVER
+void HBA_RequestTimer(
+	MV_PVOID extension,
+	MV_U32 millisecond,
+	MV_VOID (*routine) (MV_PVOID)
+	);
+void hba_spin_lock_irq(spinlock_t* plock);
+void hba_spin_unlock_irq(spinlock_t* plock);
+
+#endif
+
+#ifdef SUPPORT_REQUEST_TIMER
+#define 	add_request_timer(pReq, time, func, ctx)			\
+			{													\
+				hba_add_timer(pReq, time, (OSSW_TIMER_FUNCTION)(func));					\
+				(pReq)->err_request_ctx=(MV_PVOID)(ctx);							\
+			}
+
+#define 	remove_request_timer(pReq)			\
+			{													\
+				hba_remove_timer(pReq);					\
+			}
+
+#endif
+
+MV_BOOLEAN __is_scsi_cmd_simulated(MV_U8 cmd_type);
+void HBA_kunmap_sg(void*);
+/*set pci Device ID */
+MV_U16 SetDeviceID(MV_U32 pad_test);
+MV_U32 hba_parse_ata_protocol(struct scsi_cmnd *scmd);
+
+#endif /* __HBA_EXPOSE_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/linux/hba_header.h
@@ -0,0 +1,27 @@
+#ifndef __MV_HBA_HEADER_LINUX__
+#define  __MV_HBA_HEADER_LINUX__
+
+struct hba_extension;
+typedef struct hba_extension HBA_Extension, *PHBA_Extension;
+
+#include "mv_os.h"
+
+#include "com_type.h"
+#include "com_u64.h"
+#include "com_util.h"
+#include "com_list.h"
+#include "com_dbg.h"
+#include "com_scsi.h"
+#include "com_api.h"
+#include "com_struct.h"
+#include "com_event_struct.h"
+#include "com_sgd.h"
+
+#include "oss_wrapper.h"
+#include "hba_mod.h"
+#include "hba_timer.h"
+#include "hba_inter.h"
+
+#include "res_mgmt.h"
+
+#endif /* __MV_HBA_HEADER_LINUX__ */
--- /dev/null
+++ b/drivers/scsi/thor/linux/hba_inter.h
@@ -0,0 +1,64 @@
+#ifndef HBA_INTERNAL_H
+#define HBA_INTERNAL_H
+
+#include "hba_header.h"
+
+typedef struct _Timer_Module
+{
+	MV_PVOID context;
+	MV_VOID (*routine) (MV_PVOID);
+} Timer_Module, *PTimer_Module;
+
+
+struct hba_extension {
+/* Must be the first */
+/* self-descriptor */
+	struct mv_mod_desc *desc;
+	/* Device extention */
+	MV_PVOID host_data;
+
+
+	struct list_head        next;
+	struct pci_dev          *dev;
+	struct cdev             cdev;
+
+#ifdef THOR_DRIVER
+	struct timer_list	timer;
+	struct _Timer_Module    Timer_Module;
+#endif
+	//spinlock_t              lock;
+
+	struct Scsi_Host	*host;
+	struct completion       cmpl;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_t                hba_sync;
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+
+	MV_U32                  State;
+	MV_BOOLEAN              Is_Dump;
+	MV_U8                   Io_Count;
+	MV_U16                  Max_Io;
+
+#ifdef SUPPORT_EVENT
+	struct list_head        Stored_Events;
+	struct list_head        Free_Events;
+	MV_U32	                SequenceNumber;
+	MV_U8                   Num_Stored_Events;
+#endif /* SUPPORT_EVENT */
+	MV_PVOID                req_pool;
+
+	MV_U8                   Memory_Pool[1];
+
+#ifdef SUPPORT_TASKLET
+	struct tasklet_struct mv_tasklet;
+#endif
+
+};
+
+#define DRIVER_STATUS_IDLE      1    /* The first status */
+#define DRIVER_STATUS_STARTING  2    /* Begin to start all modules */
+#define DRIVER_STATUS_STARTED   3    /* All modules are all settled. */
+#define DRIVER_STATUS_SHUTDOWN   4    /* All modules are all settled. */
+
+#endif /* HBA_INTERNAL_H */
--- /dev/null
+++ b/drivers/scsi/thor/linux/hba_mod.c
@@ -0,0 +1,2822 @@
+/*
+ * module management module - module of the modules
+ *
+ *
+ */
+#define IOCTL_TEMPLATE
+#ifdef IOCTL_TEMPLATE
+#define SECTOR_SIZE 512
+#include <linux/hdreg.h>
+
+#endif
+#include "hba_header.h"
+
+#include "linux_main.h"
+#include "linux_iface.h"
+
+#include "hba_timer.h"
+#include "hba_mod.h"
+
+#include "res_mgmt.h"
+
+#include "hba_exp.h"
+#include <scsi/scsi_tcq.h>
+#include "core_inter.h"
+#include "core_sat.h"
+#include "core_ata.h"
+#include <scsi/scsi_eh.h>
+
+#if 0
+static void __dump_hex(const unsigned char *buf, int len)
+{
+	int i;
+
+	for (i = 0; i < len; i++) {
+		if (i && !(i % 8))
+			printk(YELLOW(" :"));
+		if (i && !(i % 16))
+			printk("\n");
+		printk(" %02X", buf[i]);
+	}
+	printk("\n");
+}
+#endif
+
+void __hba_dump_req_info(MV_U32 id, PMV_Request preq)
+{
+#if 0
+	unsigned long lba =0;
+	char *buf;
+	struct scsi_cmnd *scmd = NULL;
+	struct scatterlist *sg = NULL;
+
+	switch (preq->Cdb[0]) {
+	case SCSI_CMD_READ_10:
+	case SCSI_CMD_WRITE_10:
+		lba = preq->Cdb[2]<<24 | preq->Cdb[3]<<16 | preq->Cdb[4]<<8 | \
+			preq->Cdb[5];
+		break;
+	case SCSI_CMD_INQUIRY:
+		if (REQ_STATUS_SUCCESS != preq->Scsi_Status)
+			break;
+
+		scmd = (struct scsi_cmnd *) preq->Org_Req;
+		sg   = (struct scatterlist *) mv_rq_bf(scmd);
+
+		/* ignored  */
+		break;
+
+		buf = map_sg_page(sg) + sg->offset;
+		MV_DBG(DMSG_SCSI, BLUE("-- inquiry dump starts -- ")"\n");
+		__dump_hex((unsigned char *)buf, mv_rq_bf_l(scmd));
+		MV_DBG(DMSG_SCSI, "-- inquiry dump ends -- \n");
+		kunmap_atomic(buf, KM_IRQ0);
+		break;
+	default:
+		lba = 0;
+		break;
+	}
+#if 0
+	MV_DBG(DMSG_FREQ,
+	       "(%u) req "RED("%p")
+	       " dev %d : cmd %2X : lba %lu - %lu : length %d.\n",
+	       id, preq, preq->Device_Id, preq->Cdb[0], lba,
+	       lba + preq->Data_Transfer_Length/512,
+	       preq->Data_Transfer_Length);
+#endif
+#endif
+
+}
+
+static void generate_sg_table(struct hba_extension *phba,
+			      struct scsi_cmnd *scmd,
+			      PMV_SG_Table sg_table)
+{
+	struct scatterlist *sg;
+	unsigned int sg_count = 0;
+	unsigned int length;
+	BUS_ADDRESS busaddr = 0;
+	int i;
+
+	//MV_DBG(DMSG_FREQ,"%s : in %s.\n", mv_product_name, __FUNCTION__);
+
+	if (mv_rq_bf_l(scmd) > (mv_scmd_host(scmd)->max_sectors << 9)) {
+		MV_DBG(DMSG_SCSI, "ERROR: request length exceeds "
+		       "the maximum alowed value.\n");
+	}
+
+	if (0 == mv_rq_bf_l(scmd))
+		return ;
+
+	if (mv_use_sg(scmd)) {
+		MV_DBG(DMSG_SCSI_FREQ, "%s : map %d sg entries.\n",
+		       mv_product_name, mv_use_sg(scmd));
+
+
+		sg = (struct scatterlist *) mv_rq_bf(scmd);
+		if (MV_SCp(scmd)->mapped == 0){
+			#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 23)
+			sg_count = pci_map_sg(phba->dev,
+					      sg,
+					      mv_use_sg(scmd),
+						  scsi_to_pci_dma_dir(scmd->sc_data_direction));
+			#else
+				sg_count = scsi_dma_map(scmd);
+			#endif
+			if (sg_count != mv_use_sg(scmd)) {
+				MV_PRINT("WARNING sg_count(%d) != scmd->use_sg(%d)\n",
+					 (unsigned int) sg_count, mv_use_sg(scmd));
+			}
+			MV_SCp(scmd)->mapped = 1;
+		}
+		for (i = 0; i < sg_count; i++) {
+			busaddr = sg_dma_address(&sg[i]);
+			length = sg_dma_len(&sg[i]);
+	#ifdef MV_DEBUG
+			if(length > 4096)
+				MV_DPRINT(("Warning: sg[%i] length %d > 4096\n", i, length));
+	#endif
+			sgdt_append_pctx(sg_table,
+					 LO_BUSADDR(busaddr),
+					 HI_BUSADDR(busaddr),
+					 length,
+					 sg + i);
+		}
+	} else {
+		MV_DBG(DMSG_SCSI_FREQ, "map kernel addr into bus addr.\n" );
+		if (MV_SCp(scmd)->mapped == 0) {
+			busaddr = dma_map_single(&phba->dev->dev,
+				      mv_rq_bf(scmd),
+				       mv_rq_bf_l(scmd),
+					   scsi_to_pci_dma_dir(scmd->sc_data_direction));
+			MV_SCp(scmd)->bus_address = busaddr;
+			MV_SCp(scmd)->mapped = 1;
+
+		}
+	#ifdef MV_DEBUG
+			if(mv_rq_bf_l(scmd) > 4096)
+				MV_DPRINT(("Warning: single sg request_bufflen %d > 4096\n", mv_rq_bf_l(scmd)));
+	#endif
+
+		sgdt_append_vp(sg_table,
+			       mv_rq_bf(scmd),
+			       mv_rq_bf_l(scmd),
+			       LO_BUSADDR(busaddr),
+			       HI_BUSADDR(busaddr));
+
+	}
+}
+
+void mv_complete_request(struct hba_extension *phba,
+				struct scsi_cmnd *scmd,
+				PMV_Request pReq)
+{
+	PMV_Sense_Data  senseBuffer = (PMV_Sense_Data) scmd->sense_buffer;
+
+	//MV_POKE();
+	__hba_dump_req_info(phba->desc->module_id, pReq);
+
+	if (mv_rq_bf_l(scmd)) {
+		MV_DBG(DMSG_SCSI_FREQ, "unmap %d bytes.\n",
+		        mv_rq_bf_l(scmd));
+
+	if (MV_SCp(scmd)->mapped) {
+			if (mv_use_sg(scmd)) {
+				#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 23)
+				pci_unmap_sg(phba->dev,
+					     mv_rq_bf(scmd),
+					     mv_use_sg(scmd),
+						scsi_to_pci_dma_dir(scmd->sc_data_direction));
+				#else
+				scsi_dma_unmap(scmd);
+				#endif
+			} else {
+				dma_unmap_single(&(phba->dev->dev),
+						 MV_SCp(scmd)->bus_address,
+						 mv_rq_bf_l(scmd),
+					scsi_to_pci_dma_dir(scmd->sc_data_direction));
+			}
+		}
+	}
+
+	MV_DBG(DMSG_SCSI_FREQ,
+	       " pReq->Scsi_Status = %x pcmd = %p.\n",
+	        pReq->Scsi_Status, scmd);
+
+#if 0//def MV_DEBUG
+	if((pReq->Scsi_Status != REQ_STATUS_SUCCESS) && (SCSI_IS_READ(pReq->Cdb[0]) || SCSI_IS_WRITE(pReq->Cdb[0])))
+		MV_DPRINT(( "Device %d pReq->Scsi_Status = %x cmd = 0x%x.\n", pReq->Device_Id, pReq->Scsi_Status, pReq->Cdb[0]));
+#endif
+
+	switch (pReq->Scsi_Status) {
+	case REQ_STATUS_SUCCESS:
+		scmd->result = 0x00;
+		break;
+	case REQ_STATUS_MEDIA_ERROR:
+		scmd->result = (DID_BAD_TARGET << 16);
+		break;
+	case REQ_STATUS_BUSY:
+		scmd->result = (DID_BUS_BUSY << 16);
+		break;
+	case REQ_STATUS_NO_DEVICE:
+		scmd->result = (DID_NO_CONNECT << 16);
+		break;
+	case REQ_STATUS_HAS_SENSE:
+		/* Sense buffer data is valid already. */
+		scmd->result  = (DRIVER_SENSE << 24) | (DID_OK << 16) | CONDITION_GOOD;
+		senseBuffer->Valid = 1;
+
+		MV_DBG(DMSG_SCSI, "MV Sense: response %x SK %s length %x "
+		       "ASC %x ASCQ %x.\n", ((MV_PU8) senseBuffer)[0],
+		       MV_DumpSenseKey(((MV_PU8) senseBuffer)[2]),
+		       ((MV_PU8) senseBuffer)[7],
+		       ((MV_PU8) senseBuffer)[12],
+		       ((MV_PU8) senseBuffer)[13]);
+		break;
+	default:
+		scmd->result = (DRIVER_INVALID | DID_ABORT) << 16;
+		break;
+	}
+
+	scmd->scsi_done(scmd);
+}
+
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+static inline int is_ata_16_passthrough_for_pm(struct scsi_cmnd * scmd)
+{
+	switch(scmd->cmnd[0]){
+		case ATA_16:
+			switch(scmd->cmnd[14]){
+			/* exception*/
+			#ifndef SUPPORT_ATA_SECURITY_CMD
+				case ATA_CMD_IDENTIFY_ATA:
+					return 1;
+			#endif
+				case ATA_CMD_DEV_RESET:
+				case ATA_CMD_STANDBYNOW1:
+				case ATA_CMD_IDLEIMMEDIATE:
+				case ATA_CMD_STANDBY:
+				case ATA_CMD_IDLE :
+				case ATA_CMD_CHK_POWER:
+				case ATA_CMD_SLEEP:
+				case ATA_CMD_SMART:
+					if(scmd->sc_data_direction == DMA_NONE &&
+						scmd->cmnd[4] != 0xd4)
+						return 1;
+					else
+						return 0;
+				default:
+					return 0;
+			}
+		default:
+			break;
+	}
+	return 0;
+}
+
+static void mv_pm_ata_16_complete_request(struct hba_extension * phba,
+		struct scsi_cmnd *scmd,PMV_Request pReq)
+{
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId; /*, temp;*/
+	PCore_Driver_Extension pCore;
+
+	unsigned char *sb = scmd->sense_buffer;
+	unsigned char *desc = sb + 8;
+
+	pCore = (PCore_Driver_Extension)HBA_GetModuleExtension(phba, MODULE_CORE);
+
+	if ( pReq->Device_Id != VIRTUAL_DEVICE_ID )
+	{
+		portId = PATA_MapPortId(pReq->Device_Id);
+		deviceId = PATA_MapDeviceId(pReq->Device_Id);
+		if ( portId < MAX_PORT_NUMBER )
+			pDevice = &pCore->Ports[portId].Device[deviceId];
+	}
+
+/* !DMA_NONE */
+	if (MV_SCp(scmd)->mapped) {
+		if (mv_use_sg(scmd)){
+
+			#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 23)
+			pci_unmap_sg(phba->dev,mv_rq_bf(scmd),mv_use_sg(scmd),
+						scsi_to_pci_dma_dir(scmd->sc_data_direction));
+			#else
+			scsi_dma_unmap(scmd);
+			#endif
+		}
+		else
+			dma_unmap_single(&(phba->dev->dev),MV_SCp(scmd)->bus_address,
+						mv_rq_bf_l(scmd),scsi_to_pci_dma_dir(scmd->sc_data_direction));
+	}
+
+
+	memset(sb,0,SCSI_SENSE_BUFFERSIZE);
+	scmd->result = (DRIVER_SENSE << 24) | SAM_STAT_CHECK_CONDITION;
+
+	switch(scmd->cmnd[14]) {
+		case ATA_CMD_CHK_POWER:
+			if(!(pDevice->Status & DEVICE_STATUS_SLEEP))
+				desc[5] = 0xff;
+			if(pDevice->Status & DEVICE_STATUS_STANDBY)
+				desc[5] = 0x0;
+			if((pDevice->Status & DEVICE_STATUS_IDLE))
+				desc[5] = 0xff;
+			break;
+		case ATA_CMD_STANDBYNOW1:
+			pDevice->Status |= DEVICE_STATUS_STANDBY;
+			break;
+		case ATA_CMD_IDLEIMMEDIATE:
+			pDevice->Status |= DEVICE_STATUS_IDLE;
+			break;
+		case ATA_CMD_SLEEP:
+			pDevice->Status |= DEVICE_STATUS_SLEEP;
+			break;
+
+		default:
+			break;
+	}
+
+	/* Sense Data is current and format is descriptor */
+	sb[0] = 0x72;
+	desc[0] = 0x09;
+
+	/* set length of additional sense data */
+	sb[7] = 14; // 8+14
+
+	desc[1] = 12;
+	desc[2] = 0x00;
+	desc[3] = 0; // error register
+
+	desc[13] = 0x50; // status register
+
+	if(scmd->cmnd[14] == ATA_CMD_SMART){
+		desc[9] = scmd->cmnd[10];
+		desc[10] = scmd->cmnd[11];
+		desc[11] = scmd->cmnd[12];
+	} else {
+		desc[12] = scmd->cmnd[13]; //NO_DATA
+	}
+
+	scmd->scsi_done(scmd);
+}
+#ifdef SUPPORT_ATA_SECURITY_CMD
+ void mv_ata_16_complete_request(struct hba_extension * phba,
+		struct scsi_cmnd *scmd,PMV_Request pReq)
+{
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId; /*, temp;*/
+	PCore_Driver_Extension pCore;
+	PDomain_Port pPort = NULL;
+	MV_LPVOID port_mmio = NULL;
+	MV_U32 tf_fis = 0,sig_fis = 0;
+
+	unsigned char *sb = scmd->sense_buffer;
+	unsigned char *desc = sb + 8;
+
+	pCore = (PCore_Driver_Extension)HBA_GetModuleExtension(phba, MODULE_CORE);
+
+	if ( pReq->Device_Id != VIRTUAL_DEVICE_ID ){
+		portId = PATA_MapPortId(pReq->Device_Id);
+		deviceId = PATA_MapDeviceId(pReq->Device_Id);
+		if ( portId < MAX_PORT_NUMBER )	{
+			pPort = &pCore->Ports[portId];
+			pDevice = &pCore->Ports[portId].Device[deviceId];
+		}
+	}
+
+	if (MV_SCp(scmd)->mapped) {
+		if (mv_use_sg(scmd)){
+	#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 23)
+			pci_unmap_sg(phba->dev,mv_rq_bf(scmd),mv_use_sg(scmd),
+					 scsi_to_pci_dma_dir(scmd->sc_data_direction));
+	#else
+			scsi_dma_unmap(scmd);
+	#endif
+		}else {
+			dma_unmap_single(&(phba->dev->dev),
+				MV_SCp(scmd)->bus_address,
+				mv_rq_bf_l(scmd),scsi_to_pci_dma_dir(scmd->sc_data_direction));
+		}
+	}
+
+	memset(sb,0,SCSI_SENSE_BUFFERSIZE);
+
+	if (pReq->Scsi_Status != REQ_STATUS_SUCCESS) {
+		scmd->result = (DRIVER_SENSE << 24) | SAM_STAT_CHECK_CONDITION;
+
+		sb[0] = 0x70;
+		sb[2] = SCSI_SK_NO_SENSE;
+		sb[7] = 0;
+		sb[12] = SCSI_ASC_NO_ASC;
+		sb[13] = 0x1D  /*SCSI_ASCQ_ATA_PASSTHRU_INFO*/;
+
+		switch(pReq->Scsi_Status) {
+		case REQ_STATUS_ABORT:
+			sb[1]=SCSI_SK_ABORTED_COMMAND;
+			desc[3]=0x04;
+			desc[13]=0x51;
+			break;
+		default:
+			break;
+		}
+	} else if (((scmd->cmnd[2] >> 5) & 0x01)
+		&& pReq->Scsi_Status == REQ_STATUS_SUCCESS) {
+		/* if chk_cond is set to 1, then always return check condition */
+		scmd->result = (DRIVER_SENSE << 24) | SAM_STAT_CHECK_CONDITION;
+
+		/* Sense Data is current and format is descriptor */
+		sb[0] = 0x72;
+		sb[1]=0x0;
+		/* set length of additional sense data */
+		sb[7] = 14; // 8+14
+
+		desc[0] = 0x09;
+		desc[1] = 12;
+		desc[2] = 0x00;
+		desc[3] = 0; 		// error register
+		desc[13] = 0x04; 	// status register
+		desc[12] = scmd->cmnd[13]; //device
+
+		switch(scmd->cmnd[14]) {
+		case ATA_CMD_SMART:
+			desc[9] = scmd->cmnd[10];
+			desc[10] = scmd->cmnd[11];
+			desc[11] = scmd->cmnd[12];
+			break;
+		case ATA_CMD_SEC_UNLOCK:
+		case ATA_CMD_SEC_ERASE_UNIT:
+			if(pDevice->Capacity & DEVICE_CAPACITY_SECURITY_SUPPORTED){
+				if(pDevice->Setting & DEVICE_SETTING_SECURITY_LOCKED){
+					MV_DPRINT(("securiy: unlocked\n"));
+					pDevice->Setting &= ~DEVICE_SETTING_SECURITY_LOCKED;
+				}
+			}
+			break;
+		case ATA_CMD_IDENTIFY_ATA:
+			sb[3]=0x1d;
+			break;
+		case ATA_CMD_SEC_PASSWORD:
+		case ATA_CMD_SEC_DISABLE_PASSWORD:
+		case ATA_CMD_SEC_ERASE_PRE:
+			break;
+		case ATA_CMD_DOWNLOAD_MICROCODE:
+		{
+			if(pPort != NULL){
+				port_mmio = pPort->Mmio_Base;
+				sig_fis = MV_REG_READ_DWORD(port_mmio, PORT_SIG);
+				tf_fis  =  MV_REG_READ_DWORD(port_mmio, PORT_TFDATA);
+				tf_fis &= 0xFFFF;
+
+				desc[13]= (MV_U8)(tf_fis & 0xFF);//status
+				desc[3] = (MV_U8)(tf_fis >> 8);//error
+
+				desc[5] = (MV_U8)(sig_fis & 0xFF); //sector count
+				desc[7] = (MV_U8)((sig_fis >> 8) & 0xFF);//lba low
+				desc[9] = (MV_U8)((sig_fis >> 16) & 0xFF);//lba mid
+				desc[11] = (MV_U8)((sig_fis >> 24) & 0xFF);//lba high
+			}else{
+				desc[5] = 0;//scmd->cmnd[6];
+				desc[7] = scmd->cmnd[8];
+				desc[9] = scmd->cmnd[10];
+				desc[11] = scmd->cmnd[12];
+
+				if(pReq->Cmd_Flag & CMD_FLAG_48BIT){
+					desc[4] = scmd->cmnd[5];
+					desc[6] = scmd->cmnd[7];
+					desc[8] = scmd->cmnd[9];
+					desc[10] = scmd->cmnd[11];
+				}else{
+					desc[4] = 0;
+					desc[6] = 0;
+					desc[8] = 0;
+					desc[10] = 0;
+				}
+			}
+		}
+			break;
+		default:
+			break;
+		}
+
+	}
+
+	scmd->scsi_done(scmd);
+}
+
+#endif
+void mv_clear_device_status(struct hba_extension * phba,PMV_Request pReq)
+{
+	PDomain_Device pDevice = NULL;
+	MV_U8 portId, deviceId; /*, temp;*/
+	PCore_Driver_Extension pCore;
+
+	pCore = (PCore_Driver_Extension)HBA_GetModuleExtension(phba, MODULE_CORE);
+
+	if ( pReq->Device_Id != VIRTUAL_DEVICE_ID )
+	{
+		portId = PATA_MapPortId(pReq->Device_Id);
+		deviceId = PATA_MapDeviceId(pReq->Device_Id);
+		if ( portId < MAX_PORT_NUMBER )
+			pDevice = &pCore->Ports[portId].Device[deviceId];
+		pDevice->Status &= ~(DEVICE_STATUS_IDLE|DEVICE_STATUS_STANDBY);
+	}
+	return ;
+}
+
+#endif
+
+/* This should be the _only_ os request exit point. */
+static void hba_req_callback(MV_PVOID This, PMV_Request pReq)
+{
+	struct hba_extension *phba = (struct hba_extension *)This;
+	struct scsi_cmnd *scmd = (struct scsi_cmnd *)pReq->Org_Req;
+
+	/* Return this request to OS. */
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+	if(is_ata_16_passthrough_for_pm(scmd) &&
+		pReq->Scsi_Status == REQ_STATUS_SUCCESS)
+			mv_pm_ata_16_complete_request(phba,scmd,pReq);
+#endif
+#ifdef SUPPORT_ATA_SECURITY_CMD
+	else if(scmd->cmnd[0]== ATA_16){
+		mv_ata_16_complete_request(phba,scmd,pReq);
+	}
+#endif
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+	else{
+		mv_clear_device_status(phba,pReq);
+#endif
+		mv_complete_request(phba, scmd, pReq);
+#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+	}
+#endif
+	phba->Io_Count--;
+	res_free_req_to_pool(phba->req_pool, pReq);
+}
+
+
+static int scsi_cmd_to_req_conv(struct hba_extension *phba,
+				struct scsi_cmnd *scmd,
+				PMV_Request pReq)
+{
+	/*
+	 * Set three flags: CMD_FLAG_NON_DATA
+	 *                  CMD_FLAG_DATA_IN
+	 *                  CMD_FLAG_DMA
+	 * currently data in/out all go thru DMA
+	 */
+	pReq->Cmd_Flag = 0;
+	switch (scmd->sc_data_direction) {
+	case DMA_NONE:
+		pReq->Cmd_Flag = CMD_FLAG_NON_DATA;
+		break;
+	case DMA_FROM_DEVICE:
+		pReq->Cmd_Flag = CMD_FLAG_DMA|CMD_FLAG_DATA_IN;
+		break;
+	case DMA_TO_DEVICE:
+		pReq->Cmd_Flag = CMD_FLAG_DMA|CMD_FLAG_DATA_OUT;
+		break;
+	case DMA_BIDIRECTIONAL :
+		MV_DBG(DMSG_SCSI, "unexpected DMA_BIDIRECTIONAL.\n");
+		break;
+	default:
+		break;
+	}
+
+	/* handling of specific cmds */
+	memset(pReq->Cdb, 0, MAX_CDB_SIZE);
+	switch (scmd->cmnd[0]){
+#if 0
+	case MODE_SELECT:
+		pReq->Cdb[0] = MODE_SELECT_10;
+		pReq->Cdb[1] = scmd->cmnd[1];
+		pReq->Cdb[8] = scmd->cmnd[4];
+		break;
+#endif
+	case READ_6:
+		pReq->Cdb[0] = READ_10;
+		pReq->Cdb[3] = scmd->cmnd[1]&0x1f;
+		pReq->Cdb[4] = scmd->cmnd[2];
+		pReq->Cdb[5] = scmd->cmnd[3];
+		pReq->Cdb[8] = scmd->cmnd[4];
+		pReq->Cdb[9] = scmd->cmnd[5];
+		break;
+
+	case WRITE_6:
+		pReq->Cdb[0] = WRITE_10;
+		pReq->Cdb[3] = scmd->cmnd[1]&0x1f;
+		pReq->Cdb[4] = scmd->cmnd[2];
+		pReq->Cdb[5] = scmd->cmnd[3];
+		pReq->Cdb[8] = scmd->cmnd[4];
+		pReq->Cdb[9] = scmd->cmnd[5];
+		break;
+	/*0xA1==SCSI_CMD_BLANK== ATA_12 */
+	case 0xA1:
+		if(!(IS_ATA_12_CMD(scmd))){
+			memcpy(pReq->Cdb, scmd->cmnd, MAX_CDB_SIZE);
+			break;
+		}
+	case ATA_16:
+		pReq->Cmd_Flag = hba_parse_ata_protocol(scmd);
+		if(IS_ATA_12_CMD(scmd))
+			pReq->Cmd_Flag |= CMD_FLAG_ATA_12;
+		else
+			pReq->Cmd_Flag |= CMD_FLAG_ATA_16;
+
+		pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+		pReq->Cdb[1] = CDB_CORE_MODULE;
+
+		memcpy(&pReq->Cdb[3],&scmd->cmnd[3],13);
+
+		switch(scmd->cmnd[ (pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?9 : 14])
+		{
+		#ifdef SUPPORT_ATA_POWER_MANAGEMENT
+			case 0x08:
+				pReq->Cdb[2] = CDB_CORE_RESET_DEVICE;
+				break;
+			case 0xE0:
+				pReq->Cdb[2] = CDB_CORE_ATA_STANDBY_IMMEDIATE;
+				break;
+			case 0xE1:
+				pReq->Cdb[2] = CDB_CORE_ATA_IDLE_IMMEDIATE;
+				break;
+			case 0xE2:
+				pReq->Cdb[2] = CDB_CORE_ATA_STANDBY;
+				break;
+			case 0xE3:
+				pReq->Cdb[2] = CDB_CORE_ATA_IDLE;
+				break;
+			case 0xE5:
+				pReq->Cdb[2] = CDB_CORE_ATA_CHECK_POWER_MODE;
+				break;
+			case 0xE6:
+				pReq->Cdb[2] = CDB_CORE_ATA_SLEEP;
+				break;
+		#endif
+			case 0xEC:
+				pReq->Cdb[2] = CDB_CORE_ATA_IDENTIFY;
+				break;
+			case ATA_CMD_DOWNLOAD_MICROCODE:
+				pReq->Cdb[2] = CDB_CORE_ATA_DOWNLOAD_MICROCODE;
+				if(scmd->cmnd[1]&0x1)
+					pReq->Cmd_Flag |= CMD_FLAG_48BIT;
+				break;
+		#ifdef SUPPORT_ATA_SMART
+			case ATA_IDENTIFY_PACKET_DEVICE:
+				pReq->Cdb[2] = CDB_CORE_ATA_IDENTIFY_PACKET_DEVICE;
+				break;
+			case ATA_SMART_CMD :
+				switch(scmd->cmnd[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?3 : 4])
+				{
+					case ATA_SMART_ENABLE:
+						pReq->Cdb[2] = CDB_CORE_ENABLE_SMART;
+						break;
+					case ATA_SMART_DISABLE:
+						pReq->Cdb[2] = CDB_CORE_DISABLE_SMART;
+						break;
+					case ATA_SMART_STATUS:
+						pReq->Cdb[2] = CDB_CORE_SMART_RETURN_STATUS;
+						break;
+					case ATA_SMART_READ_VALUES:
+						pReq->Cdb[2] = CDB_CORE_ATA_SMART_READ_VALUES;
+						break;
+					case ATA_SMART_READ_THRESHOLDS:
+						pReq->Cdb[2] = CDB_CORE_ATA_SMART_READ_THRESHOLDS;
+						break;
+					case  ATA_SMART_READ_LOG_SECTOR:
+						pReq->Cdb[2] = CDB_CORE_ATA_SMART_READ_LOG_SECTOR;
+						break;
+					case ATA_SMART_WRITE_LOG_SECTOR:
+						pReq->Cdb[2] = CDB_CORE_ATA_SMART_WRITE_LOG_SECTOR;
+						break;
+					case  ATA_SMART_AUTO_OFFLINE:
+						pReq->Cdb[2] = CDB_CORE_ATA_SMART_AUTO_OFFLINE;
+						break;
+					case  ATA_SMART_AUTOSAVE:
+						pReq->Cdb[2] = CDB_CORE_ATA_SMART_AUTOSAVE;
+						break;
+					case  ATA_SMART_IMMEDIATE_OFFLINE:
+						pReq->Cdb[2] = CDB_CORE_ATA_SMART_IMMEDIATE_OFFLINE;
+						break;
+					default:
+						pReq->Scsi_Status   =REQ_STATUS_INVALID_PARAMETER;
+						MV_PRINT("Unsupported ATA-12 or 16  subcommand =0x%x\n", \
+						scmd->cmnd[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?3 : 4]);
+						return -1;
+				}
+				break;
+			case 0xef:
+				switch(scmd->cmnd[4]){
+					case 0x2:
+						pReq->Cdb[2]=CDB_CORE_ENABLE_WRITE_CACHE ;
+					break;
+					case 0x82:
+						pReq->Cdb[2]=CDB_CORE_DISABLE_WRITE_CACHE;
+					break;
+					default:
+					pReq->Scsi_Status = REQ_STATUS_INVALID_PARAMETER;
+					MV_PRINT("Enable/Disable Write Cache command error: parameter error:%d\n",scmd->cmnd[4]);
+					return -1;
+				}
+			break;
+		#endif /*#ifdef SUPPORT_ATA_SMART*/
+		#ifdef SUPPORT_ATA_SECURITY_CMD
+			case ATA_CMD_SEC_PASSWORD:
+			case ATA_CMD_SEC_UNLOCK:
+					if(0){
+						int i;
+						char *buf=NULL;
+						for(i=0;i<16;i++)
+							printk(" %d:%x ",i,scmd->cmnd[i]);
+						printk("\n");
+						buf=NULL;
+						buf=bio_data(scmd->request->bio);
+						if(buf){
+							printk("bio=");
+							for(i=0;i<16;i++)
+							printk(" %x ",buf[i]);
+							printk("\n");
+							} else
+							printk("bio is NULL\n");
+					}
+			case ATA_CMD_SEC_ERASE_PRE:
+			case ATA_CMD_SEC_ERASE_UNIT:
+			case ATA_CMD_SEC_FREEZE_LOCK:
+			case ATA_CMD_SEC_DISABLE_PASSWORD:
+				pReq->Cmd_Flag &= ~CMD_FLAG_ATA_16;
+				memcpy(&pReq->Cdb[0],&scmd->cmnd[0],16);
+				break;
+			#endif
+			default:
+				pReq->Scsi_Status = REQ_STATUS_INVALID_PARAMETER;
+				MV_PRINT("Unsupported ATA-12 or 16 Command=0x%x\n",\
+				scmd->cmnd[(pReq->Cmd_Flag & CMD_FLAG_ATA_12) ?9 : 14]);
+				return -1;
+		}
+		break;
+	default:
+		memcpy(pReq->Cdb, scmd->cmnd, MAX_CDB_SIZE);
+		break;
+	}
+
+	pReq->Data_Buffer = mv_rq_bf(scmd);
+	pReq->Data_Transfer_Length = mv_rq_bf_l(scmd);
+	pReq->Sense_Info_Buffer = scmd->sense_buffer;
+	pReq->Sense_Info_Buffer_Length = SCSI_SENSE_BUFFERSIZE;
+
+	SGTable_Init(&pReq->SG_Table, 0);
+	generate_sg_table(phba, scmd, &pReq->SG_Table);
+
+	pReq->LBA.value = 0;
+	pReq->Sector_Count = 0;
+	MV_SetLBAandSectorCount(pReq);
+
+	pReq->Req_Type      = REQ_TYPE_OS;
+	pReq->Org_Req       = scmd;
+	pReq->Tag           = scmd->tag;
+	pReq->Scsi_Status   = REQ_STATUS_PENDING;
+	pReq->Completion    = hba_req_callback;
+	pReq->Cmd_Initiator = phba;
+	//pReq->Scsi_Status   = REQ_STATUS_INVALID_REQUEST;
+	pReq->Device_Id     = __MAKE_DEV_ID(mv_scmd_target(scmd),
+					    mv_scmd_lun(scmd));
+
+	return 0;
+}
+static void hba_shutdown_req_cb(MV_PVOID this, PMV_Request req)
+{
+	struct hba_extension *phba = (struct hba_extension *) this;
+	#ifdef SUPPORT_REQUEST_TIMER
+	if(req!=NULL)
+	{
+		MV_DBG(DMSG_HBA,"Shutdown HBA timer!\n");
+		hba_remove_timer_sync(req);
+	}
+	#endif
+	res_free_req_to_pool(phba->req_pool, req);
+	phba->Io_Count--;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_set(&phba->hba_sync, 0);
+#else
+	complete(&phba->cmpl);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+/* will wait for atomic value atomic to become zero until timed out */
+/* return how much 'timeout' is left or 0 if already timed out */
+int __hba_wait_for_atomic_timeout(atomic_t *atomic, unsigned long timeout)
+{
+	unsigned intv = HZ/20;
+
+	while (timeout) {
+		if (0 == atomic_read(atomic))
+			break;
+
+		if (timeout < intv)
+			intv = timeout;
+		set_current_state(TASK_INTERRUPTIBLE);
+		timeout -= (intv - schedule_timeout(intv));
+	}
+	return timeout;
+}
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+
+#ifdef CACHE_MODULE_SUPPORT
+static void _hba_send_shutdown_req(PHBA_Extension phba)
+{
+	unsigned long flags;
+	PMV_Request pReq;
+
+	/*Send MV_REQUEST to do something.*/
+	pReq = res_get_req_from_pool(phba->req_pool);
+	if (NULL == pReq) {
+		MV_PRINT("cannot allocate memory for req.\n"
+		       );
+		return;
+	}
+
+	pReq->Cmd_Initiator = phba;
+	pReq->Org_Req = NULL;
+	pReq->Completion = hba_shutdown_req_cb;
+	pReq->Req_Type = REQ_TYPE_OS;
+	pReq->Cmd_Flag = 0;
+	pReq->Cmd_Flag |= CMD_FLAG_NON_DATA;
+	pReq->Sense_Info_Buffer_Length = 0;
+	pReq->Data_Transfer_Length = 0;
+	pReq->Data_Buffer = NULL;
+	pReq->Sense_Info_Buffer = NULL;
+
+	pReq->LBA.value = 0;
+	pReq->Sector_Count = 0;
+
+	pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+
+	SGTable_Init(&pReq->SG_Table, 0);
+	memset(pReq->Context, 0, sizeof(MV_PVOID) * MAX_POSSIBLE_MODULE_NUMBER);
+
+	MV_DPRINT(("send SHUTDOWN request to CACHE.\n"));
+	pReq->Cdb[0] = APICDB0_ADAPTER;
+	pReq->Cdb[1] = APICDB1_ADAPTER_POWER_STATE_CHANGE;
+	pReq->Cdb[2] = 0;
+
+	spin_lock_irqsave(&phba->desc->hba_desc->global_lock, flags);
+	phba->Io_Count++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_set(&phba->hba_sync, 1);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+	phba->desc->ops->module_sendrequest(phba->desc->extension, pReq);
+	spin_unlock_irqrestore(&phba->desc->hba_desc->global_lock, flags);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	__hba_wait_for_atomic_timeout(&phba->hba_sync, 10*HZ);
+#else
+	wait_for_completion_timeout(&phba->cmpl, 10*HZ);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+}
+#endif /* CACHE_MODULE_SUPPORT */
+
+void hba_send_shutdown_req(struct hba_extension *phba)
+{
+	unsigned long flags;
+	PMV_Request pReq;
+#ifdef CACHE_MODULE_SUPPORT
+	_hba_send_shutdown_req(phba);
+#endif
+
+	pReq = res_get_req_from_pool(phba->req_pool);
+	if (NULL == pReq) {
+		MV_PRINT("cannot allocate memory for req.\n");
+		return;
+	}
+
+	pReq->Cmd_Initiator = phba;
+	pReq->Org_Req = pReq;
+	pReq->Req_Type = REQ_TYPE_INTERNAL;
+	pReq->Scsi_Status = REQ_STATUS_INVALID_REQUEST;
+	pReq->Completion = hba_shutdown_req_cb;
+
+#ifdef RAID_DRIVER
+	pReq->Cdb[0] = APICDB0_LD;
+	pReq->Cdb[1] = APICDB1_LD_SHUTDOWN;
+#else
+	pReq->Device_Id = 0;
+	pReq->Cmd_Flag = 0;
+	pReq->Cmd_Flag |= CMD_FLAG_NON_DATA;
+	pReq->Sense_Info_Buffer_Length = 0;
+	pReq->Data_Transfer_Length = 0;
+	pReq->Data_Buffer = NULL;
+	pReq->Sense_Info_Buffer = NULL;
+	SGTable_Init(&pReq->SG_Table, 0);
+	pReq->Cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;
+	pReq->Cdb[1] = CDB_CORE_MODULE;
+	pReq->Cdb[2] = CDB_CORE_SHUTDOWN;
+	pReq->LBA.value = 0;
+	pReq->Sector_Count = 0;
+	pReq->Scsi_Status = REQ_STATUS_PENDING;
+#endif
+
+	spin_lock_irqsave(&phba->desc->hba_desc->global_lock, flags);
+	phba->Io_Count++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_set(&phba->hba_sync, 1);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+	phba->desc->ops->module_sendrequest(phba->desc->extension, pReq);
+	spin_unlock_irqrestore(&phba->desc->hba_desc->global_lock, flags);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	__hba_wait_for_atomic_timeout(&phba->hba_sync, 10*HZ);
+#else
+	wait_for_completion_timeout(&phba->cmpl, 10*HZ);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 7)
+/*openSUSE 11.1 SLES 11 SLED 11*/
+#if ((LINUX_VERSION_CODE == KERNEL_VERSION(2, 6, 27))&&(IS_OPENSUSE_SLED_SLES))
+static enum blk_eh_timer_return mv_linux_timed_out(struct scsi_cmnd *cmd)
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 28)
+static enum scsi_eh_timer_return mv_linux_timed_out(struct scsi_cmnd *cmd)
+#else
+static enum blk_eh_timer_return mv_linux_timed_out(struct scsi_cmnd *cmd)
+#endif
+{
+	MV_BOOLEAN ret = MV_TRUE;
+#if ((LINUX_VERSION_CODE == KERNEL_VERSION(2, 6, 27))&&(IS_OPENSUSE_SLED_SLES))
+	return (ret)?BLK_EH_RESET_TIMER:BLK_EH_NOT_HANDLED;
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+	return (ret)?EH_RESET_TIMER:EH_NOT_HANDLED;
+#else
+	return (ret)?BLK_EH_RESET_TIMER:BLK_EH_NOT_HANDLED;
+#endif
+}
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 7) */
+
+struct mv_mod_desc *
+__get_lowest_module(struct mv_adp_desc *hba_desc)
+{
+	struct mv_mod_desc *p;
+
+	/* take a random module, and trace through its child */
+	p = list_entry(hba_desc->online_module_list.next,
+		       struct mv_mod_desc,
+		       mod_entry);
+
+	WARN_ON(NULL == p);
+	while (p) {
+		if (NULL == p->child)
+			break;
+		p = p->child;
+	}
+	return p;
+}
+
+static int mv_linux_queue_command(struct scsi_cmnd *scmd,
+				  void (*done) (struct scsi_cmnd *))
+{
+	struct Scsi_Host *host = mv_scmd_host(scmd);
+	struct hba_extension *hba = *((struct hba_extension * *) host->hostdata);
+	PMV_Request req;
+	unsigned long flags;
+
+	if (done == NULL) {
+		MV_PRINT( ": in queuecommand, done function can't be NULL\n");
+		return 0;
+	}
+
+
+#if 1
+	MV_DBG(DMSG_SCSI_FREQ,
+	       "mv_linux_queue_command %p (%d/%d/%d/%d \
+	       Cdb=(%x-%x-%x-%x-%x-%x-%x-%x-%x-%x-%x-%x-%x-%x-%x-%x))\n",
+	       scmd, host->host_no, mv_scmd_channel(scmd),
+	       mv_scmd_target(scmd), mv_scmd_lun(scmd),
+	       *(scmd->cmnd), *(scmd->cmnd+1),
+	       *(scmd->cmnd+2), *(scmd->cmnd+3),
+	        *(scmd->cmnd+4), *(scmd->cmnd+5),
+	        *(scmd->cmnd+6), *(scmd->cmnd+7),
+	        *(scmd->cmnd+8), *(scmd->cmnd+9),
+	        *(scmd->cmnd+10), *(scmd->cmnd+11),
+	        *(scmd->cmnd+12), *(scmd->cmnd+13),
+	        *(scmd->cmnd+14),*(scmd->cmnd+15));
+#endif
+	scmd->result = 0;
+	scmd->scsi_done = done;
+	MV_SCp(scmd)->bus_address = 0;
+	MV_SCp(scmd)->mapped = 0;
+	MV_SCp(scmd)->map_atomic = 0;
+
+#ifdef COMMAND_ISSUE_WORKROUND
+{
+	MV_U8 mv_core_check_is_reseeting(MV_PVOID core_ext);
+	struct mv_mod_desc *core_desc=__get_lowest_module(hba->desc->hba_desc);
+	if(mv_core_check_is_reseeting(core_desc->extension)){
+		//MV_DPRINT(("HOST is resetting, wait..\n"));
+		return SCSI_MLQUEUE_HOST_BUSY;
+	}
+
+}
+#endif
+	if (mv_scmd_channel(scmd)) {
+		scmd->result = DID_BAD_TARGET << 16;
+		goto done;
+	}
+
+	/*
+	 * Get mv_request resource and translate the scsi_cmnd request
+	 * to mv_request.
+	 */
+	req = res_get_req_from_pool(hba->req_pool);
+	if (req == NULL)
+	{
+		MV_DPRINT(("No sufficient request.\n"));
+		return SCSI_MLQUEUE_HOST_BUSY;
+	}
+	if (scsi_cmd_to_req_conv(hba, scmd, req)) {
+		struct scsi_cmnd *cmd = (struct scsi_cmnd *)req->Org_Req;
+		/*
+		 * Even TranslateOSRequest failed,
+		 * it still should set some of the variables to the MV_Request
+		 * especially MV_Request.Org_Req and MV_Request.Scsi_Status;
+		 */
+		if(!cmd || req->Scsi_Status==REQ_STATUS_INVALID_PARAMETER){
+			res_free_req_to_pool(hba->req_pool, req);
+			//scmd->result = (DRIVER_INVALID | SUGGEST_ABORT) << 24;
+			scmd->result |= DID_ABORT << 16;
+			scmd->scsi_done(scmd);
+			return 0;
+		}
+		MV_DBG(DMSG_HBA,
+		       "ERROR - Translation from OS Request failed.\n");
+		hba_req_callback(hba, req);
+		return 0;
+	}
+
+	spin_lock_irqsave(&hba->desc->hba_desc->global_lock, flags);
+	hba->Io_Count++;
+
+	if (hba->State != DRIVER_STATUS_STARTED) {
+		MV_ASSERT(0);
+		/*if ( hba->State==DRIVER_STATUS_IDLE )
+		  {
+		  hba->State = DRIVER_STATUS_STARTING;
+		  Module_StartAll(module_manage, MODULE_CORE);
+		  }*/
+	} else {
+		hba->desc->ops->module_sendrequest(hba->desc->extension, req);
+	}
+	spin_unlock_irqrestore(&hba->desc->hba_desc->global_lock, flags);
+
+	return 0;
+done:
+        scmd->scsi_done(scmd);
+        return 0;
+}
+
+#if 0
+static int mv_linux_abort(struct scsi_cmnd *cmd)
+{
+	struct Scsi_Host *host;
+	struct hba_extension *phba;
+	int  ret = FAILED;
+
+	MV_PRINT("__MV__ abort command %p.\n", cmd);
+
+	return ret;
+}
+#endif /* 0 */
+static void id_to_string(const u16 *iden, unsigned char *s,
+			unsigned int ofs, unsigned int len)
+{
+	unsigned int c;
+	while (len > 0) {
+		c = iden[ofs] >> 8;
+		*s = c;
+		s++;
+
+		c = iden[ofs] & 0xff;
+		*s = c;
+		s++;
+
+		ofs++;
+		len -= 2;
+	}
+}
+
+static int io_get_identity(void *kbuf)
+{
+	char buf[40];
+	u16 *dst = kbuf;
+	unsigned int i,j;
+
+	if (!dst)
+		return -1;
+
+#ifdef __MV_BIG_ENDIAN_BITFIELD__
+        for (i = 0; i < 256; i++)
+                dst[i] = MV_LE16_TO_CPU(dst[i]);
+#endif
+
+	/*identify data -> model number: word 27-46*/
+	id_to_string(dst, buf, 27, 40);
+	memcpy(&dst[27], buf, 40);
+
+	/*identify data -> firmware revision: word 23-26*/
+	id_to_string(dst, buf, 23, 8);
+	memcpy(&dst[23], buf, 8);
+
+	/*identify data -> serial number: word 10-19*/
+	id_to_string(dst, buf, 10, 20);
+	memcpy(&dst[10], buf, 20);
+
+	return 0;
+}
+
+static int mv_identify_ata_cmd(struct scsi_device *scsidev, void __user *arg)
+{
+	int rc = 0;
+	u8 scsi_cmd[MAX_COMMAND_SIZE];
+	u8 *argbuf = NULL, *sensebuf = NULL;
+	int argsize = 512;
+	enum dma_data_direction data_dir;
+	int cmd_result;
+	char ata_ident[] = {0x85, 0x08, 0x0e, 0x00, 0x00, 0x00, 0x01, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xec, 0x00};
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	struct scsi_request *sreq;
+#endif
+	if (arg == NULL)
+		return -EINVAL;
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	sensebuf = kmalloc(SCSI_SENSE_BUFFERSIZE, GFP_NOIO);
+	if (sensebuf) {
+		memset(sensebuf, 0, SCSI_SENSE_BUFFERSIZE);
+	}
+#else
+	sensebuf = kzalloc(SCSI_SENSE_BUFFERSIZE, GFP_NOIO);
+#endif
+
+	if (!sensebuf)
+		return -ENOMEM;
+
+	memcpy(scsi_cmd,ata_ident, sizeof(scsi_cmd));
+	data_dir = DMA_FROM_DEVICE;
+	argbuf = kmalloc(argsize, GFP_KERNEL);
+		if (argbuf == NULL) {
+			rc = -ENOMEM;
+			goto error;
+			}
+
+	/* Good values for timeout and retries?  Values below
+		from scsi_ioctl_send_command() for default case... */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	sreq = scsi_allocate_request(scsidev, GFP_KERNEL);
+	if (!sreq) {
+		rc= -EINTR;
+		goto free_req;
+	}
+	sreq->sr_data_direction = data_dir;
+	scsi_wait_req(sreq, scsi_cmd, argbuf, argsize, (10*HZ), 5);
+
+	/*
+	 * If there was an error condition, pass the info back to the user.
+	 */
+	cmd_result = sreq->sr_result;
+	sensebuf = sreq->sr_sense_buffer;
+
+#elif LINUX_VERSION_CODE >=KERNEL_VERSION(2, 6, 29)
+	cmd_result = scsi_execute(scsidev, scsi_cmd, data_dir, argbuf, argsize,
+                                sensebuf, (10*HZ), 5, 0,0);
+#else
+	cmd_result = scsi_execute(scsidev, scsi_cmd, data_dir, argbuf, argsize,
+                                sensebuf, (10*HZ), 5, 0);
+#endif
+
+
+	if (driver_byte(cmd_result) == DRIVER_SENSE) {/* sense data available */
+		u8 *desc = sensebuf + 8;
+		cmd_result &= ~(0xFF<<24); /* DRIVER_SENSE is not an error */
+
+		/* If we set cc then ATA pass-through will cause a
+		* check condition even if no error. Filter that. */
+		if (cmd_result & SAM_STAT_CHECK_CONDITION) {
+		struct scsi_sense_hdr sshdr;
+		scsi_normalize_sense(sensebuf, SCSI_SENSE_BUFFERSIZE,
+                                   &sshdr);
+		if (sshdr.sense_key==0 &&
+			sshdr.asc==0 && sshdr.ascq==0)
+			cmd_result &= ~SAM_STAT_CHECK_CONDITION;
+		}
+
+		/* Send userspace a few ATA registers (same as drivers/ide) */
+		if (sensebuf[0] == 0x72 &&     /* format is "descriptor" */
+			desc[0] == 0x09 ) {        /* code is "ATA Descriptor" */
+		}
+		if (cmd_result != CONDITION_GOOD) {
+			rc = -EIO;
+			goto free_req;
+		}
+	}
+	if(argbuf)
+		io_get_identity(argbuf);
+	if ((argbuf) && copy_to_user(arg, argbuf, argsize))
+		rc = -EFAULT;
+
+free_req:
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	scsi_release_request(sreq);
+#endif
+error:
+	if (sensebuf) kfree(sensebuf);
+	if (argbuf) kfree(argbuf);
+	return rc;
+}
+
+#ifdef IOCTL_TEMPLATE
+/****************************************************************
+*  Name:   mv_ial_ht_ata_cmd
+*
+*  Description:    handles mv_sata ata IOCTL special drive command (HDIO_DRIVE_CMD)
+*
+*  Parameters:     scsidev - Device to which we are issuing command
+*                  arg     - User provided data for issuing command
+*
+*  Returns:        0 on success, otherwise of failure.
+*
+****************************************************************/
+static int mv_ial_ht_ata_cmd(struct scsi_device *scsidev, void __user *arg)
+{
+	int rc = 0;
+	u8 scsi_cmd[MAX_COMMAND_SIZE];
+	u8 args[4] , *argbuf = NULL, *sensebuf = NULL;
+	int argsize = 0;
+	enum dma_data_direction data_dir;
+	int cmd_result;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	struct scsi_request *sreq;
+#endif
+	if (arg == NULL)
+		return -EINVAL;
+
+	if (copy_from_user(args, arg, sizeof(args)))
+		return -EFAULT;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	sensebuf = kmalloc(SCSI_SENSE_BUFFERSIZE, GFP_NOIO);
+	if (sensebuf) {
+		memset(sensebuf, 0, SCSI_SENSE_BUFFERSIZE);
+	}
+#else
+	sensebuf = kzalloc(SCSI_SENSE_BUFFERSIZE, GFP_NOIO);
+#endif
+
+	if (!sensebuf)
+		return -ENOMEM;
+
+	memset(scsi_cmd, 0, sizeof(scsi_cmd));
+	if (args[3]) {
+		argsize = SECTOR_SIZE * args[3];
+		argbuf = kmalloc(argsize, GFP_KERNEL);
+		if (argbuf == NULL) {
+			rc = -ENOMEM;
+			goto error;
+	}
+
+	scsi_cmd[1]  = (4 << 1); /* PIO Data-in */
+	scsi_cmd[2]  = 0x0e;     /* no off.line or cc, read from dev,
+                                             block count in sector count field */
+	data_dir = DMA_FROM_DEVICE;
+	} else {
+		scsi_cmd[1]  = (3 << 1); /* Non-data */
+		scsi_cmd[2]  = 0x20;     /* cc but no off.line or data xfer */
+		data_dir = DMA_NONE;
+	}
+
+	scsi_cmd[0] = ATA_16;
+
+	scsi_cmd[4] = args[2];
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 27)
+	if (args[0] == WIN_SMART) { /* hack -- ide driver does this too... */
+#else
+	if (args[0] == ATA_CMD_SMART) { /* hack -- ide driver does this too... */
+#endif
+		scsi_cmd[6]  = args[3];
+		scsi_cmd[8]  = args[1];
+		scsi_cmd[10] = 0x4f;
+		scsi_cmd[12] = 0xc2;
+	} else {
+		scsi_cmd[6]  = args[1];
+	}
+	scsi_cmd[14] = args[0];
+
+	/* Good values for timeout and retries?  Values below
+		from scsi_ioctl_send_command() for default case... */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	sreq = scsi_allocate_request(scsidev, GFP_KERNEL);
+	if (!sreq) {
+		rc= -EINTR;
+		goto free_req;
+	}
+	sreq->sr_data_direction = data_dir;
+	scsi_wait_req(sreq, scsi_cmd, argbuf, argsize, (10*HZ), 5);
+
+	/*
+	 * If there was an error condition, pass the info back to the user.
+	 */
+	cmd_result = sreq->sr_result;
+	sensebuf = sreq->sr_sense_buffer;
+
+#elif LINUX_VERSION_CODE >=KERNEL_VERSION(2, 6, 29)
+	cmd_result = scsi_execute(scsidev, scsi_cmd, data_dir, argbuf, argsize,
+                                sensebuf, (10*HZ), 5, 0,0);
+#else
+	cmd_result = scsi_execute(scsidev, scsi_cmd, data_dir, argbuf, argsize,
+                                sensebuf, (10*HZ), 5, 0);
+#endif
+
+
+	if (driver_byte(cmd_result) == DRIVER_SENSE) {/* sense data available */
+		u8 *desc = sensebuf + 8;
+		cmd_result &= ~(0xFF<<24); /* DRIVER_SENSE is not an error */
+
+		/* If we set cc then ATA pass-through will cause a
+		* check condition even if no error. Filter that. */
+		if (cmd_result & SAM_STAT_CHECK_CONDITION) {
+		struct scsi_sense_hdr sshdr;
+		scsi_normalize_sense(sensebuf, SCSI_SENSE_BUFFERSIZE,
+                                   &sshdr);
+		if (sshdr.sense_key==0 &&
+			sshdr.asc==0 && sshdr.ascq==0)
+			cmd_result &= ~SAM_STAT_CHECK_CONDITION;
+		}
+
+		/* Send userspace a few ATA registers (same as drivers/ide) */
+		if (sensebuf[0] == 0x72 &&     /* format is "descriptor" */
+			desc[0] == 0x09 ) {        /* code is "ATA Descriptor" */
+			args[0] = desc[13];    /* status */
+			args[1] = desc[3];     /* error */
+			args[2] = desc[5];     /* sector count (0:7) */
+			if (copy_to_user(arg, args, sizeof(args)))
+				rc = -EFAULT;
+		}
+	}
+
+	if (cmd_result) {
+		rc = -EIO;
+		goto free_req;
+	}
+
+	if ((argbuf) && copy_to_user(arg + sizeof(args), argbuf, argsize))
+		rc = -EFAULT;
+
+free_req:
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	scsi_release_request(sreq);
+#endif
+error:
+	if (sensebuf) kfree(sensebuf);
+	if (argbuf) kfree(argbuf);
+	return rc;
+}
+
+#ifdef SUPPORT_ATA_SECURITY_CMD
+static int check_dma (__u8 ata_op)
+{
+	switch (ata_op) {
+		case ATA_CMD_READ_DMA_EXT:
+		case ATA_CMD_READ_FPDMA_QUEUED:
+		case ATA_CMD_WRITE_DMA_EXT:
+		case ATA_CMD_WRITE_FPDMA_QUEUED:
+		case ATA_CMD_READ_DMA:
+		case ATA_CMD_WRITE_DMA:
+			return SG_DMA;
+		default:
+			return SG_PIO;
+	}
+}
+unsigned char excute_taskfile(struct scsi_device *dev,ide_task_request_t *req_task,u8
+ rw,char *argbuf,unsigned int buff_size)
+{
+	int rc = 0;
+	u8 scsi_cmd[MAX_COMMAND_SIZE];
+	u8 *sensebuf = NULL;
+	int argsize=0;
+	enum dma_data_direction data_dir;
+	int cmd_result;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	struct scsi_request *sreq;
+#endif
+	argsize=buff_size;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	sensebuf = kmalloc(SCSI_SENSE_BUFFERSIZE, GFP_NOIO);
+	if (sensebuf) {
+		memset(sensebuf, 0, SCSI_SENSE_BUFFERSIZE);
+	}
+#else
+	sensebuf = kzalloc(SCSI_SENSE_BUFFERSIZE, GFP_NOIO);
+#endif
+	if (!sensebuf)
+		return -ENOMEM;
+
+	memset(scsi_cmd, 0, sizeof(scsi_cmd));
+
+	data_dir = DMA_FROM_DEVICE;   // need to fixed
+	scsi_cmd[0] = ATA_16;
+	scsi_cmd[13] = 0x40;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 24)
+	scsi_cmd[14] = ((task_struct_t *)(&req_task->io_ports))->command;
+#else
+	scsi_cmd[14] = ((char *)(&req_task->io_ports))[7];
+#endif
+	if(check_dma(scsi_cmd[14])){
+		scsi_cmd[1] = argbuf ? SG_ATA_PROTO_DMA : SG_ATA_PROTO_NON_DATA;
+	} else {
+		scsi_cmd[1] = argbuf ? (rw ? SG_ATA_PROTO_PIO_OUT : SG_ATA_PROTO_PIO_IN) : SG_ATA_PROTO_NON_DATA;
+	}
+	scsi_cmd[ 2] = SG_CDB2_CHECK_COND;
+	if (argbuf) {
+		scsi_cmd[2] |= SG_CDB2_TLEN_NSECT | SG_CDB2_TLEN_SECTORS;
+		scsi_cmd[2] |= rw ? SG_CDB2_TDIR_TO_DEV : SG_CDB2_TDIR_FROM_DEV;
+	}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	sreq = scsi_allocate_request(dev, GFP_KERNEL);
+	if (!sreq) {
+		rc= -EINTR;
+		goto free_req;
+	}
+	sreq->sr_data_direction = data_dir;
+	scsi_wait_req(sreq, scsi_cmd, argbuf, argsize, (10*HZ), 5);
+
+	/*
+	 * If there was an error condition, pass the info back to the user.
+	 */
+	cmd_result = sreq->sr_result;
+	sensebuf = sreq->sr_sense_buffer;
+
+#elif LINUX_VERSION_CODE >=KERNEL_VERSION(2, 6, 29)
+	cmd_result = scsi_execute(dev, scsi_cmd, data_dir, argbuf, argsize,
+                                sensebuf, (10*HZ), 5, 0,0);
+#else
+	cmd_result = scsi_execute(dev, scsi_cmd, data_dir, argbuf, argsize,
+                                sensebuf, (10*HZ), 5, 0);
+#endif
+
+	if (driver_byte(cmd_result) == DRIVER_SENSE) {/* sense data available */
+		u8 *desc = sensebuf + 8;
+		cmd_result &= ~(0xFF<<24); /* DRIVER_SENSE is not an error */
+
+		/* If we set cc then ATA pass-through will cause a
+		* check condition even if no error. Filter that. */
+		if (cmd_result & SAM_STAT_CHECK_CONDITION) {
+		struct scsi_sense_hdr sshdr;
+		scsi_normalize_sense(sensebuf, SCSI_SENSE_BUFFERSIZE,
+                                   &sshdr);
+		if (sshdr.sense_key==0 &&
+			sshdr.asc==0 && sshdr.ascq==0)
+			cmd_result &= ~SAM_STAT_CHECK_CONDITION;
+		}
+
+	}
+
+	if (cmd_result) {
+		rc = EIO;
+		MV_PRINT("EIO=%d\n",-EIO);
+		goto free_req;
+	}
+#if 0
+	if ( copy_to_user(argbuf,sensebuf, argsize))
+		rc = -EFAULT;
+#endif
+free_req:
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+	scsi_release_request(sreq);
+#endif
+	if (sensebuf) kfree(sensebuf);
+	return rc;
+}
+u8 mv_do_taskfile_ioctl(struct scsi_device *dev,void __user *arg){
+	ide_task_request_t *req_task=NULL;
+	char __user *buf = (char __user *)arg;
+	u8 *outbuf	= NULL;
+	u8 *inbuf	= NULL;
+	int err		= 0;
+	int tasksize	= sizeof(ide_task_request_t);
+	int taskin	= 0;
+	int taskout	= 0;
+	int rw = SG_READ;
+
+	req_task = kzalloc(tasksize, GFP_KERNEL);
+	if (req_task == NULL) return -ENOMEM;
+	if (copy_from_user(req_task, buf, tasksize)) {
+		kfree(req_task);
+		return -EFAULT;
+	}
+
+	switch (req_task->req_cmd) {
+		case TASKFILE_CMD_REQ_OUT:
+		case TASKFILE_CMD_REQ_RAW_OUT:
+			rw         = SG_WRITE;
+			break;
+		case TASKFILE_CMD_REQ_IN:
+			break;
+	}
+	taskout = (int) req_task->out_size;
+	taskin  = (int) req_task->in_size;
+
+
+	if (taskout) {
+		int outtotal = tasksize;
+		outbuf = kzalloc(taskout, GFP_KERNEL);
+		if (outbuf == NULL) {
+			err = -ENOMEM;
+			goto abort;
+		}
+		if (copy_from_user(outbuf, buf + outtotal, taskout)) {
+			err = -EFAULT;
+			goto abort;
+		}
+	}
+
+	if (taskin) {
+		int intotal = tasksize + taskout;
+		inbuf = kzalloc(taskin, GFP_KERNEL);
+		if (inbuf == NULL) {
+			err = -ENOMEM;
+			goto abort;
+		}
+		if (copy_from_user(inbuf, buf + intotal, taskin)) {
+			err = -EFAULT;
+			goto abort;
+		}
+	}
+
+	switch(req_task->data_phase) {
+		case TASKFILE_DPHASE_PIO_OUT:
+			err=excute_taskfile(dev,req_task,rw,outbuf,taskout);
+		default:
+			err = -EFAULT;
+			goto abort;
+	}
+	if (copy_to_user(buf, req_task, tasksize)) {
+		err = -EFAULT;
+		goto abort;
+	}
+	if (taskout) {
+		int outtotal = tasksize;
+		if (copy_to_user(buf + outtotal, outbuf, taskout)) {
+			err = -EFAULT;
+			goto abort;
+		}
+	}
+	if (taskin) {
+		int intotal = tasksize + taskout;
+		if (copy_to_user(buf + intotal, inbuf, taskin)) {
+			err = -EFAULT;
+			goto abort;
+		}
+	}
+abort:
+	kfree(req_task);
+	kfree(outbuf);
+	kfree(inbuf);
+
+	return err;
+}
+#endif
+
+/****************************************************************
+ *  Name:   mv_ial_ht_ioctl
+ *
+ *  Description:    mv_sata scsi ioctl
+ *
+ *  Parameters:     scsidev - Device to which we are issuing command
+ *                  cmd     - ioctl command
+ *                  arg     - User provided data for issuing command
+ *
+ *  Returns:        0 on success, otherwise of failure.
+ *
+ ****************************************************************/
+int mv_ial_ht_ioctl(struct scsi_device *scsidev, int cmd, void __user *arg)
+{
+	int rc = -ENOTTY;
+
+	/* No idea how this happens.... */
+	if (!scsidev)
+		return -ENXIO;
+
+	if (arg == NULL)
+		return -EINVAL;
+
+	switch (cmd) {
+		case HDIO_DRIVE_CMD:
+			if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))
+				return -EACCES;
+
+			rc =  mv_ial_ht_ata_cmd(scsidev, arg);
+		break;
+		#ifdef SUPPORT_ATA_SECURITY_CMD
+		case HDIO_DRIVE_TASKFILE:
+			rc=mv_do_taskfile_ioctl(scsidev,arg);
+			break;
+		#endif
+		case HDIO_GET_IDENTITY:
+			rc=mv_identify_ata_cmd(scsidev,arg);
+			break;
+		default:
+			rc = -ENOTTY;
+	}
+
+	return rc;
+}
+#endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+irqreturn_t mv_intr_handler(int irq, void *dev_id, struct pt_regs *regs)
+#else
+irqreturn_t mv_intr_handler(int irq, void *dev_id)
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19) */
+{
+	/* MV_FALSE should be equal to IRQ_NONE (0) */
+	irqreturn_t retval = MV_FALSE;
+	unsigned long flags;
+
+	struct hba_extension *hba = (struct hba_extension *) dev_id;
+	//MV_DBG(DMSG_FREQ, "__MV__ Enter intr handler.\n");
+
+	spin_lock_irqsave(&hba->desc->hba_desc->global_lock, flags);
+	retval = hba->desc->child->ops->module_service_isr(hba->desc->child->extension);
+	#ifdef SUPPORT_TASKLET
+	if(retval)
+		tasklet_schedule(&hba->mv_tasklet);
+	#endif
+
+	spin_unlock_irqrestore(&hba->desc->hba_desc->global_lock, flags);
+
+	//MV_DBG(DMSG_FREQ, "__MV__ Exit intr handler retval=0x%x.\n ",retval);
+	return IRQ_RETVAL(retval);
+}
+
+static int mv_linux_reset (struct scsi_cmnd *cmd)
+{
+	MV_PRINT("__MV__ reset handler %p.\n", cmd);
+	return FAILED;
+}
+
+static struct scsi_host_template mv_driver_template = {
+	.module                      =  THIS_MODULE,
+        .name                        =  "Marvell Storage Controller",
+        .proc_name                   =  mv_driver_name,
+        .proc_info                   =  mv_linux_proc_info,
+        .queuecommand                =  mv_linux_queue_command,
+#ifdef IOCTL_TEMPLATE
+        .ioctl				= mv_ial_ht_ioctl,
+#endif
+#if 0
+        .eh_abort_handler            =  mv_linux_abort,
+        .eh_device_reset_handler     =  mv_linux_reset,
+        .eh_bus_reset_handler        =  mv_linux_reset,
+#endif /* 0 */
+        .eh_host_reset_handler       =  mv_linux_reset,
+#if  LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 7) && \
+     LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 16)
+        .eh_timed_out                =  mv_linux_timed_out,
+#endif
+        .can_queue                   =  MV_MAX_REQUEST_DEPTH,
+        .this_id                     =  MV_SHT_THIS_ID,
+        .max_sectors                 =  (256*1024) >> 9,//max os data size 256k
+        .sg_tablesize                =  256*1024/PAGE_SIZE,
+        .cmd_per_lun                 =  MV_MAX_REQUEST_PER_LUN,
+        .use_clustering              =  MV_SHT_USE_CLUSTERING,
+        .emulated                    =  MV_SHT_EMULATED,
+};
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 15)
+static struct scsi_transport_template mv_transport_template = {
+	.eh_timed_out   =  mv_linux_timed_out,
+};
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+
+
+/* module management & hba module code */
+extern struct mv_module_ops *mv_core_register_module(void);
+extern struct mv_module_ops *mv_hba_register_module(void);
+
+#ifdef RAID_DRIVER
+extern struct mv_module_ops *mv_raid_register_module(void);
+#else
+static inline struct mv_module_ops *mv_raid_register_module(void)
+{
+	return NULL;
+}
+#endif /* RAID_DRIVER */
+
+#ifdef CACHE_MODULE_SUPPORT
+extern struct mv_module_ops *mv_cache_register_module(void);
+#else
+static inline struct mv_module_ops *mv_cache_register_module(void)
+{
+	return NULL;
+}
+#endif /* CACHE_DRIVER */
+
+static LIST_HEAD(mv_online_adapter_list);
+
+int __mv_get_adapter_count(void)
+{
+	struct mv_adp_desc *p;
+	int i = 0;
+
+	list_for_each_entry(p, &mv_online_adapter_list, hba_entry)
+		i++;
+
+	return i;
+}
+
+ inline struct mv_adp_desc *__dev_to_desc(struct pci_dev *dev)
+{
+	struct mv_adp_desc *p;
+
+	list_for_each_entry(p, &mv_online_adapter_list, hba_entry)
+		if (p->dev == dev)
+			return p;
+	return NULL;
+}
+
+MV_PVOID *mv_get_hba_extension(struct mv_adp_desc *hba_desc)
+{
+	struct mv_mod_desc *p;
+
+	list_for_each_entry(p, &hba_desc->online_module_list, mod_entry)
+		if (MODULE_HBA == p->module_id)
+			return p->extension;
+	return NULL;
+}
+
+static inline void __mv_release_hba(struct mv_adp_desc *hba_desc)
+{
+	struct mv_mod_desc *mod_desc, *p;
+
+	list_for_each_entry_safe(mod_desc,
+				 p,
+				 &hba_desc->online_module_list,
+				 mod_entry) {
+		list_del(&mod_desc->mod_entry);
+		vfree(mod_desc);
+	}
+
+	list_del(&hba_desc->hba_entry);
+	vfree(hba_desc);
+}
+
+static struct mv_adp_desc *mv_hba_init_modmm(struct pci_dev *dev)
+{
+	struct mv_adp_desc *hba_desc;
+
+	hba_desc = vmalloc(sizeof(struct mv_adp_desc));
+	if (NULL == hba_desc) {
+		printk("Unable to get memory at hba init.\n");
+		return NULL;
+	}
+	memset(hba_desc, 0, sizeof(struct mv_adp_desc));
+
+	INIT_LIST_HEAD(&hba_desc->online_module_list);
+	hba_desc->dev = dev;
+	list_add(&hba_desc->hba_entry, &mv_online_adapter_list);
+
+	return hba_desc;
+}
+
+static void mv_hba_release_modmm(struct pci_dev *dev)
+{
+	struct mv_adp_desc *hba_desc;
+
+	hba_desc = __dev_to_desc(dev);
+
+	if (hba_desc)
+		__mv_release_hba(hba_desc);
+	else
+		printk("Weired! dev %p unassociated with any desc.\n", dev);
+}
+
+static inline struct mv_mod_desc *__alloc_mod_desc(void)
+{
+	struct mv_mod_desc *desc;
+
+	desc = vmalloc(sizeof(struct mv_mod_desc));
+	if (desc)
+		memset(desc, 0, sizeof(struct mv_mod_desc));
+	return desc;
+}
+
+static int register_online_modules(struct mv_adp_desc *hba_desc)
+{
+	struct mv_mod_desc *mod_desc, *prev;
+	struct mv_module_ops *ops;
+
+	/*
+	 * iterate through online_module_list manually , from the lowest(CORE)
+	 * to the highest layer (HBA)
+	 */
+	hba_desc->running_mod_num = 0;
+	/* CORE */
+	ops = mv_core_register_module();
+	if (NULL == ops) {
+		printk("No core no life.\n");
+		return -1;
+	}
+	mod_desc = __alloc_mod_desc();
+	if (NULL == mod_desc)
+		goto disaster;
+
+	mod_desc->hba_desc  = hba_desc;
+	mod_desc->ops       = ops;
+	mod_desc->status    = MV_MOD_REGISTERED;
+	mod_desc->module_id = MODULE_CORE;
+	mod_desc->child     = NULL;
+	list_add(&mod_desc->mod_entry, &hba_desc->online_module_list);
+	hba_desc->running_mod_num++;
+
+#ifdef ODIN_DRIVER
+	/* when running in non-RAID, both CACHE and RAID must be disabled */
+	if(!hba_desc->RunAsNonRAID)
+	{
+#endif
+
+#ifdef RAID_DRIVER
+	/* RAID */
+	ops = mv_raid_register_module();
+	if (ops) {
+		prev = mod_desc;
+		mod_desc = __alloc_mod_desc();
+		if (NULL == mod_desc)
+			goto disaster;
+
+		mod_desc->hba_desc  = hba_desc;
+		mod_desc->ops       = ops;
+		mod_desc->status    = MV_MOD_REGISTERED;
+		mod_desc->module_id = MODULE_RAID;
+		mod_desc->child     = prev;
+		prev->parent        = mod_desc;
+		list_add(&mod_desc->mod_entry, &hba_desc->online_module_list);
+		hba_desc->running_mod_num++;
+	}
+#endif
+
+#ifdef CACHE_MODULE_SUPPORT
+	/* CACHE */
+	ops = mv_cache_register_module();
+	if (ops) {
+		prev = mod_desc;
+		mod_desc = __alloc_mod_desc();
+		if (NULL == mod_desc)
+			goto disaster;
+
+		mod_desc->hba_desc  = hba_desc;
+		mod_desc->ops       = ops;
+		mod_desc->status    = MV_MOD_REGISTERED;
+		mod_desc->module_id = MODULE_CACHE;
+		mod_desc->child     = prev;
+		prev->parent        = mod_desc;
+		list_add(&mod_desc->mod_entry, &hba_desc->online_module_list);
+		hba_desc->running_mod_num++;
+	}
+#endif
+
+#ifdef ODIN_DRIVER
+	}
+#endif
+
+	/* HBA */
+	prev = mod_desc;
+	mod_desc = __alloc_mod_desc();
+	if (NULL == mod_desc)
+		goto disaster;
+
+	mod_desc->ops = mv_hba_register_module();
+	if (NULL == mod_desc->ops) {
+		printk("No HBA no life.\n");
+		return -1;
+	}
+
+	mod_desc->hba_desc  = hba_desc;
+	mod_desc->status    = MV_MOD_REGISTERED;
+	mod_desc->module_id = MODULE_HBA;
+	mod_desc->child     = prev;
+	mod_desc->parent    = NULL;
+	prev->parent        = mod_desc;
+	list_add(&mod_desc->mod_entry, &hba_desc->online_module_list);
+	hba_desc->running_mod_num++;
+
+	return 0;
+disaster:
+	return -1;
+}
+
+
+static void __release_consistent_mem(struct mv_mod_res *mod_res,
+				     struct pci_dev *dev)
+{
+	dma_addr_t       dma_addr;
+	MV_PHYSICAL_ADDR phy_addr;
+
+	phy_addr = mod_res->bus_addr;
+	dma_addr = (dma_addr_t) (phy_addr.parts.low |
+				 ((u64) phy_addr.parts.high << 32));
+	pci_free_consistent(dev,
+			    mod_res->size,
+			    mod_res->virt_addr,
+			    dma_addr);
+}
+
+static int __alloc_consistent_mem(struct mv_mod_res *mod_res,
+				  struct pci_dev *dev)
+{
+	unsigned long size;
+	dma_addr_t    dma_addr;
+	BUS_ADDRESS   bus_addr;
+	MV_PHYSICAL_ADDR phy_addr;
+
+	size = mod_res->size;
+	WARN_ON(size != ROUNDING(size, 8));
+	size = ROUNDING(size, 8);
+	mod_res->virt_addr = (MV_PVOID) pci_alloc_consistent(dev,
+							     size,
+							     &dma_addr);
+	if (NULL == mod_res->virt_addr) {
+		MV_DBG(DMSG_HBA, "unable to alloc 0x%lx consistent mem.\n",
+		       size);
+		return -1;
+	}
+	memset(mod_res->virt_addr, 0, size);
+	bus_addr            = (BUS_ADDRESS) dma_addr;
+	phy_addr.parts.low  = LO_BUSADDR(bus_addr);
+	phy_addr.parts.high = HI_BUSADDR(bus_addr);
+	mod_res->bus_addr   = phy_addr;
+
+	return 0;
+}
+
+int HBA_GetResource(struct mv_mod_desc *mod_desc,
+		    enum Resource_Type type,
+		    MV_U32  size,
+		    Assigned_Uncached_Memory *dma_res)
+{
+	struct mv_mod_res *mod_res;
+
+	mod_res = vmalloc(sizeof(struct mv_mod_res));
+	if (NULL == mod_res) {
+		printk("unable to allocate memory for resource management.\n");
+		return -1;
+	}
+
+	memset(mod_res, 0, sizeof(struct mv_mod_res));
+	mod_res->size = size;
+	mod_res->type = type;
+	switch (type) {
+	case RESOURCE_UNCACHED_MEMORY :
+		if (__alloc_consistent_mem(mod_res, mod_desc->hba_desc->dev)) {
+			printk("unable to allocate 0x%x uncached mem.\n", size);
+			vfree(mod_res);
+			return -1;
+		}
+		list_add(&mod_res->res_entry, &mod_desc->res_list);
+		memset(mod_res->virt_addr, 0, size);
+		dma_res->Virtual_Address  = mod_res->virt_addr;
+		dma_res->Physical_Address = mod_res->bus_addr;
+		dma_res->Byte_Size        = size;
+		break;
+	case RESOURCE_CACHED_MEMORY :
+	default:
+		vfree(mod_res);
+		printk("unknown resource type %d.\n", type);
+		return -1;
+	}
+	return 0;
+}
+
+static void __release_resource(struct mv_adp_desc *hba_desc,
+			       struct mv_mod_desc *mod_desc)
+{
+	struct mv_mod_res *mod_res, *tmp;
+
+	list_for_each_entry_safe(mod_res,
+				 tmp,
+				 &mod_desc->res_list,
+				 res_entry) {
+		switch (mod_res->type) {
+		case RESOURCE_UNCACHED_MEMORY :
+			__release_consistent_mem(mod_res, hba_desc->dev);
+			break;
+		case RESOURCE_CACHED_MEMORY :
+			vfree(mod_res->virt_addr);
+			break;
+		default:
+			MV_DBG(DMSG_HBA, "res type %d unknown.\n",
+			       mod_res->type);
+			break;
+		}
+		list_del(&mod_res->res_entry);
+		vfree(mod_res);
+	}
+}
+
+static void __release_module_resource(struct mv_mod_desc *mod_desc)
+{
+	__release_resource(mod_desc->hba_desc, mod_desc);
+}
+
+static int __alloc_module_resource(struct mv_mod_desc *mod_desc,
+				   unsigned int max_io)
+{
+	struct mv_mod_res *mod_res = NULL;
+	unsigned int size = 0;
+
+	/*
+	 * alloc only cached mem at this stage, uncached mem will be alloc'ed
+	 * during mod init.
+	 */
+	INIT_LIST_HEAD(&mod_desc->res_list);
+	mod_res = vmalloc(sizeof(struct mv_mod_res));
+	if (NULL == mod_res)
+		return -1;
+	memset(mod_res, 0, sizeof(sizeof(struct mv_mod_res)));
+	mod_desc->res_entry = 1;
+
+	size = mod_desc->ops->get_res_desc(RESOURCE_CACHED_MEMORY, max_io);
+	if (size) {
+		mod_res->virt_addr = vmalloc(size);
+		if (NULL == mod_res->virt_addr) {
+			vfree(mod_res);
+			return -1;
+		}
+		memset(mod_res->virt_addr, 0, size);
+		mod_res->type                = RESOURCE_CACHED_MEMORY;
+		mod_res->size                = size;
+		mod_desc->extension          = mod_res->virt_addr;
+		mod_desc->extension_size     = size;
+		list_add(&mod_res->res_entry, &mod_desc->res_list);
+	}
+
+	return 0;
+}
+
+static void mv_release_module_resource(struct mv_adp_desc *hba_desc)
+{
+	struct mv_mod_desc *mod_desc;
+
+	list_for_each_entry(mod_desc, &hba_desc->online_module_list,
+			    mod_entry) {
+		if (mod_desc->status == MV_MOD_INITED) {
+			__release_module_resource(mod_desc);
+			mod_desc->status = MV_MOD_REGISTERED;
+		}
+	}
+}
+
+static int mv_alloc_module_resource(struct mv_adp_desc *hba_desc)
+{
+	struct mv_mod_desc *mod_desc;
+	int ret;
+
+	list_for_each_entry(mod_desc, &hba_desc->online_module_list,
+			    mod_entry) {
+		ret = __alloc_module_resource(mod_desc, hba_desc->max_io);
+		if (ret)
+			goto err_out;
+		mod_desc->status = MV_MOD_INITED;
+	}
+	return 0;
+
+err_out:
+	MV_DBG(DMSG_HBA, "error %d allocating resource for mod %d.\n",
+	       ret, mod_desc->module_id);
+	list_for_each_entry(mod_desc, &hba_desc->online_module_list,
+			    mod_entry) {
+		if (mod_desc->status == MV_MOD_INITED) {
+			__release_module_resource(mod_desc);
+			mod_desc->status = MV_MOD_REGISTERED;
+		}
+	}
+	return -1;
+}
+
+
+
+static inline struct mv_mod_desc *
+__get_highest_module(struct mv_adp_desc *hba_desc)
+{
+	struct mv_mod_desc *p;
+
+	/* take a random module, and trace through its parent */
+	p = list_entry(hba_desc->online_module_list.next,
+		       struct mv_mod_desc,
+		       mod_entry);
+
+	WARN_ON(NULL == p);
+	while (p) {
+		if (NULL == p->parent)
+			break;
+		p = p->parent;
+	}
+	return p;
+}
+
+static void __map_pci_addr(struct pci_dev *dev, MV_PVOID *addr_array)
+{
+	int i;
+	unsigned long addr;
+	unsigned long range;
+
+	for (i = 0; i < MAX_BASE_ADDRESS; i++) {
+		addr  = pci_resource_start(dev, i);
+		range = pci_resource_len(dev, i);
+
+		if (pci_resource_flags(dev, i) & IORESOURCE_MEM)
+			addr_array[i] =(MV_PVOID) ioremap(addr, range);
+		else if (pci_resource_flags(dev, i) & IORESOURCE_IO)
+			addr_array[i] = (MV_PVOID) ioport_map(addr, range);
+
+
+		MV_DBG(DMSG_HBA, "BAR %d : %p.\n",
+		       i, addr_array[i]);
+	}
+}
+
+static void __unmap_pci_addr(struct pci_dev *dev, MV_PVOID *addr_array)
+{
+	int i;
+
+	for (i = 0; i < MAX_BASE_ADDRESS; i++)
+		if (pci_resource_flags(dev, i) & IORESOURCE_MEM)
+                        iounmap(addr_array[i]);
+		else if (pci_resource_flags(dev, i) & IORESOURCE_IO)
+			ioport_unmap(addr_array[i]);
+}
+
+int __mv_is_mod_all_started(struct mv_adp_desc *adp_desc)
+{
+	struct mv_mod_desc *mod_desc;
+
+	mod_desc = __get_lowest_module(adp_desc);
+
+	while (mod_desc) {
+		if (MV_MOD_STARTED != mod_desc->status)
+			return 0;
+
+		mod_desc = mod_desc->parent;
+	}
+	return 1;
+}
+
+struct hba_extension *__mv_get_ext_from_adp_id(int id)
+{
+	struct mv_adp_desc *p;
+
+	list_for_each_entry(p, &mv_online_adapter_list, hba_entry)
+		if (p->id == id)
+			return __get_highest_module(p)->extension;
+
+	return NULL;
+}
+
+int mv_hba_start(struct pci_dev *dev)
+{
+	struct mv_adp_desc *hba_desc;
+	struct mv_mod_desc *mod_desc;
+
+	hba_desc = __dev_to_desc(dev);
+	BUG_ON(NULL == hba_desc);
+#if 0
+	mod_desc = __get_lowest_module(hba_desc);
+	if (NULL == mod_desc)
+		return -1;
+	MV_DPRINT(("Start lowest module.\n"));
+
+#ifdef THOR_DRIVER
+	spin_lock_irq(&hba_desc->global_lock);
+#endif
+
+	mod_desc->ops->module_start(mod_desc->extension);
+
+#ifdef THOR_DRIVER
+	spin_unlock_irq(&hba_desc->global_lock);
+#endif
+#endif
+	MV_DPRINT(("Start highest module.\n"));
+	mod_desc = __get_highest_module(hba_desc);
+	if (NULL == mod_desc)
+		return -1;
+
+	mod_desc->ops->module_start(mod_desc->extension);
+
+	return 0;
+}
+
+static void __hba_module_stop(struct mv_adp_desc *hba_desc)
+{
+	struct mv_mod_desc *mod_desc;
+
+	mod_desc = __get_highest_module(hba_desc);
+	if (NULL == mod_desc)
+		return;
+
+	/* starting from highest module, unlike module_start */
+	while (mod_desc) {
+		if ((MV_MOD_STARTED == mod_desc->status)) {
+			mod_desc->ops->module_stop(mod_desc->extension);
+			mod_desc->status = MV_MOD_INITED;
+		}
+		mod_desc = mod_desc->child;
+	}
+}
+
+/* stop all HBAs if dev == NULL */
+void mv_hba_stop(struct pci_dev *dev)
+{
+	struct mv_adp_desc *hba_desc;
+
+	hba_wait_eh();
+
+	if (dev) {
+		hba_desc = __dev_to_desc(dev);
+		__hba_module_stop(hba_desc);
+	} else {
+		list_for_each_entry(hba_desc, &mv_online_adapter_list, hba_entry)
+			__hba_module_stop(hba_desc);
+	}
+}
+
+void mv_hba_release(struct pci_dev *dev)
+{
+	struct mv_adp_desc *hba_desc;
+
+	hba_desc = __dev_to_desc(dev);
+	if (NULL != hba_desc) {
+		__unmap_pci_addr(hba_desc->dev, hba_desc->Base_Address);
+		mv_release_module_resource(hba_desc);
+		mv_hba_release_modmm(hba_desc->dev);
+	}
+}
+
+int mv_hba_init(struct pci_dev *dev, MV_U32 max_io)
+{
+	struct mv_adp_desc *hba_desc;
+	struct mv_mod_desc *mod_desc;
+	int    dbg_ret = 0;
+
+	hba_desc = mv_hba_init_modmm(dev);
+	if (NULL == hba_desc)
+		goto ext_err_init;
+
+
+	hba_desc->max_io = max_io;
+	hba_desc->id     = __mv_get_adapter_count() - 1;
+
+	if (pci_read_config_byte(hba_desc->dev,
+				 PCI_REVISION_ID,
+				 &hba_desc->Revision_Id)) {
+		MV_PRINT("Failed to get hba's revision id.\n");
+		goto ext_err_pci;
+	}
+
+	hba_desc->vendor = dev->vendor;
+	hba_desc->device = dev->device;
+	hba_desc->subsystem_vendor = dev->subsystem_vendor;
+	hba_desc->subsystem_device = dev->subsystem_device;
+
+	__map_pci_addr(dev, hba_desc->Base_Address);
+
+	spin_lock_init(&hba_desc->lock);
+	spin_lock_init(&hba_desc->global_lock);
+
+	/* For Odin family, read PCIE configuration space register bit 31:25 [PAD_TEST] (offset=0x60) */
+	/* to identify different part */
+#ifdef ODIN_DRIVER
+	if (hba_desc->device != DEVICE_ID_6480) {
+		MV_U32 padTest = 0;
+		pci_read_config_dword(dev, 0x60, &padTest);
+		hba_desc->device = SetDeviceID(padTest);
+		hba_desc->subsystem_device = hba_desc->device;
+		if (hba_desc->device == DEVICE_ID_6445)
+			hba_desc->RunAsNonRAID = MV_TRUE;
+
+		MV_DBG(DMSG_HBA, "controller device id 0x%x.\n",
+	       hba_desc->device);
+
+	}
+#endif
+
+
+
+	MV_DBG(DMSG_HBA, "HBA ext struct init'ed at %p.\n",
+	        hba_desc);
+
+	if (register_online_modules(hba_desc))
+		goto ext_err_modmm;
+
+	if (mv_alloc_module_resource(hba_desc))
+		goto ext_err_modmm;
+	#ifdef CONFIG_PM
+	mod_desc =  __get_highest_module(hba_desc);
+	pci_set_drvdata(dev,mod_desc);
+	#endif
+	mod_desc = __get_lowest_module(hba_desc);
+	if (NULL == mod_desc)
+		goto ext_err_pci;
+
+	while (mod_desc) {
+		if (MV_MOD_INITED != mod_desc->status)
+			continue;
+		mod_desc->ops->module_initialize(mod_desc,
+						 mod_desc->extension_size,
+						 hba_desc->max_io);
+		/* there's no support for sibling module at the moment  */
+		mod_desc = mod_desc->parent;
+	}
+	return 0;
+
+ext_err_pci:
+	++dbg_ret;
+	mv_release_module_resource(hba_desc);
+ext_err_modmm:
+	++dbg_ret;
+	mv_hba_release_modmm(dev);
+ext_err_init:
+        ++dbg_ret;
+	return dbg_ret;
+}
+
+static MV_U32 HBA_ModuleGetResourceQuota(enum Resource_Type type, MV_U16 maxIo)
+{
+	MV_U32 size = 0;
+
+	if (type == RESOURCE_CACHED_MEMORY) {
+		/* Fixed memory */
+		size = OFFSET_OF(HBA_Extension, Memory_Pool);
+		size = ROUNDING(size, 8);
+
+#ifdef SUPPORT_EVENT
+		size += sizeof(Driver_Event_Entry) * MAX_EVENTS;
+		MV_ASSERT(size == ROUNDING(size, 8));
+#endif /* SUPPORT_EVENT */
+	}
+
+	return size;
+}
+
+#ifdef SUPPORT_TASKLET
+MV_BOOLEAN Core_TaskletHandler(MV_PVOID This);
+static void run_tasklet(PHBA_Extension   phba)
+{
+	struct mv_mod_desc *core_desc=__get_lowest_module(phba->desc->hba_desc);
+	unsigned long flags;
+	MV_PVOID pcore=core_desc->extension;
+	spin_lock_irqsave(&phba->desc->hba_desc->global_lock, flags);
+	Core_TaskletHandler(pcore);
+	spin_unlock_irqrestore(&phba->desc->hba_desc->global_lock, flags);
+}
+#endif
+
+static void HBA_ModuleInitialize(MV_PVOID this,
+				 MV_U32   size,
+				 MV_U16   max_io)
+{
+	struct   mv_mod_desc *mod_desc=(struct   mv_mod_desc *)this;
+	PHBA_Extension phba;
+	MV_PTR_INTEGER temp;
+	MV_U32 i;
+	MV_U32 sg_num;
+
+#ifdef SUPPORT_EVENT
+	PDriver_Event_Entry pEvent = NULL;
+#endif /* SUPPORT_EVENT */
+
+	phba     = (PHBA_Extension) mod_desc->extension;
+	temp     = (MV_PTR_INTEGER) phba->Memory_Pool;
+
+	WARN_ON(sizeof(MV_Request) != ROUNDING(sizeof(MV_Request), 8));
+	/*
+	 * Initialize data structure however following variables have been
+	 * set already.
+	 *	Device_Extension
+	 *	Is_Dump
+	 */
+	phba->State    = DRIVER_STATUS_IDLE;
+	phba->Io_Count = 0;
+	phba->Max_Io   = max_io;
+	phba->desc     = mod_desc;
+
+	init_completion(&phba->cmpl);
+	temp = ROUNDING(((MV_PTR_INTEGER) temp), 8);
+
+	if (max_io > 1)
+		sg_num   = MAX_SG_ENTRY;
+	else
+		sg_num   = MAX_SG_ENTRY_REDUCED;
+
+	phba->req_pool = (MV_PVOID) res_reserve_req_pool(MODULE_HBA,
+							 max_io,
+							 sg_num);
+	BUG_ON(NULL == phba->req_pool);
+#ifdef THOR_DRIVER
+	//spin_lock_init(&phba->lock);
+	init_timer(&phba->timer);
+#endif
+
+#ifdef SUPPORT_TASKLET
+	tasklet_init(&phba->mv_tasklet,
+			(void (*)(unsigned long))run_tasklet, (unsigned long)phba);
+#endif
+
+#ifdef SUPPORT_EVENT
+	INIT_LIST_HEAD(&phba->Stored_Events);
+	INIT_LIST_HEAD(&phba->Free_Events);
+	phba->Num_Stored_Events = 0;
+	phba->SequenceNumber = 0;	/* Event sequence number */
+
+	MV_ASSERT(sizeof(Driver_Event_Entry) ==
+		  ROUNDING(sizeof(Driver_Event_Entry), 8));
+	temp = ROUNDING(((MV_PTR_INTEGER) temp), 8);
+
+	for (i = 0; i < MAX_EVENTS; i++) {
+		pEvent = (PDriver_Event_Entry) temp;
+		list_add_tail(&pEvent->Queue_Pointer, &phba->Free_Events);
+		temp += sizeof(Driver_Event_Entry);
+	}
+#endif /* SUPPORT_EVENT */
+
+}
+
+static void HBA_ModuleStart(MV_PVOID extension)
+{
+	struct Scsi_Host *host = NULL;
+	struct hba_extension *hba;
+	int ret;
+
+	struct mv_adp_desc *adp_desc;
+	struct mv_mod_desc *core_desc;
+
+	hba = (struct hba_extension *) extension;
+	host = scsi_host_alloc(&mv_driver_template, sizeof(void *));
+	if (NULL == host) {
+		MV_PRINT("Unable to allocate a scsi host.\n" );
+		goto err_out;
+	}
+
+	*((MV_PVOID *) host->hostdata) = extension;
+	hba->host = host;
+	hba->dev  = hba->desc->hba_desc->dev;
+
+	host->irq          = hba->dev->irq;
+	host->max_id       = MV_MAX_TARGET_NUMBER;
+	host->max_lun      = MV_MAX_LUN_NUMBER;
+	host->max_channel  = 0;
+	host->max_cmd_len  = 16;
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 15)
+	host->transportt   = &mv_transport_template;
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+
+#ifdef USE_MSI
+	pci_enable_msi(hba->dev);
+#endif /* USE_MSI */
+
+	MV_DBG(DMSG_KERN, "start install request_irq.\n");
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
+	ret = request_irq(hba->dev->irq, mv_intr_handler, IRQF_SHARED,
+			  mv_driver_name, hba);
+#else
+	ret = request_irq(hba->dev->irq, mv_intr_handler, SA_SHIRQ,
+			  mv_driver_name, hba);
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19) */
+	if (ret < 0) {
+		MV_PRINT("Error upon requesting IRQ %d.\n",
+		       hba->dev->irq);
+		goto  err_request_irq;
+	}
+	MV_DBG(DMSG_KERN, "request_irq has been installed.\n");
+
+	hba->desc->status = MV_MOD_STARTED;
+
+	/* To start CORE layer */
+	adp_desc = hba->desc->hba_desc;
+	core_desc = __get_lowest_module(adp_desc);
+	if (NULL == core_desc) {
+		goto err_wait_cmpl;
+	}
+
+	core_desc->ops->module_start(core_desc->extension);
+
+
+	HBA_ModuleStarted(hba->desc);
+
+	MV_DBG(DMSG_KERN, "wait_for_completion_timeout.....\n");
+	/* wait for MODULE(CORE,RAID,HBA) init */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_set(&hba->hba_sync, 1);
+	if (0 == __hba_wait_for_atomic_timeout(&hba->hba_sync, 90 * HZ))
+		goto err_wait_cmpl;
+#else
+	if (0 == wait_for_completion_timeout(&hba->cmpl, 90 * HZ))
+		goto err_wait_cmpl;
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+
+	hba_house_keeper_run();
+
+	if (scsi_add_host(host, &hba->dev->dev))
+		goto err_wait_cmpl;
+
+	MV_DPRINT(("Start scsi_scan_host.\n"));
+
+	scsi_scan_host(host);
+
+	if (mv_register_chdev(hba))
+		printk("Unable to register character device interface.\n");
+
+
+	MV_DPRINT(("Finished HBA_ModuleStart.\n"));
+
+	return;
+err_wait_cmpl:
+	printk("Timeout waiting for module start.\n");
+	free_irq(hba->dev->irq, hba);
+err_request_irq:
+	scsi_host_put(host);
+	hba->host = NULL;
+err_out:
+	return;
+}
+
+extern void hba_send_shutdown_req(PHBA_Extension phba);
+static void HBA_ModuleShutdown(MV_PVOID extension)
+{
+	PHBA_Extension hba = (PHBA_Extension) extension;
+
+	mv_unregister_chdev(hba);
+
+	if (DRIVER_STATUS_STARTED == hba->State) {
+		scsi_remove_host(hba->host);
+		scsi_host_put(hba->host);
+		hba->host = NULL;
+
+		hba_send_shutdown_req(hba);
+		while(hba->Io_Count != 0)
+			mdelay(1);
+		free_irq(hba->dev->irq, hba);
+	}
+	res_release_req_pool(hba->req_pool);
+	hba->State = DRIVER_STATUS_SHUTDOWN;
+}
+
+#ifdef SUPPORT_EVENT
+static MV_BOOLEAN add_event(IN MV_PVOID extension,
+			    IN MV_U32 eventID,
+			    IN MV_U16 deviceID,
+			    IN MV_U8 severityLevel,
+			    IN MV_U8 param_cnt,
+			    IN MV_PVOID params)
+{
+	struct hba_extension * hba = (struct hba_extension *) extension;
+	PDriver_Event_Entry pEvent;
+	static MV_U32 sequenceNo = 1;
+	if (param_cnt > MAX_EVENT_PARAMS)
+		return MV_FALSE;
+
+	if (list_empty(&hba->Free_Events)) {
+		/* No free entry, we need to reuse the oldest entry from
+		 * Stored_Events.
+		 */
+		MV_ASSERT(!list_empty(&hba->Stored_Events));
+		MV_ASSERT(hba->Num_Stored_Events == MAX_EVENTS);
+		pEvent = List_GetFirstEntry((&hba->Stored_Events), Driver_Event_Entry, Queue_Pointer);
+	}
+	else
+	{
+		pEvent = List_GetFirstEntry((&hba->Free_Events), Driver_Event_Entry, Queue_Pointer);
+		hba->Num_Stored_Events++;
+		MV_ASSERT(hba->Num_Stored_Events <= MAX_EVENTS);
+	}
+
+	pEvent->Event.AdapterID  = hba->desc->hba_desc->id;
+	pEvent->Event.EventID    = eventID;
+	pEvent->Event.SequenceNo = sequenceNo++;
+	pEvent->Event.Severity   = severityLevel;
+	pEvent->Event.DeviceID   = deviceID;
+//	pEvent->Event.Param_Cnt  = param_cnt;
+	pEvent->Event.TimeStamp  = ossw_get_time_in_sec();
+
+	if (param_cnt > 0 && params != NULL)
+		MV_CopyMemory( (MV_PVOID)pEvent->Event.Params, (MV_PVOID)params, param_cnt * 4 );
+
+	list_add_tail(&pEvent->Queue_Pointer, &hba->Stored_Events);
+
+	return MV_TRUE;
+}
+
+static void get_event(MV_PVOID This, PMV_Request pReq)
+{
+	struct hba_extension * hba = (struct hba_extension *) This;
+	PEventRequest pEventReq = (PEventRequest)pReq->Data_Buffer;
+	PDriver_Event_Entry pfirst_event;
+	MV_U8 count = 0;
+
+	pEventReq->Count = 0;
+
+	if ( hba->Num_Stored_Events > 0 )
+	{
+		MV_DASSERT( !list_empty(&hba->Stored_Events) );
+		while (!list_empty(&hba->Stored_Events) &&
+		       (count < MAX_EVENTS_RETURNED)) {
+			pfirst_event = List_GetFirstEntry((&hba->Stored_Events), Driver_Event_Entry, Queue_Pointer);
+			MV_CopyMemory(&pEventReq->Events[count],
+				      &pfirst_event->Event,
+				      sizeof(DriverEvent));
+			hba->Num_Stored_Events--;
+			list_add_tail(&pfirst_event->Queue_Pointer,
+				      &hba->Free_Events );
+			count++;
+		}
+		pEventReq->Count = count;
+	}
+
+	pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+	return;
+}
+#else /* SUPPORT_EVENT */
+static inline MV_BOOLEAN add_event(IN MV_PVOID extension,
+				   IN MV_U32 eventID,
+				   IN MV_U16 deviceID,
+				   IN MV_U8 severityLevel,
+				   IN MV_U8 param_cnt,
+				   IN MV_PVOID params) {}
+
+static inline void get_event(MV_PVOID This, PMV_Request pReq) {}
+#endif /* SUPPORT_EVENT */
+
+
+void HBA_ModuleNotification(MV_PVOID This,
+			     enum Module_Event event,
+			     struct mod_notif_param *event_param)
+{
+	/* "This" passed in is not hba extension, it is caller's extension */
+	/* has to find own extension like the implementation in HBA */
+	MV_PVOID hba = HBA_GetModuleExtension(This, MODULE_HBA);
+	MV_DPRINT(("Enter HBA_ModuleNotification event %x\n",event));
+//	MV_POKE();
+	switch (event) {
+	case EVENT_LOG_GENERATED:
+		add_event(hba,
+			  event_param->event_id,
+			  event_param->dev_id,
+			  event_param->severity_lvl,
+			  event_param->param_count,
+			  event_param->p_param);
+		break;
+	case EVENT_DEVICE_REMOVAL:
+	case EVENT_DEVICE_ARRIVAL:
+		hba_msg_insert(hba,
+			       event,
+			       (event_param == NULL)?0:event_param->lo);
+		break;
+	case EVENT_HOT_PLUG:
+		hba_msg_insert(event_param->p_param,
+			       event,
+			       event_param->event_id);
+		break;
+	default:
+		break;
+	}
+}
+#if 1//def SUPPORT_SCSI_PASSTHROUGH
+
+/* helper functions related to HBA_ModuleSendRequest */
+static void mvGetAdapterInfo( MV_PVOID This, PMV_Request pReq )
+{
+	PHBA_Extension hba = (PHBA_Extension)This;
+	struct mv_adp_desc *pHBA=hba->desc->hba_desc;
+	PAdapter_Info pAdInfo;
+#ifdef ODIN_DRIVER
+	PCore_Driver_Extension pCore = (PCore_Driver_Extension)HBA_GetModuleExtension(This,MODULE_CORE);
+#endif
+	/* initialize */
+	pAdInfo = (PAdapter_Info)pReq->Data_Buffer;
+	MV_ZeroMemory(pAdInfo, sizeof(Adapter_Info));
+
+
+	pAdInfo->DriverVersion.VerMajor = VER_MAJOR;
+	pAdInfo->DriverVersion.VerMinor = VER_MINOR;
+	pAdInfo->DriverVersion.VerOEM = VER_OEM;
+	pAdInfo->DriverVersion.VerBuild = VER_BUILD;
+
+	pAdInfo->SystemIOBusNumber = pHBA->Adapter_Bus_Number;
+	pAdInfo->SlotNumber = pHBA->Adapter_Device_Number;
+	pAdInfo->VenID = pHBA->vendor;
+	pAdInfo->DevID = pHBA->device;
+	pAdInfo->SubDevID = pHBA->subsystem_device;
+	pAdInfo->SubVenID = pHBA->subsystem_vendor;
+	pAdInfo->RevisionID = pHBA->Revision_Id;
+
+	if ( pHBA->device == DEVICE_ID_THORLITE_2S1P||pHBA->device == DEVICE_ID_THORLITE_2S1P_WITH_FLASH ){
+		pAdInfo->PortCount = 3;
+		pAdInfo->MaxHD = 1+2*4;//1PATA + 2SATA * 4 PM disk
+		pAdInfo->MaxPM = 2;
+		pAdInfo->PortSupportType = HD_TYPE_SATA | HD_TYPE_PATA;
+	}else if ( pHBA->device == DEVICE_ID_THORLITE_0S1P){
+		pAdInfo->PortCount = 1;
+		pAdInfo->MaxHD = 1;
+		pAdInfo->MaxPM = 0;
+		pAdInfo->PortSupportType = HD_TYPE_PATA;
+	}else if (pHBA->device == DEVICE_ID_THORLITE_1S1P ){
+		pAdInfo->PortCount = 2;
+		pAdInfo->MaxHD = 1+1*4;
+		pAdInfo->MaxPM = 1;
+		pAdInfo->PortSupportType = HD_TYPE_PATA|HD_TYPE_SATA;
+	}else if ( pHBA->device == DEVICE_ID_THOR_4S1P ||pHBA->device == DEVICE_ID_THOR_4S1P_NEW ){
+		pAdInfo->PortCount = 5;
+		pAdInfo->MaxHD = 1+4*4;
+		pAdInfo->MaxPM = 4;
+		pAdInfo->PortSupportType = HD_TYPE_SATA | HD_TYPE_PATA;
+	}else if ( (pHBA->device == DEVICE_ID_6440) ||
+			   (pHBA->device == DEVICE_ID_6445) ||
+			   (pHBA->device == DEVICE_ID_6340) ){
+		pAdInfo->PortCount = 4;
+		pAdInfo->PortSupportType = HD_TYPE_SATA | HD_TYPE_SAS;
+	}else if ( pHBA->device == DEVICE_ID_6480 ){
+		pAdInfo->PortCount = 8;
+		pAdInfo->PortSupportType = HD_TYPE_SATA | HD_TYPE_SAS;
+	}else if ( pHBA->device == DEVICE_ID_6320 ){
+		pAdInfo->PortCount = 2;
+		pAdInfo->PortSupportType = HD_TYPE_SATA | HD_TYPE_SAS;
+	}else {
+		pAdInfo->PortCount = 5;
+		pAdInfo->PortSupportType = HD_TYPE_SATA | HD_TYPE_PATA;
+	}
+
+#ifdef SIMULATOR
+	pAdInfo->MaxHD = MAX_DEVICE_SUPPORTED_PERFORMANCE;
+	pAdInfo->MaxExpander = 0;
+	pAdInfo->MaxPM = MAX_PM_SUPPORTED;
+#elif defined(ODIN_DRIVER)
+	pAdInfo->MaxHD = pCore->PD_Count_Supported;
+	pAdInfo->MaxExpander = pCore->Expander_Count_Supported;
+	pAdInfo->MaxPM = MAX_PM_SUPPORTED;
+#else
+	pAdInfo->MaxExpander = MAX_EXPANDER_SUPPORTED;
+#endif
+#ifdef RAID_DRIVER
+	raid_capability(This,  pAdInfo);
+#endif	/* RAID_DRIVER */
+
+	pReq->Scsi_Status = REQ_STATUS_SUCCESS;
+}
+
+#endif
+
+
+static void HBA_ModuleSendRequest(MV_PVOID this, PMV_Request req)
+{
+	PHBA_Extension phba = (PHBA_Extension) this;
+
+	switch (req->Cdb[0])
+	{
+	case APICDB0_ADAPTER:
+		if (req->Cdb[1] == APICDB1_ADAPTER_GETINFO)
+			mvGetAdapterInfo(phba, req);
+		else
+			req->Scsi_Status = REQ_STATUS_INVALID_REQUEST;
+
+		req->Completion(req->Cmd_Initiator, req);
+		break;
+	case APICDB0_EVENT:
+		if (req->Cdb[1] == APICDB1_EVENT_GETEVENT)
+			get_event(phba, req);
+		else
+			req->Scsi_Status = REQ_STATUS_INVALID_REQUEST;
+
+		req->Completion(req->Cmd_Initiator, req);
+		break;
+	default:
+		/* submit to child layer */
+		phba->desc->child->ops->module_sendrequest(
+			phba->desc->child->extension,
+			req);
+		break;
+	}
+}
+
+static struct mv_module_ops hba_module_interface = {
+	.module_id              = MODULE_HBA,
+	.get_res_desc           = HBA_ModuleGetResourceQuota,
+	.module_initialize      = HBA_ModuleInitialize,
+	.module_start           = HBA_ModuleStart,
+	.module_stop            = HBA_ModuleShutdown,
+	.module_notification    = HBA_ModuleNotification,
+	.module_sendrequest     = HBA_ModuleSendRequest,
+};
+
+struct mv_module_ops *mv_hba_register_module(void)
+{
+	return &hba_module_interface;
+}
--- /dev/null
+++ b/drivers/scsi/thor/linux/hba_mod.h
@@ -0,0 +1,21 @@
+#ifndef __MODULE_MANAGE_H__
+#define __MODULE_MANAGE_H__
+#include "hba_header.h"
+
+#include "com_mod_mgmt.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+/* will wait for atomic value atomic to become zero until timed out */
+/* return how much 'timeout' is left or 0 if already timed out */
+int __hba_wait_for_atomic_timeout(atomic_t *atomic, unsigned long timeout);
+#endif
+
+int  mv_hba_init(struct pci_dev *dev, MV_U32 max_io);
+void mv_hba_release(struct pci_dev *dev);
+void mv_hba_stop(struct pci_dev *dev);
+int  mv_hba_start(struct pci_dev *dev);
+
+int __mv_get_adapter_count(void);
+struct hba_extension *__mv_get_ext_from_adp_id(int id);
+ void raid_capability( MV_PVOID This, PAdapter_Info pAdInfo);
+#endif /* __MODULE_MANAGE_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/linux/hba_timer.c
@@ -0,0 +1,315 @@
+#include "hba_header.h"
+#include "core_exp.h"
+
+/* how long a time between which should each keeper work be done */
+#define KEEPER_SHIFT (HZ >> 1)
+
+static struct mv_hba_msg_queue mv_msg_queue;
+
+#ifndef SUPPORT_WORKQUEUE
+static struct task_struct *house_keeper_task;
+#endif
+
+static int __msg_queue_state;
+
+static inline int queue_state_get(void)
+{
+	return __msg_queue_state;
+}
+
+static inline void queue_state_set(int state)
+{
+	__msg_queue_state = state;
+}
+
+static void hba_proc_msg(struct mv_hba_msg *pmsg)
+{
+	PHBA_Extension phba;
+	struct scsi_device *psdev;
+
+	/* we don't do things without pmsg->data */
+	if (NULL == pmsg->data)
+		return;
+
+	phba = (PHBA_Extension) pmsg->data;
+
+
+	MV_DBG(DMSG_HBA, "__MV__ In hba_proc_msg.\n");
+
+	MV_ASSERT(pmsg);
+
+	switch (pmsg->msg) {
+	case EVENT_DEVICE_ARRIVAL:
+		if (scsi_add_device(phba->host, 0, pmsg->param, 0))
+			MV_DBG(DMSG_SCSI,
+			       "__MV__ add scsi disk %d-%d-%d failed.\n",
+			       0, pmsg->param, 0);
+		else
+			MV_DBG(DMSG_SCSI,
+			       "__MV__ add scsi disk %d-%d-%d.\n",
+			       0, pmsg->param, 0);
+		break;
+	case EVENT_DEVICE_REMOVAL:
+		psdev = scsi_device_lookup(phba->host, 0, pmsg->param, 0);
+
+		if (NULL != psdev) {
+			MV_DBG(DMSG_SCSI,
+			       "__MV__ remove scsi disk %d-%d-%d.\n",
+			       0, pmsg->param, 0);
+			scsi_remove_device(psdev);
+			scsi_device_put(psdev);
+		} else {
+			MV_DBG(DMSG_SCSI,
+			       "__MV__ no disk to remove %d-%d-%d\n",
+			       0, pmsg->param, 0);
+		}
+		break;
+	case EVENT_HOT_PLUG:
+		sata_hotplug(pmsg->data, pmsg->param);
+		break;
+	default:
+		break;
+	}
+}
+
+static void mv_proc_queue(void)
+{
+	struct mv_hba_msg *pmsg;
+
+	/* work on queue non-stop, pre-empty me! */
+	queue_state_set(MSG_QUEUE_PROC);
+
+	while (1) {
+		MV_DBG(DMSG_HBA, "__MV__ process queue starts.\n");
+		spin_lock_irq(&mv_msg_queue.lock);
+		if (list_empty(&mv_msg_queue.tasks)) {
+			/* it's important we put queue_state_set here. */
+			queue_state_set(MSG_QUEUE_IDLE);
+			spin_unlock_irq(&mv_msg_queue.lock);
+			MV_DBG(DMSG_HBA, "__MV__ process queue ends.\n");
+			break;
+		}
+		pmsg = list_entry(mv_msg_queue.tasks.next, struct mv_hba_msg, msg_list);
+		spin_unlock_irq(&mv_msg_queue.lock);
+
+		hba_proc_msg(pmsg);
+
+		/* clean the pmsg before returning it to free?*/
+		pmsg->data = NULL;
+		spin_lock_irq(&mv_msg_queue.lock);
+		list_move_tail(&pmsg->msg_list, &(mv_msg_queue.free));
+		spin_unlock_irq(&mv_msg_queue.lock);
+		MV_DBG(DMSG_HBA, "__MV__ process queue ends.\n");
+	}
+
+}
+
+static inline MV_U32 hba_msg_queue_empty(void)
+{
+	return list_empty(&(mv_msg_queue.tasks));
+}
+
+#ifndef SUPPORT_WORKQUEUE
+static int hba_house_keeper(void *data)
+{
+	set_user_nice(current, -15);
+
+	set_current_state(TASK_INTERRUPTIBLE);
+#if LINUX_VERSION_CODE >  KERNEL_VERSION(2, 6, 22)
+	set_freezable();
+#endif
+	while (!kthread_should_stop()) {
+		try_to_freeze();
+		if (!hba_msg_queue_empty() &&
+		    MSG_QUEUE_IDLE == queue_state_get()) {
+			set_current_state(TASK_RUNNING);
+			mv_proc_queue();
+		} else {
+			schedule();
+			set_current_state(TASK_INTERRUPTIBLE);
+		}
+	}
+
+	return 0;
+}
+
+#endif
+
+#ifdef SUPPORT_WORKQUEUE
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+static void mv_wq_handler(void *work)
+#else
+static void mv_wq_handler(struct work_struct *work)
+#endif
+{
+	if (hba_msg_queue_empty() ){
+		MV_DBG(DMSG_KERN,"__MV__  msg queue is empty.\n");
+		return;
+	}
+	else if (!hba_msg_queue_empty() &&
+	    MSG_QUEUE_IDLE == queue_state_get()) {
+		MV_DBG(DMSG_KERN,"__MV__  msg queue isn't empty.\n");
+		mv_proc_queue();
+	}
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+static DECLARE_WORK(mv_wq, mv_wq_handler,NULL);
+#else
+static DECLARE_WORK(mv_wq, mv_wq_handler);
+#endif
+
+#endif /* SUPPORT_WORKQUEUE */
+
+
+
+static void hba_msg_queue_init(void)
+{
+	int i;
+
+	spin_lock_init(&mv_msg_queue.lock);
+
+/* as we're in init, there should be no need to hold the spinlock*/
+	INIT_LIST_HEAD(&(mv_msg_queue.free));
+	INIT_LIST_HEAD(&(mv_msg_queue.tasks));
+
+	for (i = 0; i < MSG_QUEUE_DEPTH; i++) {
+		list_add_tail(&mv_msg_queue.msgs[i].msg_list,
+			      &mv_msg_queue.free);
+	}
+
+}
+
+
+void hba_house_keeper_init(void)
+{
+	hba_msg_queue_init();
+
+	queue_state_set(MSG_QUEUE_IDLE);
+#ifndef SUPPORT_WORKQUEUE
+	house_keeper_task = kthread_run(hba_house_keeper, NULL, "399B4F5");
+
+	if (IS_ERR(house_keeper_task)) {
+		printk("Error creating kthread, out of memory?\n");
+		house_keeper_task = NULL;
+	}
+#endif
+}
+
+void hba_house_keeper_run(void)
+{
+	/* hey hey my my */
+}
+
+void hba_house_keeper_exit(void)
+{
+#ifndef SUPPORT_WORKQUEUE
+	if (house_keeper_task)
+		kthread_stop(house_keeper_task);
+	house_keeper_task = NULL;
+#else
+	flush_scheduled_work();
+#endif
+}
+
+void hba_wait_eh()
+{
+	while (!hba_msg_queue_empty()) {
+		schedule();
+	}
+}
+
+
+void hba_msg_insert(void *data, unsigned int msg, unsigned int param)
+{
+	struct mv_hba_msg *pmsg;
+	unsigned long flags;
+
+	MV_DBG(DMSG_HBA, "__MV__ msg insert  %d.\n", msg);
+
+	spin_lock_irqsave(&mv_msg_queue.lock, flags);
+	if (list_empty(&mv_msg_queue.free)) {
+		/* should wreck some havoc ...*/
+		MV_DBG(DMSG_HBA, "-- MV -- Message queue is full.\n");
+		spin_unlock_irqrestore(&mv_msg_queue.lock, flags);
+		return;
+	}
+
+	pmsg = list_entry(mv_msg_queue.free.next, struct mv_hba_msg, msg_list);
+	pmsg->data = data;
+	pmsg->msg  = msg;
+
+	switch (msg) {
+	case EVENT_DEVICE_REMOVAL:
+	case EVENT_DEVICE_ARRIVAL:
+		pmsg->param = param;
+		break;
+	default:
+		pmsg->param = param;
+                /*(NULL==param)?0:*((unsigned int*) param);*/
+		break;
+	}
+
+	list_move_tail(&pmsg->msg_list, &mv_msg_queue.tasks);
+	spin_unlock_irqrestore(&mv_msg_queue.lock, flags);
+
+#ifndef SUPPORT_WORKQUEUE
+	if (house_keeper_task)
+		wake_up_process(house_keeper_task);
+#else
+	schedule_work(&mv_wq);
+#endif
+
+}
+#if 1//def REQUEST_TIMER
+void hba_remove_timer(PMV_Request req)
+{
+	/* should be using del_timer_sync, but ... HBA->lock ... */
+	if ( req->eh_timeout.function ) {
+		del_timer(&req->eh_timeout);
+		req->eh_timeout.function = NULL;
+#ifdef SUPPORT_REQUEST_TIMER
+		req->err_request_ctx = NULL;
+#endif
+	}
+}
+
+/* for req */
+void hba_add_timer(PMV_Request req, int timeout,
+		   MV_VOID (*function)(MV_PVOID data))
+{
+	WARN_ON(timer_pending(&req->eh_timeout));
+	hba_remove_timer(req);
+	req->eh_timeout.data = (unsigned long)req;
+	/* timeout is in unit of second */
+	req->eh_timeout.expires = jiffies + timeout * HZ;
+	req->eh_timeout.function = (void (*)(unsigned long)) function;
+
+	add_timer(&req->eh_timeout);
+	return;
+}
+
+
+void hba_remove_timer_sync(PMV_Request req)
+{
+	/* should be using del_timer_sync, but ... HBA->lock ... */
+	if ( req->eh_timeout.function ) {
+		del_timer_sync(&req->eh_timeout);
+		req->eh_timeout.function = NULL;
+#ifdef SUPPORT_REQUEST_TIMER
+		req->err_request_ctx = NULL;
+#endif
+	}
+}
+void hba_init_timer(PMV_Request req)
+{
+	/*
+	 * as we have no init routine for req, we'll do init_timer every
+	 * time it is used until we could uniformly init. all reqs
+	 */
+	req->eh_timeout.function = NULL;
+	init_timer(&req->eh_timeout);
+}
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/linux/hba_timer.h
@@ -0,0 +1,74 @@
+#ifndef __HBA_TIMER_H__
+#define __HBA_TIMER_H__
+
+#include "hba_header.h"
+
+#define HBA_REQ_TIMER_AFTER_RESET 30
+#define HBA_REQ_TIMER 20
+#define HBA_REQ_TIMER_IOCTL (HBA_REQ_TIMER_AFTER_RESET+3)
+
+#define __hba_init_timer(x) \
+	do {\
+		init_timer(x);\
+		(x)->function = NULL;\
+	} while (0)
+#define __hba_add_timer(x, time, d, func) \
+	do {\
+		(x)->expires = jiffies + time;\
+		(x)->data = (unsigned long)d;\
+		(x)->function = (void (*)(unsigned long))func;\
+		add_timer(x);\
+	} while (0)
+#define __hba_mod_timer(x, time, d, func) \
+	do {\
+		(x)->expires = jiffies + time;\
+		(x)->data = (unsigned long)d;\
+		(x)->function = (void (*)(unsigned long))func;\
+		mod_timer(x, jiffies + time);\
+	} while (0)
+#define __hba_del_timer(x) \
+	do {\
+		if ((x)->function) {\
+			del_timer(x);\
+			(x)->function = NULL;\
+		}\
+	} while (0)
+
+enum _tag_hba_msg_state{
+	MSG_QUEUE_IDLE=0,
+	MSG_QUEUE_PROC
+};
+
+struct mv_hba_msg {
+	MV_PVOID data;
+	MV_U32   msg;
+	MV_U32   param;
+	struct  list_head msg_list;
+};
+
+#define MSG_QUEUE_DEPTH 32
+
+struct mv_hba_msg_queue {
+	spinlock_t lock;
+	struct list_head free;
+	struct list_head tasks;
+	struct mv_hba_msg msgs[MSG_QUEUE_DEPTH];
+};
+
+enum {
+	HBA_TIMER_IDLE = 0,
+	HBA_TIMER_RUNNING,
+	HBA_TIMER_LEAVING,
+};
+void hba_house_keeper_init(void);
+void hba_house_keeper_run(void);
+void hba_house_keeper_exit(void);
+void hba_msg_insert(void *data, unsigned int msg, unsigned int param);
+void hba_init_timer(PMV_Request req);
+void hba_remove_timer(PMV_Request req);
+void hba_remove_timer_sync(PMV_Request req);
+void hba_add_timer(PMV_Request req, int timeout,
+		   MV_VOID (*function)(MV_PVOID data));
+void hba_wait_eh(void);
+
+#endif /* __HBA_TIMER_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/linux/linux_iface.c
@@ -0,0 +1,684 @@
+/*
+ *
+ * linux_iface.c - Kernel/CLI interface
+ *
+ */
+
+#include "hba_header.h"
+#include "hba_mod.h"
+#include "hba_timer.h"
+
+#include "linux_iface.h"
+#include "linux_main.h"
+
+
+#define MV_DEVFS_NAME "mv"
+
+#ifdef RAID_DRIVER
+static int mv_open(struct inode *inode, struct file *file);
+#ifdef HAVE_UNLOCKED_IOCTL
+static long mv_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+#else
+static int mv_ioctl(struct inode *inode, struct file *file, unsigned int cmd,
+		    unsigned long arg);
+#endif /* HAVE_UNLOCKED_IOCTL */
+
+#define IOCTL_BUF_LEN (1024*1024)
+static unsigned char ioctl_buf[IOCTL_BUF_LEN];
+
+static struct file_operations mv_fops = {
+	.owner   =    THIS_MODULE,
+	.open    =    mv_open,
+#ifdef HAVE_UNLOCKED_IOCTL
+	.unlocked_ioctl = mv_ioctl,
+#else
+	.ioctl   =    mv_ioctl,
+#endif
+	.release =    NULL
+};
+#endif /* RAID_DRIVER */
+
+void ioctlcallback(MV_PVOID This, PMV_Request req)
+{
+	struct hba_extension *hba = (struct hba_extension *) This;
+
+	hba->Io_Count--;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_set(&hba->hba_sync, 0);
+#else
+	complete(&hba->cmpl);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+	res_free_req_to_pool(hba->req_pool, req);
+}
+
+#ifdef RAID_DRIVER
+static MV_U16 API2Driver_ID(MV_U16 API_ID)
+{
+	MV_U16	returnID = API_ID;
+	returnID &= 0xfff;
+	return returnID;
+}
+
+static LD_Info ldinfo[LDINFO_NUM] = {{0}};
+static int mv_proc_ld_info(struct Scsi_Host *host)
+{
+	struct hba_extension *hba;
+	PMV_Request req;
+	MV_U8 Cdb[MAX_CDB_SIZE];
+	MV_U16 LD_ID = 0XFF;
+	unsigned long flags;
+
+	Cdb[0] = APICDB0_LD;
+	Cdb[1] = APICDB1_LD_GETINFO;
+	Cdb[2] = LD_ID & 0xff;
+	Cdb[3] = API2Driver_ID(LD_ID)>>8;
+
+	hba = __mv_get_ext_from_host(host);
+	req = res_get_req_from_pool(hba->req_pool);
+	if (req == NULL) {
+		return -1; /*FAIL.*/
+	}
+
+	req->Cmd_Initiator = hba;
+	req->Org_Req = req;
+	req->Device_Id = VIRTUAL_DEVICE_ID;
+	req->Cmd_Flag = 0;
+	req->Req_Type = REQ_TYPE_OS;
+
+	if (SCSI_IS_READ(Cdb[0]))
+		req->Cmd_Flag |= CMD_FLAG_DATA_IN;
+	if (SCSI_IS_READ(Cdb[0]) || SCSI_IS_WRITE(Cdb[0]))
+		req->Cmd_Flag |= CMD_FLAG_DMA;
+
+	req->Data_Transfer_Length = LDINFO_NUM*sizeof(LD_Info);
+	memset(ldinfo, 0, LDINFO_NUM*sizeof(LD_Info));
+	req->Data_Buffer = ldinfo;
+	SGTable_Init(&req->SG_Table, 0);
+	memcpy(req->Cdb, Cdb, MAX_CDB_SIZE);
+	memset(req->Context, 0, sizeof(MV_PVOID)*MAX_POSSIBLE_MODULE_NUMBER);
+
+	req->LBA.value = 0;
+	req->Sector_Count = 0;
+	req->Completion = ioctlcallback;
+	req->Req_Type = REQ_TYPE_INTERNAL;
+
+	spin_lock_irqsave(&hba->desc->hba_desc->global_lock, flags);
+	hba->Io_Count++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_set(&hba->hba_sync, 1);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+	hba->desc->ops->module_sendrequest(hba->desc->extension, req);
+	spin_unlock_irqrestore(&hba->desc->hba_desc->global_lock, flags);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	if ( !__hba_wait_for_atomic_timeout(&hba->hba_sync,
+					    HBA_REQ_TIMER_IOCTL*HZ) ) {
+#else
+	if (wait_for_completion_timeout(&hba->cmpl,
+					HBA_REQ_TIMER_IOCTL*HZ) == 0) {
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+		MV_DBG(DMSG_HBA, "__MV__ ioctl req timed out.\n");
+	        return -1; /*FAIL.*/
+	}
+
+	return 0;/*SUCCESS.*/
+}
+
+
+static char* mv_ld_status(int status)
+{
+	switch (status) {
+	case LD_STATUS_FUNCTIONAL:
+		return "online";
+	case LD_STATUS_DEGRADE:
+		return "degraded";
+	case LD_STATUS_DELETED:
+		return "deleted";
+	case LD_STATUS_PARTIALLYOPTIMAL:
+		return "partially optimal";
+	case LD_STATUS_OFFLINE:
+		return "offline";
+	default:
+		return "unknown";
+	}
+}
+
+static char* mv_ld_raid_mode(int status)
+{
+	switch (status) {
+	case LD_MODE_RAID0:
+		return "RAID0";
+	case LD_MODE_RAID1:
+		return "RAID1";
+	case LD_MODE_RAID10:
+		return "RAID10";
+	case LD_MODE_RAID1E:
+		return "RAID1E";
+	case LD_MODE_RAID5:
+		return "RAID5";
+	case LD_MODE_RAID50:
+		return "RAID50";
+	case LD_MODE_RAID6:
+		return "RAID6";
+	case LD_MODE_RAID60:
+		return "RAID60";
+	case LD_MODE_JBOD:
+		return "JBOD";
+	default:
+		return "unknown";
+	}
+}
+
+static char* mv_ld_bga_status(int status)
+{
+	switch (status) {
+	case LD_BGA_STATE_RUNNING:
+		return "running";
+	case LD_BGA_STATE_ABORTED:
+		return "aborted";
+	case LD_BGA_STATE_PAUSED:
+		return "paused";
+	case LD_BGA_STATE_AUTOPAUSED:
+		return "auto paused";
+	case LD_BGA_STATE_DDF_PENDING:
+		return "DDF pending";
+	default:
+		return "N/A";
+	}
+}
+
+static int mv_ld_get_status(struct Scsi_Host *host, MV_U16 ldid, LD_Status *ldstatus)
+{
+	struct hba_extension *hba;
+	PMV_Request req;
+	MV_U8 Cdb[MAX_CDB_SIZE];
+	MV_U16 LD_ID = ldid;/*0XFF;*/
+	unsigned long flags;
+
+	hba = __mv_get_ext_from_host(host);
+	Cdb[0] = APICDB0_LD;
+	Cdb[1] = APICDB1_LD_GETSTATUS;
+	Cdb[2] = LD_ID & 0xff;
+	Cdb[3] = API2Driver_ID(LD_ID)>>8;
+
+	req = res_get_req_from_pool(hba->req_pool);
+	if (req == NULL)
+		return -1;
+
+	req->Cmd_Initiator = hba;
+	req->Org_Req = req;
+	req->Device_Id = VIRTUAL_DEVICE_ID;
+	req->Cmd_Flag = 0;
+	req->Req_Type = REQ_TYPE_OS;
+
+	if (SCSI_IS_READ(Cdb[0]))
+		req->Cmd_Flag |= CMD_FLAG_DATA_IN;
+	if ( SCSI_IS_READ(Cdb[0]) || SCSI_IS_WRITE(Cdb[0]) )
+		req->Cmd_Flag |= CMD_FLAG_DMA;
+
+	/* Data Buffer */
+	req->Data_Transfer_Length = sizeof(LD_Status);
+	memset(ldstatus, 0, sizeof(LD_Status));
+	ldstatus->Status = LD_STATUS_INVALID;
+	req->Data_Buffer = ldstatus;
+
+	SGTable_Init(&req->SG_Table, 0);
+	memcpy(req->Cdb, Cdb, MAX_CDB_SIZE);
+	memset(req->Context, 0, sizeof(MV_PVOID)*MAX_POSSIBLE_MODULE_NUMBER);
+	req->LBA.value = 0;
+	req->Sector_Count = 0;
+	req->Completion = ioctlcallback;
+	req->Req_Type = REQ_TYPE_INTERNAL;
+	req->Scsi_Status = REQ_STATUS_PENDING;
+
+	spin_lock_irqsave(&hba->desc->hba_desc->global_lock, flags);
+	hba->Io_Count++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_set(&hba->hba_sync, 1);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+	hba->desc->ops->module_sendrequest(hba->desc->extension, req);
+	spin_unlock_irqrestore(&hba->desc->hba_desc->global_lock, flags);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	if (!__hba_wait_for_atomic_timeout(&hba->hba_sync,
+					    HBA_REQ_TIMER_IOCTL*HZ)) {
+#else
+	if (!wait_for_completion_timeout(&hba->cmpl,
+					  HBA_REQ_TIMER_IOCTL*HZ)) {
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+		MV_DBG(DMSG_HBA, "__MV__ ioctl req timed out.\n");
+	        return -1; /*FAIL.*/
+	}
+
+	return 0;/*SUCCESS.*/
+}
+
+static int mv_ld_show_status(char *buf, PLD_Status pld_status)
+{
+	char *str, *str1;
+	int ret = 0;
+
+	if ( LD_BGA_STATE_RUNNING == pld_status->BgaState)
+	{
+		if (LD_BGA_REBUILD == pld_status->Bga)
+			str = "rebuilding";
+		else if (LD_BGA_INIT_QUICK == pld_status->Bga ||
+		          LD_BGA_INIT_BACK == pld_status->Bga)
+			str = "initializing";
+		else if (LD_BGA_CONSISTENCY_CHECK == pld_status->Bga ||
+		          LD_BGA_CONSISTENCY_FIX == pld_status->Bga)
+			str = "synchronizing";
+		else if (LD_BGA_MIGRATION == pld_status->Bga)
+			str = "migration";
+		else
+			str = "unknown bga action";
+		ret = sprintf(buf, "  %s is %d%% done", str, pld_status->BgaPercentage);
+	}
+	else if ((LD_BGA_STATE_ABORTED == pld_status->BgaState) ||
+	         (LD_BGA_STATE_PAUSED == pld_status->BgaState) ||
+	         (LD_BGA_STATE_AUTOPAUSED == pld_status->BgaState))
+	{
+		if (LD_BGA_REBUILD == pld_status->Bga)
+			str = "rebuilding";
+		else if (LD_BGA_INIT_QUICK == pld_status->Bga ||
+		         LD_BGA_INIT_BACK == pld_status->Bga)
+			str = "initializing";
+		else if (LD_BGA_CONSISTENCY_CHECK == pld_status->Bga ||
+		          LD_BGA_CONSISTENCY_FIX == pld_status->Bga)
+			str = "synchronizing";
+		else if (LD_BGA_MIGRATION == pld_status->Bga)
+			str = "migration";
+		else
+			str = "unknown bga action";
+
+		if (LD_BGA_STATE_ABORTED == pld_status->BgaState)
+			str1 = "aborted";
+		else if (LD_BGA_STATE_PAUSED == pld_status->BgaState)
+			str1 = "paused";
+		else if (LD_BGA_STATE_AUTOPAUSED == pld_status->BgaState)
+			str1 = "auto paused";
+		else
+			str1 = "aborted";
+		ret = sprintf(buf, "  %s is %s", str, str1);
+	}
+	return ret;
+}
+#endif /*RAID_DRIVER*/
+
+int mv_linux_proc_info(struct Scsi_Host *pSHost, char *pBuffer,
+		       char **ppStart,off_t offset, int length, int inout)
+{
+	int len = 0;
+	int datalen = 0;/*use as a temp flag.*/
+#ifdef RAID_DRIVER
+	int i = 0;
+	int j = 0;
+	int ret = -1;
+	LD_Status ld_status;
+	char *tmp = NULL;
+	int tmplen = 0;
+#endif
+	if (!pSHost || !pBuffer)
+		return (-ENOSYS);
+
+	if (inout == 1) {
+		/* User write is not supported. */
+		return (-ENOSYS);
+	}
+
+	len = sprintf(pBuffer,"Marvell %s Driver , Version %s\n",
+		      mv_product_name, mv_version_linux);
+
+#ifdef RAID_DRIVER
+	if (mv_proc_ld_info(pSHost) == -1) {
+		len = sprintf(pBuffer,
+			      "Driver is busy, please try later.\n");
+		goto out;
+	} else {
+		for (i = 0; i < MAX_LD_SUPPORTED_PERFORMANCE; i++) {
+			if (ldinfo[i].Status != LD_STATUS_INVALID) {
+				if (ldinfo[i].Status == LD_STATUS_OFFLINE
+				        && ldinfo[i].BGAStatus == LD_BGA_STATE_RUNNING) {
+					ldinfo[i].BGAStatus = LD_BGA_STATE_AUTOPAUSED;
+				}
+				if (ldinfo[i].Status == LD_STATUS_MISSING) {
+					ldinfo[i].Status = LD_STATUS_OFFLINE;
+				}
+			} else {
+				break;
+			}
+		}
+	}
+
+	len += sprintf(pBuffer+len,"Index RAID\tStatus  \t\tBGA Status\n");
+	for (i = 0 ; i < LDINFO_NUM ; i++) {
+		if (ldinfo[i].Size.value == 0) {
+			if (i == 0) {
+				len += sprintf(pBuffer+len,"NO Logical Disk\n");
+			}
+			break;
+		}
+
+		len += sprintf(pBuffer+len,
+			"%-5d %s\t%s",
+			ldinfo[i].ID,
+			mv_ld_raid_mode(ldinfo[i].RaidMode),
+			mv_ld_status(ldinfo[i].Status)
+			);
+
+		tmplen = 24 -strlen(mv_ld_status(ldinfo[i].Status));
+		while (j < tmplen) {
+			len += sprintf(pBuffer+len, "%s", " ");
+			j++;
+		}
+		j = 0;
+
+		len += sprintf(pBuffer+len, "%s", mv_ld_bga_status(ldinfo[i].BGAStatus));
+
+		if (ldinfo[i].BGAStatus != LD_BGA_STATE_NONE) {
+			ret = mv_ld_get_status(pSHost,ldinfo[i].ID,&ld_status);
+			if (ret == 0) {
+				if (ld_status.Status != LD_STATUS_INVALID) {
+					if (ld_status.Status == LD_STATUS_MISSING)
+						ld_status.Status = LD_STATUS_OFFLINE;
+					ld_status.BgaState = ldinfo[i].BGAStatus;
+				}
+				len += mv_ld_show_status(pBuffer+len,&ld_status);
+				ret = -1;
+			}
+		}
+
+		tmp = NULL;
+		tmplen = 0;
+		len += sprintf(pBuffer+len,"\n");
+	}
+
+out:
+#endif
+
+	datalen = len - offset;
+	if (datalen < 0) {
+		datalen = 0;
+		*ppStart = pBuffer + len;
+	} else {
+		*ppStart = pBuffer + offset;
+	}
+	return datalen;
+}
+
+/*
+ *Character Device Interface.
+ */
+#ifdef RAID_DRIVER
+static int mv_open(struct inode *inode, struct file *file)
+{
+	unsigned int minor_number;
+	int retval = -ENODEV;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&inode->i_lock, flags);
+	minor_number = MINOR(inode->i_rdev);
+	if (minor_number >= __mv_get_adapter_count()) {
+		printk("MV : No such device.\n");
+		goto out;
+	}
+	retval = 0;
+out:
+	spin_unlock_irqrestore(&inode->i_lock, flags);
+	return retval;
+}
+
+static dev_t _mv_major = 0;
+
+int mv_register_chdev(struct hba_extension *hba)
+{
+	int ret = 0;
+	dev_t num;
+
+	/*
+	 * look for already-allocated major number first, if not found or
+	 * fail to register, try allocate one .
+	 *
+	 */
+	if (_mv_major)
+	{
+		ret = register_chrdev_region(MKDEV(_mv_major,
+						   hba->desc->hba_desc->id),
+					     1,
+					     MV_DEVFS_NAME);
+		num = MKDEV(_mv_major, hba->desc->hba_desc->id);
+	}
+	if(ret)
+	{
+		MV_DBG(DMSG_KERN, "registered chrdev (%d, %d) failed.\n",
+		_mv_major, hba->desc->hba_desc->id);
+		return	ret;
+	}
+	if (ret || !_mv_major) {
+		ret = alloc_chrdev_region(&num,
+					  hba->desc->hba_desc->id,
+					  1,
+					  MV_DEVFS_NAME);
+		if (ret)
+		{
+			MV_DBG(DMSG_KERN, "allocate chrdev (%d, %d) failed.\n",
+			MAJOR(num), hba->desc->hba_desc->id);
+			return ret;
+		}
+		else
+			_mv_major = MAJOR(num);
+	}
+
+	memset(&hba->cdev, 0, sizeof(struct cdev));
+	cdev_init(&hba->cdev, &mv_fops);
+	hba->cdev.owner = THIS_MODULE;
+	hba->desc->hba_desc->dev_no = num;
+	MV_DBG(DMSG_KERN, "registered chrdev (%d, %d).\n",
+	       MAJOR(num), MINOR(num));
+	ret = cdev_add(&hba->cdev, num, 1);
+
+	return ret;
+}
+
+void mv_unregister_chdev(struct hba_extension *hba)
+{
+	cdev_del(&hba->cdev);
+	unregister_chrdev_region(hba->desc->hba_desc->dev_no, 1);
+}
+
+#ifdef HAVE_UNLOCKED_IOCTL
+static long mv_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+#else
+static int mv_ioctl(struct inode *inode, struct file *file, unsigned int cmd,
+		    unsigned long arg)
+#endif /* HAVE_UNLOCKED_IOCTL */
+{
+	struct hba_extension	*hba;
+	PMV_Request    req = NULL;
+	int error = 0;
+	int ret   = 0;
+	int sptdwb_size = sizeof(SCSI_PASS_THROUGH_DIRECT_WITH_BUFFER);
+	int console_id  = VIRTUAL_DEVICE_ID;
+	unsigned long flags;
+	PSCSI_PASS_THROUGH_DIRECT_WITH_BUFFER psptdwb = NULL;
+	int mv_device_count;
+	struct scsi_idlun idlun;
+
+#ifdef HAVE_UNLOCKED_IOCTL
+	hba = container_of(file->f_dentry->d_inode->i_cdev,
+			   struct hba_extension, cdev);
+#else
+	hba = container_of(inode->i_cdev,
+			   struct hba_extension, cdev);
+#endif /* HAVE_UNLOCKED_IOCTL */
+	/*if the driver is shutdown ,any process shouldn't call mv_ioctl*/
+	if(hba != NULL) {
+		if(DRIVER_STATUS_SHUTDOWN== hba->State ) {
+			MV_DBG(DMSG_IOCTL, "Marvell : Driver had been rmmoded ,Invalid operation.\n");
+			return -EPERM;
+		}
+	} else {
+		MV_DBG(DMSG_IOCTL, "Marvell : Driver is not installed ,Invalid operation.\n");
+		return -EPERM;
+	}
+	mv_device_count =  __mv_get_adapter_count();
+		if (cmd >= API_IOCTL_MAX) {
+			MV_DBG(DMSG_IOCTL, "Marvell : Invalid ioctl command.\n");
+			return -EBADF;
+	}
+
+	if (cmd == API_IOCTL_GET_VIRTURL_ID) {
+		if (copy_to_user(((PSCSI_PASS_THROUGH_DIRECT_WITH_BUFFER) arg)->sptd.DataBuffer,
+			(void *)&console_id,sizeof(int)) != 0 ) {
+			MV_DBG(DMSG_IOCTL, "Marvell : Get VIRTUAL_DEVICE_ID Error.\n");
+			return -EIO;
+		}
+		return 0;
+	}
+
+	if (cmd == API_IOCTL_GET_HBA_COUNT) {
+		if (copy_to_user(((PSCSI_PASS_THROUGH_DIRECT_WITH_BUFFER) arg)->sptd.DataBuffer,
+					(void *)&mv_device_count,sizeof(unsigned int)) != 0) {
+			MV_DBG(DMSG_IOCTL, "Marvell : Get Device Number Error.\n");
+			return -EIO;
+		}
+		return 0;
+	}
+
+	if (cmd == API_IOCTL_LOOKUP_DEV) {
+		if( copy_from_user(&idlun, ((PSCSI_PASS_THROUGH_DIRECT_WITH_BUFFER) arg)->sptd.DataBuffer,
+					sizeof(struct scsi_idlun)) !=0) {
+			MV_DBG(DMSG_IOCTL, "Marvell : Get Device idlun Error.\n");
+			return -EIO;
+		}
+
+		/*To check host no, if fail, return EFAULT (Bad address) */
+		if (hba->host->host_no != ((idlun.dev_id) >> 24)) {
+			MV_DBG(DMSG_IOCTL, "Marvell : lookup device host number error .\n");
+			return EFAULT;
+		}
+		return 0;
+	}
+
+
+	psptdwb = kmalloc(sptdwb_size, GFP_ATOMIC);
+
+	if ( NULL == psptdwb )
+		return -ENOMEM;
+
+	error = copy_from_user(psptdwb, (void *)arg, sptdwb_size);
+
+	if (error) {
+		ret = -EIO;
+		goto clean_psp;
+	}
+
+	if (psptdwb->sptd.DataTransferLength) {
+		if (psptdwb->sptd.DataTransferLength > IOCTL_BUF_LEN) {
+			MV_DBG(DMSG_IOCTL, "__MV__ not enough buf space.\n");
+			ret = -ENOMEM;
+			goto clean_psp;
+		}
+
+		psptdwb->sptd.DataBuffer = ioctl_buf;
+		memset(ioctl_buf, 0, psptdwb->sptd.DataTransferLength);
+
+		error = copy_from_user( psptdwb->sptd.DataBuffer,
+					((PSCSI_PASS_THROUGH_DIRECT_WITH_BUFFER)arg)->sptd.DataBuffer,
+					psptdwb->sptd.DataTransferLength);
+		if (error) {
+			ret = -EIO;
+			goto clean_pspbuf;
+		}
+	} else {
+		psptdwb->sptd.DataBuffer = NULL;
+	}
+
+	/*Translate request to MV_REQUEST*/
+	req = res_get_req_from_pool(hba->req_pool);
+	if (NULL == req) {
+		ret = -ENOMEM;
+		goto clean_pspbuf;
+	}
+
+	req->Cmd_Initiator = hba;
+	req->Org_Req = req;
+	req->Scsi_Status = psptdwb->sptd.ScsiStatus;
+	req->Device_Id = psptdwb->sptd.TargetId;
+	req->Cmd_Flag = 0;
+	req->Req_Type = REQ_TYPE_INTERNAL;
+
+	if (psptdwb->sptd.DataTransferLength == 0) {
+		req->Cmd_Flag |= CMD_FLAG_NON_DATA;
+	} else {
+		if (SCSI_IS_READ(psptdwb->sptd.Cdb[0]))
+			req->Cmd_Flag |= CMD_FLAG_DATA_IN;
+		if (SCSI_IS_READ(psptdwb->sptd.Cdb[0]) || SCSI_IS_WRITE(psptdwb->sptd.Cdb[0]))
+			req->Cmd_Flag |= CMD_FLAG_DMA;
+	}
+
+	req->Data_Transfer_Length = psptdwb->sptd.DataTransferLength;
+	req->Data_Buffer = psptdwb->sptd.DataBuffer;
+	req->Sense_Info_Buffer = psptdwb->Sense_Buffer;
+
+	SGTable_Init(&req->SG_Table, 0);
+
+	memcpy(req->Cdb, psptdwb->sptd.Cdb, MAX_CDB_SIZE);
+	memset(req->Context, 0, sizeof(MV_PVOID)*MAX_POSSIBLE_MODULE_NUMBER);
+	req->LBA.value = 0;
+	req->Sector_Count = 0;
+	req->Completion = ioctlcallback;
+
+	spin_lock_irqsave(&hba->desc->hba_desc->global_lock, flags);
+	hba->Io_Count++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	atomic_set(&hba->hba_sync, 1);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+	hba->desc->ops->module_sendrequest(hba->desc->extension, req);
+	spin_unlock_irqrestore(&hba->desc->hba_desc->global_lock, flags);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11)
+	if (!__hba_wait_for_atomic_timeout(&hba->hba_sync,
+					    HBA_REQ_TIMER_IOCTL*HZ)) {
+#else
+	if (!wait_for_completion_timeout(&hba->cmpl,
+					  HBA_REQ_TIMER_IOCTL*HZ)) {
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 11) */
+		MV_DBG(DMSG_HBA, "__MV__ ioctl req timed out.\n");
+	        ret = -EIO;
+	        goto clean_pspbuf;
+	}
+
+	if (psptdwb->sptd.DataTransferLength) {
+		error = copy_to_user(((PSCSI_PASS_THROUGH_DIRECT_WITH_BUFFER)arg)->sptd.DataBuffer,
+				     psptdwb->sptd.DataBuffer,
+				     psptdwb->sptd.DataTransferLength);
+		if (error) {
+			ret = -EIO;
+			goto clean_pspbuf;
+		}
+	}
+
+	error = copy_to_user((void*)arg, psptdwb, sptdwb_size);
+
+	if (error) {
+		ret = -EIO;
+		goto clean_pspbuf;
+	}
+
+	if (req->Scsi_Status)
+		ret = req->Scsi_Status;
+
+	if (req->Scsi_Status == REQ_STATUS_INVALID_PARAMETER)
+		ret = -EPERM;
+
+clean_pspbuf:
+/* use static buf instead.*/
+clean_psp:
+	kfree(psptdwb);
+	return ret;
+}
+#else  /* RAID_DRIVER */
+inline int mv_register_chdev(struct hba_extension *hba) {return 0;}
+inline void mv_unregister_chdev(struct hba_extension *hba) {}
+#endif /* RAID_DRIVER */
--- /dev/null
+++ b/drivers/scsi/thor/linux/linux_iface.h
@@ -0,0 +1,56 @@
+/*
+ *
+ *  Kernel/CLI interface
+ *
+ */
+
+#ifndef __MV_HBA_LINUX_INTERFACE__
+#define __MV_HBA_LINUX_INTERFACE__
+
+#include "com_define.h"
+#include "com_struct.h"
+#include "com_api.h"
+#include "com_scsi.h"
+#include "com_util.h"
+
+/*Request Structure.*/
+#define SENSE_INFO_BUFFER_SIZE		32
+#define MAX_COMMAND_SIZE		16
+
+/* For Character Device Interface */
+#define MV_DEVICE_MAX_SLOT 4
+
+#define LDINFO_NUM (MAX_LD_SUPPORTED_PERFORMANCE * MAX_NUM_ADAPTERS)
+#define HDINFO_NUM (MAX_DEVICE_SUPPORTED_PERFORMANCE * MAX_NUM_ADAPTERS)
+
+typedef struct _SCSI_PASS_THROUGH_DIRECT {
+	unsigned short Length;
+	unsigned char  ScsiStatus;
+	unsigned char  PathId;
+	unsigned char  TargetId;
+	unsigned char  Lun;
+	unsigned char  CdbLength;
+	unsigned char  SenseInfoLength;
+	unsigned char  DataIn;
+	unsigned long  DataTransferLength;
+	unsigned long  TimeOutValue;
+	void           *DataBuffer;
+	unsigned long  SenseInfoOffset;
+	unsigned char  Cdb[16];
+}SCSI_PASS_THROUGH_DIRECT, *PSCSI_PASS_THROUGH_DIRECT;
+
+typedef struct _SCSI_PASS_THROUGH_DIRECT_WITH_BUFFER{
+	SCSI_PASS_THROUGH_DIRECT        sptd;
+	unsigned long                   Filler;
+	unsigned char                   Sense_Buffer[SENSE_INFO_BUFFER_SIZE];
+}SCSI_PASS_THROUGH_DIRECT_WITH_BUFFER, *PSCSI_PASS_THROUGH_DIRECT_WITH_BUFFER;
+
+int mv_linux_proc_info(struct Scsi_Host *pSHost, char *pBuffer,
+		       char **ppStart,  off_t offset, int length, int inout);
+
+void IOHBARequestCallback(MV_PVOID This, PMV_Request pReq);
+
+int mv_register_chdev(struct hba_extension *hba);
+void mv_unregister_chdev(struct hba_extension *hba);
+
+#endif /* ifndef __MV_HBA_LINUX_INTERFACE__ */
--- /dev/null
+++ b/drivers/scsi/thor/linux/linux_main.c
@@ -0,0 +1,307 @@
+/*
+ *
+ *  Copyright (C) 2006 Marvell Technology Group Ltd. All Rights Reserved.
+ *  linux_main.c
+ *
+ *
+ */
+
+#include "mv_include.h"
+#include "hba_header.h"
+#include "linux_main.h"
+
+#include "hba_exp.h"
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 7)
+unsigned int mv_dbg_opts = 0;
+module_param(mv_dbg_opts, uint, S_IRWXU | S_IRWXG);
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 7) */
+
+static const struct pci_device_id mv_pci_ids[] = {
+#ifdef ODIN_DRIVER
+	{PCI_DEVICE(VENDOR_ID, DEVICE_ID_6320)},
+	{PCI_DEVICE(VENDOR_ID, DEVICE_ID_6340)},
+	{PCI_DEVICE(VENDOR_ID, DEVICE_ID_6440)},
+	{PCI_DEVICE(VENDOR_ID, DEVICE_ID_6480)},
+	{0}
+#endif
+
+#ifdef THOR_DRIVER
+	{PCI_DEVICE(VENDOR_ID, DEVICE_ID_THORLITE_0S1P)},
+	{PCI_DEVICE(VENDOR_ID, DEVICE_ID_THORLITE_2S1P)},
+	{PCI_DEVICE(VENDOR_ID, DEVICE_ID_THOR_4S1P)},
+	{PCI_DEVICE(VENDOR_ID, DEVICE_ID_THOR_4S1P_NEW)},
+	{PCI_DEVICE(VENDOR_ID,
+	           DEVICE_ID_THORLITE_2S1P_WITH_FLASH)},
+	{0}
+#endif
+
+};
+
+/* notifier block to get notified on system shutdown/halt/reboot/down */
+static int mv_linux_halt(struct notifier_block *nb, unsigned long event,
+			 void *buf)
+{
+	switch (event) {
+	case SYS_RESTART:
+	case SYS_HALT:
+	case SYS_POWER_OFF:
+		mv_hba_stop(NULL);
+		break;
+	default:
+		break;
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block mv_linux_notifier = {
+	mv_linux_halt, NULL, 0
+};
+
+#define Vendor_Unique_Register_2 0x44
+
+static int mv_probe(struct pci_dev *dev, const struct pci_device_id *id)
+{
+	unsigned int ret = PCIBIOS_SUCCESSFUL;
+	int err = 0;
+	unsigned char reg44;
+
+	ret = pci_enable_device(dev);
+	if (ret) {
+		MV_PRINT(" enable device failed.\n");
+		return ret;
+	}
+
+	ret = pci_request_regions(dev, mv_driver_name);
+	if (ret)
+		goto err_req_region;
+
+
+	if ( !pci_set_dma_mask(dev, MV_DMA_BIT_MASK_64) ) {
+		ret = pci_set_consistent_dma_mask(dev, MV_DMA_BIT_MASK_64);
+		if (ret) {
+			ret = pci_set_consistent_dma_mask(dev,
+							  MV_DMA_BIT_MASK_32);
+			if (ret)
+				goto err_dma_mask;
+		}
+	} else {
+		ret = pci_set_dma_mask(dev, MV_DMA_BIT_MASK_32);
+		if (ret)
+			goto err_dma_mask;
+
+		ret = pci_set_consistent_dma_mask(dev, MV_DMA_BIT_MASK_32);
+		if (ret)
+			goto err_dma_mask;
+
+	}
+
+	pci_set_master(dev);
+
+	pci_read_config_byte(dev,Vendor_Unique_Register_2,&reg44);
+	reg44 |= MV_BIT(6); /* oscillation 10k HZ */
+	pci_write_config_byte(dev,Vendor_Unique_Register_2,reg44);
+
+	printk("Marvell Storage Controller is found, using IRQ %d.\n",
+	       dev->irq);
+
+	ret = mv_hba_init(dev, MV_MAX_IO);
+	if (ret) {
+		MV_DBG(DMSG_HBA, "Error no %d.\n", ret);
+		ret = -ENOMEM;
+		goto err_dma_mask;
+	}
+
+	MV_DPRINT(("Start mv_hba_start.\n"));
+
+	if (mv_hba_start(dev)) {
+		ret = -ENODEV;
+		goto err_mod_start;
+	}
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,12))
+	if (__mv_get_adapter_count() == 1) {
+		register_reboot_notifier(&mv_linux_notifier);
+	}
+#endif
+	MV_DPRINT(("Finished mv_probe.\n"));
+
+	return 0;
+err_mod_start:
+	err++;
+	mv_hba_stop(dev);
+	mv_hba_release(dev);
+err_dma_mask:
+	err++;
+	pci_release_regions(dev);
+err_req_region:
+	err++;
+	pci_disable_device(dev);
+
+	MV_PRINT(" error counter %d.\n", err);
+	return ret;
+}
+
+static void __devexit mv_remove(struct pci_dev *dev)
+{
+	mv_hba_stop(dev);
+	mv_hba_release(dev);
+
+	pci_release_regions(dev);
+	pci_disable_device(dev);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,12))
+	if (__mv_get_adapter_count() == 0) {
+		unregister_reboot_notifier(&mv_linux_notifier);
+	}
+#endif
+	MV_DPRINT(("Marvell linux driver removed !\n"));
+}
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,12))
+static void mv_shutdown(struct pci_dev *pdev)
+{
+	MV_DPRINT(("%s\n",__func__));
+	mv_hba_stop(pdev);
+}
+#endif
+
+#ifdef CONFIG_PM
+
+extern struct mv_mod_desc *
+__get_lowest_module(struct mv_adp_desc *hba_desc);
+extern int core_suspend(void *ext);
+extern int core_resume(void *ext);
+//extern struct _cache_engine* gCache;;
+
+static int mv_suspend(struct pci_dev *pdev, pm_message_t state)
+{
+	struct hba_extension *ext;
+	struct mv_mod_desc *core_mod,*hba_mod = pci_get_drvdata(pdev);
+	struct mv_adp_desc *ioc = hba_mod->hba_desc;
+	int tmp;
+
+	MV_DPRINT(("start  mv_suspend.\n"));
+
+	ext = (struct hba_extension *)hba_mod->extension;
+	core_mod = __get_lowest_module(ioc);
+
+	core_suspend(core_mod->extension);
+
+	free_irq(ext->dev->irq,ext);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,11))
+	pci_save_state(pdev);
+#else
+	pci_save_state(pdev,ioc->pci_config_space);
+#endif
+	pci_disable_device(pdev);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,11))
+	pci_set_power_state(pdev,pci_choose_state(pdev,state));
+#else
+	pci_set_power_state(pdev,state);
+#endif
+
+	return 0;
+}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
+extern irqreturn_t mv_intr_handler(int irq, void *dev_id, struct pt_regs *regs);
+#else
+extern irqreturn_t mv_intr_handler(int irq, void *dev_id);
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19) */
+
+static int mv_resume (struct pci_dev *pdev)
+{
+	int ret;
+	struct hba_extension *ext;
+	struct mv_mod_desc *core_mod,*hba_mod = pci_get_drvdata(pdev);
+	struct mv_adp_desc *ioc = hba_mod->hba_desc;
+
+	ext = (struct hba_extension *)hba_mod->extension;
+	core_mod = __get_lowest_module(ioc);
+	MV_DPRINT(("start  mv_resume.\n"));
+
+	pci_set_power_state(pdev, PCI_D0);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,11))
+	pci_enable_wake(pdev, PCI_D0, 0);
+	pci_restore_state(pdev);
+#else
+	pci_restore_state(pdev,ioc->pci_config_space);
+#endif
+	pci_set_master(pdev);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
+	ret = request_irq(ext->dev->irq, mv_intr_handler, IRQF_SHARED,
+			  mv_driver_name, ext);
+#else
+	ret = request_irq(ext->dev->irq, mv_intr_handler, SA_SHIRQ,
+			  mv_driver_name, ext);
+#endif
+	if (ret < 0) {
+	        MV_PRINT("request IRQ failed.\n");
+	        return -1;
+	}
+	if (core_resume(core_mod->extension)) {
+		MV_PRINT("mv_resume_core failed.\n");
+		return -1;
+	}
+
+	return 0;
+}
+#endif
+
+static struct pci_driver mv_pci_driver = {
+	.name     = "mv_"mv_driver_name,
+	.id_table = mv_pci_ids,
+	.probe    = mv_probe,
+	.remove   = __devexit_p(mv_remove),
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,12))
+	.shutdown = mv_shutdown,
+#endif
+#ifdef CONFIG_PM
+	.suspend=mv_suspend,
+	.resume=mv_resume,
+#endif
+};
+
+static int __init mv_linux_driver_init(void)
+{
+        /* default to show no msg */
+#ifdef __MV_DEBUG__
+	//mv_dbg_opts |= (DMSG_CORE|DMSG_HBA|DMSG_KERN|DMSG_SCSI|DMSG_FREQ);
+	mv_dbg_opts |= (DMSG_CORE|DMSG_HBA|DMSG_KERN|DMSG_SCSI);
+	mv_dbg_opts |= (DMSG_CORE_EH|DMSG_RAID|DMSG_SAS|DMSG_RES);
+
+	//mv_dbg_opts |= DMSG_PROF_FREQ;
+	//mv_dbg_opts |= DMSG_SCSI_FREQ;
+
+#endif /* __MV_DEBUG__ */
+	hba_house_keeper_init();
+
+	return pci_register_driver(&mv_pci_driver);
+}
+
+static void __exit mv_linux_driver_exit(void)
+{
+	pci_unregister_driver(&mv_pci_driver);
+	hba_house_keeper_exit();
+}
+
+MODULE_AUTHOR ("Marvell Technolog Group Ltd.");
+#ifdef ODIN_DRIVER
+MODULE_DESCRIPTION ("ODIN SAS hba driver");
+
+#ifdef RAID_DRIVER
+MODULE_LICENSE("Proprietary");
+#else /* RAID_DRIVER */
+MODULE_LICENSE("GPL");
+#endif /* RAID_DRIVER */
+#endif
+
+#ifdef THOR_DRIVER
+MODULE_DESCRIPTION ("Thor ATA HBA Driver");
+
+MODULE_LICENSE("Proprietary");
+
+#endif
+
+MODULE_DEVICE_TABLE(pci, mv_pci_ids);
+
+module_init(mv_linux_driver_init);
+module_exit(mv_linux_driver_exit);
--- /dev/null
+++ b/drivers/scsi/thor/linux/linux_main.h
@@ -0,0 +1,48 @@
+#ifndef _LINUX_MAIN_H
+#define _LINUX_MAIN_H
+
+#include "hba_header.h"
+
+#define __mv_get_ext_from_host(phost) \
+          (((struct hba_extension **) (phost)->hostdata)[0])
+
+/* for communication with OS/SCSI mid layer only */
+enum {
+#ifdef RAID_DRIVER
+	MV_MAX_REQUEST_DEPTH     = MAX_REQUEST_NUMBER_PERFORMANCE - 2,
+	MV_MAX_IO                = MAX_REQUEST_NUMBER_PERFORMANCE,
+#else /* RAID_DRIVER */
+	MV_MAX_REQUEST_DEPTH     = MAX_CORE_REQUEST_NUMBER_PERFORMANCE - 2,
+	MV_MAX_IO                = MAX_CORE_REQUEST_NUMBER_PERFORMANCE,
+#endif /* RAID_DRIVER */
+	MV_MAX_REQUEST_PER_LUN   = 32,
+	MV_MAX_SG_ENTRY          = 32,
+
+	MV_SHT_USE_CLUSTERING    = DISABLE_CLUSTERING,
+	MV_SHT_EMULATED          = 0,
+	MV_SHT_THIS_ID           = -1,
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,5,0)
+#define mv_scmd_host(cmd)    cmd->device->host
+#define mv_scmd_channel(cmd) cmd->device->channel
+#define mv_scmd_target(cmd)  cmd->device->id
+#define mv_scmd_lun(cmd)     cmd->device->lun
+#else
+#define mv_scmd_host(cmd)    cmd->host
+#define mv_scmd_channel(cmd) cmd->channel
+#define mv_scmd_target(cmd)  cmd->target
+#define mv_scmd_lun(cmd)     cmd->lun
+#endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 24)
+#define MV_DMA_BIT_MASK_64 DMA_64BIT_MASK
+#define MV_DMA_BIT_MASK_32 DMA_32BIT_MASK
+#else
+#define MV_DMA_BIT_MASK_64 DMA_BIT_MASK(64)
+#define MV_DMA_BIT_MASK_32 DMA_BIT_MASK(32)
+#endif
+
+#define LO_BUSADDR(x) ((MV_U32)(x))
+#define HI_BUSADDR(x) (sizeof(BUS_ADDRESS)>4? (u64)(x) >> 32 : 0)
+
+#endif /*_LINUX_MAIN_H*/
--- /dev/null
+++ b/drivers/scsi/thor/linux/mv_os.h
@@ -0,0 +1,208 @@
+#if !defined(LINUX_OS_H)
+#define LINUX_OS_H
+
+#ifndef LINUX_VERSION_CODE
+#   include <linux/version.h>
+#endif
+
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/timer.h>
+#include <linux/time.h>
+#include <linux/reboot.h>
+#include <linux/ioport.h>
+#include <linux/delay.h>
+#include <linux/proc_fs.h>
+#include <linux/stat.h>
+#include <linux/cdev.h>
+#include <linux/spinlock.h>
+#include <linux/pci.h>
+#include <linux/completion.h>
+#include <linux/blkdev.h>
+#include <linux/vmalloc.h>
+#include <linux/kthread.h>
+#include <linux/nmi.h>
+
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <asm/div64.h>
+#if LINUX_VERSION_CODE >  KERNEL_VERSION(2, 6, 19)
+#include <linux/freezer.h>
+#else
+#include <linux/sched.h>
+#endif
+#include <scsi/scsi.h>
+#include <scsi/scsi_cmnd.h>
+#include <scsi/scsi_device.h>
+#include <scsi/scsi_host.h>
+#include <scsi/scsi_transport.h>
+#include <scsi/scsi_ioctl.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 14)
+#include <scsi/scsi_request.h>
+#endif
+
+/* OS specific flags */
+#define  _OS_LINUX            1
+#define _64_BIT_COMPILER      1
+#ifdef USE_NEW_SGTABLE
+#define USE_NEW_SGVP
+#endif /* USE_NEW_SGTABLE */
+
+#ifdef CONFIG_64BIT
+#   define __KCONF_64BIT__
+#endif /* CONFIG_64BIT */
+
+#if defined(__LITTLE_ENDIAN)
+#   define __MV_LITTLE_ENDIAN__  1
+#elif defined(__BIG_ENDIAN)
+#   define __MV_BIG_ENDIAN__     1
+#else
+#   error "error in endianness"
+#endif
+
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+#   define __MV_LITTLE_ENDIAN_BITFIELD__   1
+#elif defined(__BIG_ENDIAN_BITFIELD)
+#   define __MV_BIG_ENDIAN_BITFIELD__      1
+#else
+#   error "error in endianness"
+#endif
+
+#ifdef __MV_DEBUG__
+#   define MV_DEBUG
+#else
+#   ifdef MV_DEBUG
+#      undef MV_DEBUG
+#   endif /* MV_DEBUG */
+#endif /* __MV_DEBUG__ */
+
+#ifndef NULL
+#   define NULL 0
+#endif
+
+/* Values for T10/04-262r7 */
+#ifndef ATA_16
+#	define ATA_16                0x85      /* 16-byte pass-thru */
+#endif
+#ifndef ATA_12
+#	define ATA_12                0xa1      /* 12-byte pass-thru */
+#endif
+
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,11)
+#define PCI_D0 0
+#include <linux/suspend.h>
+typedef u32 pm_message_t;
+
+static inline int try_to_freeze(unsigned long refrigerator_flags)
+{
+        if (unlikely(current->flags & PF_FREEZE)) {
+               refrigerator(refrigerator_flags);
+                return 1;
+        } else
+             return 0;
+}
+#endif
+
+/*
+ *
+ * Primary Data Type Definition
+ *
+ */
+#include "com_define.h"
+
+#define MV_INLINE inline
+#define CDB_INQUIRY_EVPD    1
+
+
+typedef void (*OSSW_TIMER_FUNCTION)(unsigned long);
+typedef  void (*CORE_TIMER_FUNCTION)(void *);
+typedef  void (*OS_TIMER_FUNCTION)(void *);
+
+typedef unsigned long OSSW_TIMER_DATA;
+
+/* OS macro definition */
+#define MV_MAX_TRANSFER_SECTOR  (MV_MAX_TRANSFER_SIZE >> 9)
+
+/*Driver Version for Command Line Interface Query.*/
+#  define VER_MAJOR           1
+
+
+#   define VER_MINOR        2
+#   define VER_BUILD        30
+
+#ifdef __OEM_INTEL__
+#   define VER_OEM          VER_OEM_INTEL
+#elif defined(__OEM__ASUS__)
+#   define VER_OEM          VER_OEM_ASUS
+#else
+#   define VER_OEM         VER_OEM_GENERIC
+#endif /* __OEM_INTEL__ */
+
+
+#ifdef RAID_DRIVER
+#   define VER_TEST         ""
+#else
+#   define VER_TEST         "N"
+#endif /* RAID_DRIVER */
+#ifdef ODIN_DRIVER
+#define mv_driver_name   "mv64xx"
+
+#define mv_product_name  "ODIN"
+#endif
+
+#ifdef THOR_DRIVER
+#define mv_driver_name   "mv61xx"
+
+#define mv_product_name  "THOR"
+#endif
+
+/* call VER_VAR_TO_STRING */
+#define NUM_TO_STRING(num1, num2, num3, num4) #num1"."#num2"."#num3"."#num4
+#define VER_VAR_TO_STRING(major, minor, oem, build) NUM_TO_STRING(major, \
+								  minor, \
+								  oem,   \
+								  build)
+
+#define mv_version_linux   VER_VAR_TO_STRING(VER_MAJOR, VER_MINOR,       \
+					     VER_OEM, VER_BUILD) VER_TEST
+
+#define CPU_TO_LE_16 cpu_to_le16
+#define CPU_TO_LE_32 cpu_to_le32
+#define LE_TO_CPU_16 le16_to_cpu
+#define LE_TO_CPU_32 le32_to_cpu
+
+#ifndef scsi_to_pci_dma_dir
+#define scsi_to_pci_dma_dir(scsi_dir) ((int)(scsi_dir))
+#endif
+
+#ifndef TRUE
+#define TRUE 	1
+#define FALSE	0
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 24)
+#define map_sg_page(sg)		kmap_atomic(sg->page, KM_IRQ0)
+#define map_sg_page_sec(sg)		kmap_atomic(sg->page, KM_IRQ1)
+#else
+#define map_sg_page(sg)		kmap_atomic(sg_page(sg), KM_IRQ0)
+#define map_sg_page_sec(sg)		kmap_atomic(sg_page(sg), KM_IRQ1)
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 25)
+#define mv_use_sg(cmd)	cmd->use_sg
+#define mv_rq_bf(cmd)	cmd->request_buffer
+#define mv_rq_bf_l(cmd)	cmd->request_bufflen
+#else
+#define mv_use_sg(cmd)	scsi_sg_count(cmd)
+#define mv_rq_bf(cmd)	scsi_sglist(cmd)
+#define mv_rq_bf_l(cmd)	scsi_bufflen(cmd)
+#endif
+
+#endif /* LINUX_OS_H */
--- /dev/null
+++ b/drivers/scsi/thor/linux/oss_wrapper.c
@@ -0,0 +1,111 @@
+#include "hba_header.h"
+
+MV_U8 ossw_add_timer(struct timer_list *timer,
+		    u32 msec,
+		    void (*function)(unsigned long),
+		    unsigned long data)
+{
+	u64 jif;
+
+	if(timer_pending(timer))
+		del_timer(timer);
+
+	timer->function = function;
+	timer->data     = data;
+
+	jif = (u64) (msec * HZ);
+	do_div(jif, 1000);         /* wait in unit of second */
+	timer->expires = jiffies + 1 + jif;
+
+	add_timer(timer);
+	return	0;
+}
+
+void ossw_del_timer(struct timer_list *timer)
+{
+	if (timer->function)
+		del_timer(timer);
+	timer->function = NULL;
+}
+
+u32 ossw_get_time_in_sec(void)
+{
+	struct timeval tv;
+
+	do_gettimeofday(&tv);
+	return (u32) tv.tv_sec;
+}
+
+u32 ossw_get_msec_of_time(void)
+{
+	struct timeval tv;
+
+	do_gettimeofday(&tv);
+	return (u32) tv.tv_usec*1000*1000;
+}
+
+#if __LEGACY_OSSW__
+//reset nmi watchdog before delay
+static void mv_touch_nmi_watchdog(void)
+{
+#ifdef CONFIG_X86_64
+	touch_nmi_watchdog();
+#endif /* CONFIG_X86_64*/
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 13)
+	touch_softlockup_watchdog();
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 13) */
+}
+
+void HBA_SleepMillisecond(MV_PVOID ext, MV_U32 msec)
+{
+	MV_U32	tmp=0;
+	MV_U32	mod_msec=2000;
+	if (in_softirq()||in_interrupt() || irqs_disabled()){
+		mv_touch_nmi_watchdog();
+		if (msec<=mod_msec)
+			ossw_mdelay(msec);
+		else
+		{
+			for(tmp=0;tmp<msec/mod_msec;tmp++)
+			{
+				ossw_mdelay(mod_msec);
+				mv_touch_nmi_watchdog();
+			}
+			if (msec%mod_msec)
+				ossw_mdelay(msec%mod_msec);
+		}
+		mv_touch_nmi_watchdog();
+	}else {
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule_timeout( msecs_to_jiffies(msec) );
+	}
+}
+
+
+void hba_msleep(MV_U32 msec)
+{
+	MV_U32	tmp=0;
+	MV_U32	mod_msec=2000;
+	if (in_softirq()||in_interrupt() || irqs_disabled()){
+		mv_touch_nmi_watchdog();
+		if (msec<=mod_msec)
+			ossw_mdelay(msec);
+		else
+		{
+			for(tmp=0;tmp<msec/mod_msec;tmp++)
+			{
+				ossw_mdelay(mod_msec);
+				mv_touch_nmi_watchdog();
+			}
+			if (msec%mod_msec)
+				ossw_mdelay(msec%mod_msec);
+		}
+		mv_touch_nmi_watchdog();
+	}else {
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule_timeout( msecs_to_jiffies(msec));
+	}
+}
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/linux/oss_wrapper.h
@@ -0,0 +1,118 @@
+/*
+ * Operating System Service Wrapper
+ *
+ * Notes :
+ * ** DO NOT include headers other than common & os specific ones
+ *
+ * ** This header is included in mv_os.h, you don't have to include it
+ *        explicitly in your code.
+ *
+ */
+#ifndef __OSS_WRAPPER_H__
+#define __OSS_WRAPPER_H__
+
+#include "com_mod_mgmt.h"
+
+/* Sychronization Services */
+
+
+/* delay in milliseconds */
+MV_U8 ossw_add_timer(struct timer_list *timer,
+		    u32 msec,
+		    void (*function)(unsigned long),
+		    unsigned long data);
+
+void ossw_del_timer(struct timer_list *timer);
+
+/* OS Information Query Services */
+u32 ossw_get_time_in_sec(void);
+u32 ossw_get_msec_of_time(void);
+
+/* MISC Services */
+#define ossw_mdelay(x) mdelay(x)
+#define ossw_udelay(x) udelay(x)
+
+#define MV_PCI_READ_CONFIG_DWORD(ext, offset, ptr) \
+          pci_read_config_dword(ext->desc->hba_desc->dev, offset, ptr)
+
+#define MV_PCI_READ_CONFIG_WORD(ext, offset, ptr) \
+          pci_read_config_word(ext->desc->hba_desc->dev, offset, ptr)
+
+#define MV_PCI_READ_CONFIG_BYTE(ext, offset, ptr) \
+          pci_read_config_byte(ext->desc->hba_desc->dev, offset, ptr)
+
+#define MV_PCI_WRITE_CONFIG_DWORD(ext, offset, val) \
+          pci_write_config_dword(ext->desc->hba_desc->dev, offset, val)
+
+#define MV_PCI_WRITE_CONFIG_WORD(ext, offset, val) \
+          pci_write_config_word(ext->desc->hba_desc->dev, offset, val)
+
+#define MV_PCI_WRITE_CONFIG_BYTE(ext, offset, val) \
+          pci_write_config_byte(ext->desc->hba_desc->dev, offset, val)
+
+/* System dependent macro for flushing CPU write cache */
+#define MV_CPU_WRITE_BUFFER_FLUSH() smp_wmb()
+#define MV_CPU_READ_BUFFER_FLUSH()  smp_rmb()
+#define MV_CPU_BUFFER_FLUSH()       smp_mb()
+
+/* register read write: memory io */
+#define MV_REG_WRITE_BYTE(base, offset, val)    \
+    writeb(val, base + offset)
+#define MV_REG_WRITE_WORD(base, offset, val)    \
+    writew(val, base + offset)
+#define MV_REG_WRITE_DWORD(base, offset, val)    \
+    writel(val, base + offset)
+
+#define MV_REG_READ_BYTE(base, offset)			\
+	readb(base + offset)
+#define MV_REG_READ_WORD(base, offset)			\
+	readw(base + offset)
+#define MV_REG_READ_DWORD(base, offset)			\
+	readl(base + offset)
+
+/* register read write: port io */
+#define MV_IO_WRITE_BYTE(base, offset, val)    \
+    outb(val, (unsigned)(MV_PTR_INTEGER)(base + offset))
+#define MV_IO_WRITE_WORD(base, offset, val)    \
+    outw(val, (unsigned)(MV_PTR_INTEGER)(base + offset))
+#define MV_IO_WRITE_DWORD(base, offset, val)    \
+    outl(val, (unsigned)(MV_PTR_INTEGER)(base + offset))
+
+#define MV_IO_READ_BYTE(base, offset)			\
+	inb((unsigned)(MV_PTR_INTEGER)(base + offset))
+#define MV_IO_READ_WORD(base, offset)			\
+	inw((unsigned)(MV_PTR_INTEGER)(base + offset))
+#define MV_IO_READ_DWORD(base, offset)			\
+	inl((unsigned)(MV_PTR_INTEGER)(base + offset))
+
+#if __LEGACY_OSSW__
+void HBA_SleepMillisecond(MV_PVOID ext, MV_U32 msec);
+
+#   define HBA_SleepMicrosecond(_x, _y) ossw_udelay(_y)
+#   define HBA_GetTimeInSecond          ossw_get_time_in_sec
+#   define HBA_GetMillisecondInDay      ossw_get_msec_of_time
+
+#   define NO_CURRENT_TIMER  0xFF
+
+#   define SmallTimer_AddRequest(_ext, _millsec, _func, _data, _bogus)    \
+             ossw_add_timer(&_ext->timer,                               \
+			     _millsec ,                           \
+			     (OSSW_TIMER_FUNCTION) (_func),         \
+			     (OSSW_TIMER_DATA) (_data))
+
+
+
+#   define Timer_AddRequest(_ext, _sec, _func, _data, _bogus)    \
+             ossw_add_timer(&_ext->timer,                               \
+			     _sec * 1000,                           \
+			     (OSSW_TIMER_FUNCTION) (_func),         \
+			     (OSSW_TIMER_DATA) (_data))
+
+#   define Timer_CancelRequest(_data, _bogus)                     \
+              ossw_del_timer(&_data->timer)
+
+#endif /* __LEGACY_OSSW__ */
+
+void hba_msleep(MV_U32 msec);
+
+#endif /* __OSS_WRAPPER_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/linux/res_mgmt.c
@@ -0,0 +1,201 @@
+#include "hba_header.h"
+#include "res_mgmt.h"
+#include "oss_wrapper.h"
+#include "linux_main.h"
+
+struct mv_request_pool *res_reserve_req_pool(MV_U32 mod_id,
+					     MV_U32 size,
+					     MV_U32 sg_count)
+{
+	int i;
+	unsigned int mem_size;
+	struct mv_request_pool *pool;
+	struct _MV_Request *req;
+	MV_SG_Entry *sg;
+
+	MV_DBG(DMSG_RES, "module %u reserves reqcount=%d, sgcount=%d,request pool of size %lu.\n",
+	       mod_id,size,sg_count,
+	       (unsigned long) (sizeof(struct _MV_Request) * size +
+				sizeof(MV_SG_Entry)* sg_count * size));
+
+	/*
+	 * since we almost always use sgd_t with pctx, we need x2 memory
+	 * than normal sgd_t table
+	 */
+	sg_count *= 2;
+
+	mem_size = sizeof(struct mv_request_pool);
+	pool = vmalloc(mem_size);
+	if (NULL == pool)
+		goto res_err_pool;
+	memset(pool, 0, mem_size);
+
+	/* assuming its size is already 64bit-aligned */
+	mem_size = sizeof(struct _MV_Request) * size;
+	req = vmalloc(mem_size);
+	if (NULL == req)
+		goto res_err_req;
+	memset(req, 0, mem_size);
+
+	mem_size = sizeof(MV_SG_Entry) * sg_count * size;
+	sg  = vmalloc(mem_size);
+	if (NULL == sg)
+		goto res_err_sg;
+	memset(sg, 0, mem_size);
+
+
+
+	INIT_LIST_HEAD(&pool->free_list);
+	INIT_LIST_HEAD(&pool->use_list);
+	OSSW_INIT_SPIN_LOCK(&pool->lock);
+
+	pool->mod_id  = mod_id;
+	pool->size    = size;
+	pool->req_mem = (void *) req;
+	pool->sg_mem = (void *)sg;
+
+	for (i = 0; i < size; i++) {
+		//MV_DBG(DMSG_RES, "req no.%d at %p with sg at %p.\n",
+		 //      i, req, sg);
+
+		req->SG_Table.Entry_Ptr  = sg;
+		MV_ASSERT(sg_count<=255);
+		req->SG_Table.Max_Entry_Count = sg_count;
+		list_add_tail(&req->pool_entry, &pool->free_list);
+		req++;
+		sg += sg_count;
+	}
+#ifdef SEPARATE_IOCTL_REQ
+	INIT_LIST_HEAD(&pool->ioctl_free_list);
+	INIT_LIST_HEAD(&pool->ioctl_use_list);
+	OSSW_INIT_SPIN_LOCK(&pool->ioctl_lock);
+	mem_size = sizeof(struct _MV_Request) * MV_MAX_IOCTL_REQUEST;
+	req = vmalloc(mem_size);
+	if (NULL == req)
+		goto res_err_req;
+	memset(req, 0, mem_size);
+
+	mem_size = sizeof(MV_SG_Entry) * sg_count * MV_MAX_IOCTL_REQUEST;
+	sg  = vmalloc(mem_size);
+	if (NULL == sg)
+		goto res_err_sg;
+	memset(sg, 0, mem_size);
+
+	pool->ioctl_req_mem = (void *) req;
+	pool->ioctl_sg_mem = (void *)sg;
+
+	for (i = 0; i < MV_MAX_IOCTL_REQUEST; i++) {
+		req->SG_Table.Entry_Ptr  = sg;
+		MV_ASSERT(sg_count<=255);
+		req->SG_Table.Max_Entry_Count = sg_count;
+		list_add_tail(&req->pool_entry, &pool->ioctl_free_list);
+		req++;
+		sg += sg_count;
+	}
+
+#endif
+
+	return pool;
+
+res_err_sg:
+	MV_ASSERT(MV_FALSE);
+	vfree(req);
+res_err_req:
+	MV_ASSERT(MV_FALSE);
+	vfree(pool);
+res_err_pool:
+	MV_ASSERT(MV_FALSE);
+	return NULL;
+}
+
+#ifdef SUPPORT_OS_REQ_TIMER
+void mv_init_os_req_timer(struct _MV_Request * req);
+#endif
+
+struct _MV_Request *res_get_req_from_pool(struct mv_request_pool *pool)
+{
+	struct _MV_Request *req;
+	unsigned long flags;
+
+	BUG_ON(pool == NULL);
+	OSSW_SPIN_LOCK_IRQSAVE(&pool->lock, flags);
+	if (list_empty(&pool->free_list)) {
+		res_dump_pool_info(pool);
+		OSSW_SPIN_UNLOCK_IRQRESTORE(&pool->lock, flags);
+		return NULL;
+	}
+
+	req = list_entry(pool->free_list.next, struct _MV_Request, pool_entry);
+	MV_ZeroMvRequest(req); /* FIX: we can do better than this */
+	list_move_tail(pool->free_list.next, &pool->use_list);
+#ifdef SUPPORT_OS_REQ_TIMER
+	mv_init_os_req_timer(req);
+#endif
+	OSSW_SPIN_UNLOCK_IRQRESTORE(&pool->lock, flags);
+
+	return req;
+}
+
+void res_free_req_to_pool(struct mv_request_pool *pool,
+			  struct _MV_Request *req)
+{
+	unsigned long flags;
+
+	BUG_ON((NULL == pool) || (NULL == req));
+	OSSW_SPIN_LOCK_IRQSAVE(&pool->lock, flags);
+	list_move_tail(&req->pool_entry, &pool->free_list);
+	OSSW_SPIN_UNLOCK_IRQRESTORE(&pool->lock, flags);
+}
+
+void res_dump_pool_info(struct mv_request_pool *pool)
+{
+
+}
+
+void res_release_req_pool(struct mv_request_pool *pool)
+{
+	BUG_ON(NULL == pool);
+
+	MV_DBG(DMSG_RES, "module %d release pool at %p.\n",
+	       pool->mod_id, pool);
+	vfree(pool->req_mem);
+	vfree(pool->sg_mem);
+#ifdef SEPARATE_IOCTL_REQ
+	vfree(pool->ioctl_req_mem);
+	vfree(pool->ioctl_sg_mem);
+#endif
+	vfree(pool);
+}
+#ifdef SEPARATE_IOCTL_REQ
+struct _MV_Request *get_ioctl_req_from_pool(struct mv_request_pool *pool)
+{
+	struct _MV_Request *req;
+	unsigned long flags;
+
+	BUG_ON(pool == NULL);
+	OSSW_SPIN_LOCK_IRQSAVE(&pool->ioctl_lock, flags);
+	if (list_empty(&pool->ioctl_free_list)) {
+		res_dump_pool_info(pool);
+		OSSW_SPIN_UNLOCK_IRQRESTORE(&pool->ioctl_lock, flags);
+		return NULL;
+	}
+
+	req = list_entry(pool->ioctl_free_list.next, struct _MV_Request, pool_entry);
+	MV_ZeroMvRequest(req);
+	list_move_tail(pool->ioctl_free_list.next, &pool->ioctl_use_list);
+	OSSW_SPIN_UNLOCK_IRQRESTORE(&pool->ioctl_lock, flags);
+
+	return req;
+}
+
+void free_ioctl_req_to_pool(struct mv_request_pool *pool,struct _MV_Request *req)
+{
+	unsigned long flags;
+
+	BUG_ON((NULL == pool) || (NULL == req));
+	OSSW_SPIN_LOCK_IRQSAVE(&pool->ioctl_lock, flags);
+	list_move_tail(&req->pool_entry, &pool->ioctl_free_list);
+	OSSW_SPIN_UNLOCK_IRQRESTORE(&pool->ioctl_lock, flags);
+}
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/linux/res_mgmt.h
@@ -0,0 +1,47 @@
+#ifndef __RES_MGMT_H__
+#define __RES_MGMT_H__
+
+#include "com_type.h"
+
+struct mv_request_pool {
+	struct list_head free_list;
+	struct list_head use_list;
+#ifdef SEPARATE_IOCTL_REQ
+	struct list_head ioctl_free_list;
+	struct list_head ioctl_use_list;
+	void 		*ioctl_sg_mem;
+	void 		*ioctl_req_mem;
+	spinlock_t  ioctl_lock;
+#endif
+
+	void        *req_mem;  /* starting address of the request mem pool */
+	void 		*sg_mem;
+	spinlock_t  lock;
+	MV_U32      mod_id;
+	MV_U32      size;
+};
+
+/* request structure related
+ * mod_id   : id of the module that is making the request
+ * size     : size of the pool, measured in the number of requests
+ * sg_count : scatter gather list entries supported by each request
+ */
+struct mv_request_pool *res_reserve_req_pool(MV_U32 mod_id,
+					     MV_U32 size,
+					     MV_U32 sg_count);
+
+struct _MV_Request *res_get_req_from_pool(struct mv_request_pool *pool);
+
+void res_free_req_to_pool(struct mv_request_pool *pool,
+			  struct _MV_Request *req);
+
+void res_release_req_pool(struct mv_request_pool *pool);
+
+void res_dump_pool_info(struct mv_request_pool *pool);
+#ifdef SEPARATE_IOCTL_REQ
+struct _MV_Request *get_ioctl_req_from_pool(struct mv_request_pool *pool);
+void free_ioctl_req_to_pool(struct mv_request_pool *pool,struct _MV_Request *req);
+#endif
+
+
+#endif /* __RES_MGMT_H__ */
--- /dev/null
+++ b/drivers/scsi/thor/mv_conf.mk
@@ -0,0 +1 @@
+SUPPORT_THOR=y
--- /dev/null
+++ b/drivers/scsi/thor/mv_config.h
@@ -0,0 +1,17 @@
+#if !defined(MV_CONFIGURATION_H)
+#define MV_CONFIGURATION_H
+
+#ifdef PRODUCTNAME_ODIN
+#include "mv_odin.h"
+
+#elif defined(SIMULATOR)
+
+#include "mv_simu.h"
+
+#elif defined(PRODUCTNAME_THOR)
+
+#include "mv_thor.h"
+
+#endif /*PRODUCTNAME_ODIN , PRODUCTNAME_THOR*/
+
+#endif /* MV_CONFIGURATION_H */
--- /dev/null
+++ b/drivers/scsi/thor/mv_include.h
@@ -0,0 +1,49 @@
+#if !defined(MV_INCLUDE_H)
+#define MV_INCLUDE_H
+
+#ifdef SIMULATOR
+#include "stdafx.h"
+#endif
+
+#include "mv_config.h"
+
+#include "mv_os.h"
+#include "com_type.h"
+#include "com_u64.h"
+
+#include "com_util.h"
+#include "com_list.h"
+
+#include "com_dbg.h"
+
+#include "hba_exp.h"
+#include "core_exp.h"
+
+#ifdef SUPPORT_TIMER
+#include "hba_timer.h"
+#endif
+
+#include "com_scsi.h"
+
+#include "com_api.h"
+#include "com_struct.h"
+#ifdef SUPPORT_SCSI_PASSTHROUGH
+#include "com_ioctl.h"
+#endif
+
+#ifndef _RAID_PARTIAL
+#ifdef RAID_DRIVER
+#include "raid_exp.h"
+#endif
+
+#ifdef SUPPORT_RAID6
+#include "math_gf.h"
+#endif
+
+#ifdef CACHE_MODULE_SUPPORT
+#include "cache_exp.h"
+#endif
+
+#endif
+
+#endif /* MV_INCLUDE_H */
--- /dev/null
+++ b/drivers/scsi/thor/mv_thor.h
@@ -0,0 +1,174 @@
+#ifndef __MV_PRODUCT_THOR_H__
+#define __MV_PRODUCT_THOR_H__
+
+/* OEM Account definition */
+#define VER_OEM_GENERIC			0
+#define VER_OEM_INTEL			1
+#define VER_OEM_ASUS			2
+#if defined(__MV_LINUX__) || defined(__QNXNTO__)
+#	define THOR_DRIVER						1
+#	define NEW_LINUX_DRIVER				1
+#   define __MM_SE__                       1
+#   define __RES_MGMT__                    1
+#endif /* __MV_LINUX__ && __QNXNTO__ */
+
+#ifndef __MV_LINUX__
+#define DISPATCH_HOOK
+/*
+ * define USE_NEW_SGTABLE to use the new SG format as defined in
+ * "General SG Format" document.
+ */
+#define USE_NEW_SGTABLE
+
+/*
+ * define USE_SRBEXT_AS_REQ to use Windows SrbExtension to store
+ * the MV_Request with its SG Table.
+ */
+#define USE_SRBEXT_AS_REQ
+
+#define FILTER_ASSISTED_COMMON_BUFFER
+
+#endif /* _OS_WINDOWS */
+
+#define USE_NEW_SGTABLE
+
+/* HBA macro definition */
+#define MV_MAX_TRANSFER_SIZE	(128*1024)
+#define MAX_REQUEST_NUMBER		32
+#define MAX_BASE_ADDRESS		6
+
+#define MAX_DEVICE_SUPPORTED	8
+
+/* Core driver macro definition */
+#define MAX_SG_ENTRY			68 //34*2, support > 128k OS tx data
+#define MAX_SG_ENTRY_REDUCED	16
+#define MV_MAX_PHYSICAL_BREAK	(MAX_SG_ENTRY - 1)
+
+#ifdef USE_NEW_SGTABLE
+#define HW_SG_ENTRY_MAX		(MAX_SG_ENTRY)
+#endif
+
+//#define ENABLE_PATA_ERROR_INTERRUPT	//ATAPI???
+
+/* It's dangerous. Never enable it unless we have to. */
+#define PRD_SIZE_WORD_ALIGN
+
+/*
+ * For ATAPI device, we can choose
+ * 1. Enable USE_PIO_FOR_ALL_PACKET_COMMAND
+ * 2. Enable USE_DMA_FOR_ALL_PACKET_COMMAND
+ * 3. Don't enable either of them. It'll use DMA for read/write. Others will use PIO.
+ * So far, USE_DMA_FOR_ALL_PACKET_COMMAND is the best choice.
+ */
+//#define USE_PIO_FOR_ALL_PACKET_COMMAND
+#define USE_DMA_FOR_ALL_PACKET_COMMAND
+
+
+#define CORE_SUPPORT_API      1
+#define CORE_IGNORE_START_STOP_UNIT
+
+
+//#define CORE_DEBUG_MODE
+
+/* hot plug & PM */
+#define SUPPORT_HOT_PLUG	1
+#define SUPPORT_PM			1
+#define SUPPORT_ERROR_HANDLING		1
+
+#ifndef __MV_LINUX__
+#define SUPPORT_CONSOLIDATE			1
+#define SUPPORT_TIMER				1
+#define SUPPORT_SCSI_PASSTHROUGH  1
+#endif /* _OS_WINDOWS */
+
+/* pass through */
+#ifdef SUPPORT_SCSI_PASSTHROUGH
+#define SUPPORT_VIRTUAL_DEVICE
+#define MV_MAX_TARGET_NUMBER		21		// virtual device 5 port*4 device
+#else
+#define MV_MAX_TARGET_NUMBER		20		// virtual device 5 port*4 device
+#endif
+#define MV_MAX_LUN_NUMBER			1
+#define MV_MAX_HD_DEVICE_ID		20		// virtual device 5 port*4 device
+
+#define VIRTUAL_DEVICE_ID					(MV_MAX_TARGET_NUMBER - 1) * MV_MAX_LUN_NUMBER
+
+#define SUPPORT_EVENT			  1
+
+/* RAID */
+#ifndef __MV_LINUX__
+#define PERFORMANCE_WHQL_SWITCH
+
+#define RAID_DRIVER				1
+#define SUPPORT_RAID6
+#define CACHE_MODULE_SUPPORT
+#endif /* _OS_WINDOWS */
+
+#ifdef RAID_DRIVER
+#define BGA_SUPPORT				1
+#define SOFTWARE_XOR			1
+#define SUPPORT_FREE_POLICY		1
+//#define SUPPORT_SRL				1
+#define SUPPORT_XOR_DWORD		1
+//#define SUPPORT_RAID1E				1
+#define SUPPORT_DUAL_DDF			0
+
+#ifdef SUPPORT_RAID6
+#define USE_MATH_LIBARY
+#define SUPPORT_READ_MODIFY_WRITE
+#define RAID_USE_64K_SU
+#endif
+/*
+ * define SUPPORT_NON_STRIPE to force RAID not to split RAID0/1
+ * access by stripes.
+ */
+#define SUPPORT_NON_STRIPE
+#define ALLOCATE_SENSE_BUFFER		1
+
+//#define SUPPORT_MIGRATION
+#define MV_MIGRATION_RESERVED_SPACE_V1	2048	/* 1MB & by sectors */
+#define MV_MIGRATION_RESERVED_SPACE_V2	32768 //16MB	// SECTORS
+#define MV_MIGRATION_SHIFT_SPACE		2048  // 1MB	// SECTORS
+#endif /* RAID_DRIVER */
+
+#if defined(USE_NEW_SGTABLE) && (defined(SOFTWARE_XOR) || defined(CACHE_MODULE_SUPPORT))
+#define USE_VIRTUAL_PRD_TABLE           1
+#endif
+
+#define ERROR_HANDLING_SUPPORT	0
+#define ERROR_SIMULATOR			0 /*1 for error simulator enable, 0 for disable*/
+
+/* Cache */
+#ifdef CACHE_MODULE_SUPPORT
+#define CACHE_FREE_DISK_ENABLE 1
+#endif
+
+#ifndef USE_NEW_SGTABLE
+#define SUPPORT_VIRTUAL_AND_PHYSICAL_SG
+#endif
+
+/* hardware-dependent definitions */
+#define MAX_EXPANDER_SUPPORTED				0
+#define MAX_PM_SUPPORTED					4
+#define MAX_BLOCK_PER_HD_SUPPORTED			8
+
+#define MAX_REQUEST_NUMBER_WHQL				MAX_REQUEST_NUMBER
+#define MAX_DEVICE_SUPPORTED_WHQL			8
+#define MAX_LD_SUPPORTED_WHQL				8
+#define MAX_BLOCK_SUPPORTED_WHQL			32
+
+#define MAX_REQUEST_NUMBER_PERFORMANCE		MAX_REQUEST_NUMBER_WHQL
+#define MAX_DEVICE_SUPPORTED_PERFORMANCE	MAX_DEVICE_SUPPORTED_WHQL
+#define MAX_LD_SUPPORTED_PERFORMANCE		MAX_LD_SUPPORTED_WHQL
+#define MAX_BLOCK_SUPPORTED_PERFORMANCE		MAX_BLOCK_SUPPORTED_WHQL
+#define MAX_CORE_REQUEST_NUMBER_PERFORMANCE   32
+
+#define COMMAND_ISSUE_WORKROUND		1
+
+#define SUPPORT_ATA_SMART        1
+#define SUPPORT_ATA_SECURITY_CMD 1
+#define HOTPLUG_ISSUE_WORKROUND 1
+#define SUPPORT_ATA_POWER_MANAGEMENT   1
+//#define SUPPORT_WORKQUEUE             1
+
+#endif
--- /dev/null
+++ b/drivers/scsi/thor/patch.kbuild
@@ -0,0 +1,10 @@
+--- scsi/Makefile	2009-10-15 23:03:27.153059900 -0400
++++ scsi_new/Makefile	2009-10-15 23:11:29.673495100 -0400
+@@ -48,6 +48,7 @@
+ obj-$(CONFIG_A2091_SCSI)	+= a2091.o	wd33c93.o
+ obj-$(CONFIG_GVP11_SCSI)	+= gvp11.o	wd33c93.o
+ obj-$(CONFIG_MVME147_SCSI)	+= mvme147.o	wd33c93.o
++obj-$(CONFIG_SCSI_MV_61xx)	+= mv/
+ obj-$(CONFIG_SGIWD93_SCSI)	+= sgiwd93.o	wd33c93.o
+ obj-$(CONFIG_ATARI_SCSI)	+= atari_scsi.o
+ obj-$(CONFIG_MAC_SCSI)		+= mac_scsi.o
--- /dev/null
+++ b/drivers/scsi/thor/patch.sh
@@ -0,0 +1,87 @@
+#!/bin/sh
+. mv_conf.mk
+subtxt(){
+
+    if [ ! -f "$1" ] || [ ! -w "$1" ]; then
+	echo "File does not exist or is not writable."
+	exit 1
+    fi
+
+    if [ "$2" == "a" ];then
+        if [ "$SUPPORT_THOR" == "y"  ];then
+
+            grep SCSI_MV_61xx "$1" >/dev/null 2>&1
+	    if [ "$?" == "0" ];then
+	        cat "$1"
+                return
+	    fi
+
+            sed -e '/if SCSI_LOWLEVEL && SCSI/{
+                    a\
+config SCSI_MV_61xx\
+	tristate "Marvell Storage Controller 6121/6122/6141/6145"\
+	depends on SCSI && BLK_DEV_SD\
+	help\
+		Provides support for Marvell 61xx Storage Controller series.\n
+}' "$1"
+
+        elif [ "$SUPPORT_VANIR" == "y" ]; then
+
+            grep SCSI_MV_94xx "$1" >/dev/null 2>&1
+            if [ "$?" == "0" ];then
+            cat "$1"
+               return
+            fi
+
+            sed -e '/if SCSI_LOWLEVEL && SCSI/{
+                    a\
+config SCSI_MV_94xx\
+	tristate "Marvell Storage Controller 9180/9480"\
+	depends on SCSI && BLK_DEV_SD\
+	help\
+		Provides support for Marvell 94xx Storage Controller series.\n
+}' "$1"
+
+        elif [ "$SUPPORT_ODIN" == "y" ]; then
+            grep SCSI_MV_64xx "$1" >/dev/null 2>&1
+            if [ "$?" == "0" ];then
+            cat "$1"
+               return
+            fi
+
+            sed -e '/if SCSI_LOWLEVEL && SCSI/{
+                    a\
+config SCSI_MV_64xx\
+	tristate "Marvell Storage Controller 6430/6320/6440/6445/6480/6485"\
+	depends on SCSI && BLK_DEV_SD\
+	help\
+		Provides support for Marvell 64xx Storage Controller series.\n
+}' "$1"
+        else
+	   echo "Cannot find the specified product, define mv_conf.mk."
+	   exit 1
+        fi
+    else
+        if [ "$SUPPORT_THOR" == "y"  ];then
+            sed -e '/SCSI_MV_61xx/,+5 d' "$1"
+        elif [ "$SUPPORT_VANIR" == "y"  ];then
+            sed -e '/SCSI_MV_94xx/,+5 d' "$1"
+        elif [ "$SUPPORT_ODIN" == "y"  ];then
+            sed -e '/SCSI_MV_64xx/,+5 d' "$1"
+        else
+	   echo "Cannot find the specified product, define mv_conf.mk."
+	   exit 1
+        fi
+    fi
+}
+
+# $1 is supposed to be the $KERNEL_SRC/drivers/scsi
+if [ ! -d "$1" ];then
+    echo "Cannot find the specified directory."
+    exit 1
+fi
+
+cd "$1"
+subtxt Kconfig $2 > Kconfig.new
+mv Kconfig Kconfig.orig
+mv Kconfig.new Kconfig
